[2025-10-23 02:23:26,092: INFO: template: Creating Directory:.github\workflow for the .gitkeep]
[2025-10-23 02:23:26,093: INFO: template: Creating empty file : .github\workflow\.gitkeep]
[2025-10-23 02:23:26,094: INFO: template: Creating Directory:src\Chatbot for the __init__.py]
[2025-10-23 02:23:26,095: INFO: template: Creating empty file : src\Chatbot\__init__.py]
[2025-10-23 02:23:26,095: INFO: template: Creating Directory:src\Chatbot\components for the __init__.py]
[2025-10-23 02:23:26,096: INFO: template: Creating empty file : src\Chatbot\components\__init__.py]
[2025-10-23 02:23:26,098: INFO: template: Creating Directory:src\Chatbot\utils\common.pysrc\Chatbot\research\research.ipynbsrc\Chatbot\utils for the __init__.py]
[2025-10-23 02:23:26,105: INFO: template: Creating empty file : src\Chatbot\utils\common.pysrc\Chatbot\research\research.ipynbsrc\Chatbot\utils\__init__.py]
[2025-10-23 02:23:26,106: INFO: template: Creating Directory:src\Chatbot\logging for the __init__.py]
[2025-10-23 02:23:26,107: INFO: template: __init__.py is Already exists]
[2025-10-23 02:23:26,107: INFO: template: Creating Directory:src\Chatbot\config for the __init__.py]
[2025-10-23 02:23:26,108: INFO: template: Creating empty file : src\Chatbot\config\__init__.py]
[2025-10-23 02:23:26,109: INFO: template: Creating Directory:src\Chatbot\config for the configration.py]
[2025-10-23 02:23:26,110: INFO: template: Creating empty file : src\Chatbot\config\configration.py]
[2025-10-23 02:23:26,111: INFO: template: Creating Directory:src\Chatbot\pipeline for the __init__.py]
[2025-10-23 02:23:26,112: INFO: template: Creating empty file : src\Chatbot\pipeline\__init__.py]
[2025-10-23 02:23:26,113: INFO: template: Creating Directory:src\Chatbot\entity for the __init__.py]
[2025-10-23 02:23:26,114: INFO: template: Creating empty file : src\Chatbot\entity\__init__.py]
[2025-10-23 02:23:26,114: INFO: template: Creating Directory:src\Chatbot\constants for the __init__.py]
[2025-10-23 02:23:26,115: INFO: template: Creating empty file : src\Chatbot\constants\__init__.py]
[2025-10-23 02:23:26,120: INFO: template: Creating Directory:config for the config.yaml]
[2025-10-23 02:23:26,121: INFO: template: Creating empty file : config\config.yaml]
[2025-10-23 02:23:26,122: INFO: template: Creating Directory:param for the params.yaml]
[2025-10-23 02:23:26,123: INFO: template: Creating empty file : param\params.yaml]
[2025-10-23 02:23:26,123: INFO: template: Creating empty file : app.py]
[2025-10-23 02:23:26,124: INFO: template: Creating empty file : main.py]
[2025-10-23 02:23:26,126: INFO: template: Creating empty file : .gitignore]
[2025-10-23 02:23:26,127: INFO: template: Creating empty file : Dockerfile]
[2025-10-23 02:23:26,128: INFO: template: Creating empty file : .dockerignore]
[2025-10-23 02:23:26,129: INFO: template: Creating empty file : requirements.txt]
[2025-10-23 02:23:26,133: INFO: template: Creating empty file : setup.py]
[2025-10-28 01:36:45,559: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 01:36:50,856: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 01:38:38,851: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 01:38:40,251: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 01:40:17,298: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 01:40:18,738: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 01:43:18,956: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 01:43:20,239: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 01:43:28,833: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 01:43:28,849: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 01:43:28,852: INFO: common: created directory at: artifacts]
[2025-10-28 01:43:28,854: INFO: common: created directory at: artifacts/data_ingestion]
[2025-10-28 01:43:28,856: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face...]
[2025-10-28 01:46:41,296: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 01:46:43,492: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 01:46:52,120: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 01:46:52,127: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 01:46:52,129: INFO: common: created directory at: artifacts]
[2025-10-28 01:46:52,130: INFO: common: created directory at: artifacts/data_ingestion]
[2025-10-28 01:46:52,131: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face...]
[2025-10-28 01:47:05,268: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train.parquet]
[2025-10-28 01:48:49,710: INFO: data_ingestion:  Dataset downloaded and saved successfully!]
[2025-10-28 01:48:50,067: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 01:48:51,598: INFO: data_transformation:  Loading dataset from artifacts/data_ingestion]
[2025-10-28 02:13:48,134: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 02:13:50,482: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 02:14:02,782: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 02:14:02,792: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 02:14:02,793: INFO: common: created directory at: artifacts]
[2025-10-28 02:14:02,794: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 03:13:17,221: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:13:22,441: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:13:44,405: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 03:13:44,423: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 03:13:44,424: INFO: common: created directory at: artifacts]
[2025-10-28 03:13:44,426: INFO: common: created directory at: artifacts/data_ingestion]
[2025-10-28 03:13:44,426: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face...]
[2025-10-28 03:14:02,207: INFO: data_ingestion:  Saving dataset to artifacts\data_ingestion in Arrow format...]
[2025-10-28 03:15:04,270: INFO: data_ingestion:  Dataset downloaded and saved successfully in Arrow format!]
[2025-10-28 03:15:04,679: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 03:15:06,283: INFO: data_transformation:  Loading dataset from artifacts/data_ingestion]
[2025-10-28 03:15:34,954: INFO: data_transformation:  Dataset loaded with 1000000 samples]
[2025-10-28 03:15:34,956: INFO: data_transformation: Converting conversations into input-target pairs...]
[2025-10-28 03:16:20,670: INFO: data_transformation: Saving batch 1...]
[2025-10-28 03:18:00,492: INFO: data_transformation: Saving batch 2...]
[2025-10-28 03:19:17,057: INFO: data_transformation: Saving batch 3...]
[2025-10-28 03:20:22,552: INFO: data_transformation: Saving batch 4...]
[2025-10-28 03:21:25,289: INFO: data_transformation: Saving batch 5...]
[2025-10-28 03:22:35,621: INFO: data_transformation: Saving batch 6...]
[2025-10-28 03:22:58,952: INFO: data_transformation:  Created total 3031290 conversation pairs]
[2025-10-28 03:22:59,020: INFO: data_transformation:  Applying tokenization in batches (memory safe)...]
[2025-10-28 03:26:47,186: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:26:50,088: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:27:08,147: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 03:27:08,158: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 03:27:08,160: INFO: common: created directory at: artifacts]
[2025-10-28 03:27:08,161: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 03:27:09,921: INFO: data_transformation:  Loading dataset from artifacts/data_ingestion]
[2025-10-28 03:27:16,739: INFO: data_transformation:  Dataset loaded with 1000000 samples]
[2025-10-28 03:27:16,741: INFO: data_transformation: Converting conversations into input-target pairs...]
[2025-10-28 03:27:34,844: INFO: data_transformation: Saving batch 1...]
[2025-10-28 03:27:59,416: INFO: data_transformation: Saving batch 2...]
[2025-10-28 03:28:28,025: INFO: data_transformation: Saving batch 3...]
[2025-10-28 03:28:56,002: INFO: data_transformation: Saving batch 4...]
[2025-10-28 03:29:26,646: INFO: data_transformation: Saving batch 5...]
[2025-10-28 03:29:57,595: INFO: data_transformation: Saving batch 6...]
[2025-10-28 03:30:29,032: INFO: data_transformation: Saving batch 7...]
[2025-10-28 03:31:02,989: INFO: data_transformation: Saving batch 8...]
[2025-10-28 03:31:36,365: INFO: data_transformation: Saving batch 9...]
[2025-10-28 03:32:06,093: INFO: data_transformation: Saving batch 10...]
[2025-10-28 03:32:14,436: INFO: data_transformation:  Created total 3031290 conversation pairs]
[2025-10-28 03:32:14,465: INFO: data_transformation:  Applying tokenization in batches (memory safe)...]
[2025-10-28 03:36:57,623: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:37:00,705: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:37:17,583: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 03:37:17,598: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 03:37:17,601: INFO: common: created directory at: artifacts]
[2025-10-28 03:37:17,603: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 03:37:19,304: INFO: data_transformation:  Loading dataset from artifacts/data_ingestion]
[2025-10-28 03:37:25,098: INFO: data_transformation:  Dataset loaded with 1000000 samples]
[2025-10-28 03:37:25,100: INFO: data_transformation: Converting conversations into input-target pairs...]
[2025-10-28 03:37:42,925: INFO: data_transformation: Saving batch 1...]
[2025-10-28 03:38:07,119: INFO: data_transformation: Saving batch 2...]
[2025-10-28 03:38:31,887: INFO: data_transformation: Saving batch 3...]
[2025-10-28 03:38:47,637: INFO: data_transformation: Saving batch 4...]
[2025-10-28 03:39:04,853: INFO: data_transformation: Saving batch 5...]
[2025-10-28 03:39:27,093: INFO: data_transformation: Saving batch 6...]
[2025-10-28 03:39:44,075: INFO: data_transformation: Saving batch 7...]
[2025-10-28 03:40:11,113: INFO: data_transformation: Saving batch 8...]
[2025-10-28 03:40:36,680: INFO: data_transformation: Saving batch 9...]
[2025-10-28 03:40:56,750: INFO: data_transformation: Saving batch 10...]
[2025-10-28 03:41:07,917: INFO: data_transformation:  Created total 3031290 conversation pairs]
[2025-10-28 03:41:07,943: INFO: data_transformation:  Applying tokenization in batches (memory safe)...]
[2025-10-28 03:41:07,955: INFO: data_transformation: Processing batch 1/61: samples 0-50000]
[2025-10-28 03:41:11,901: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:41:11,901: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:41:14,720: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:41:14,720: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:41:14,736: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:42:32,775: INFO: data_transformation: Processing batch 2/61: samples 50000-100000]
[2025-10-28 03:42:35,019: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:42:35,019: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:42:35,019: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:42:36,638: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:42:36,640: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:42:36,684: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:45:40,984: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:45:42,114: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:45:47,020: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 03:45:47,026: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 03:45:47,027: INFO: common: created directory at: artifacts]
[2025-10-28 03:45:47,028: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 03:45:48,315: INFO: data_transformation:  Loading dataset from artifacts/data_ingestion]
[2025-10-28 03:45:54,902: INFO: data_transformation:  Dataset loaded with 1000000 samples]
[2025-10-28 03:45:54,932: INFO: data_transformation: Converting conversations into input-target pairs...]
[2025-10-28 03:46:08,347: INFO: data_transformation: Saving batch 1...]
[2025-10-28 03:46:29,255: INFO: data_transformation: Saving batch 2...]
[2025-10-28 03:46:51,811: INFO: data_transformation: Saving batch 3...]
[2025-10-28 03:47:14,672: INFO: data_transformation: Saving batch 4...]
[2025-10-28 03:47:41,852: INFO: data_transformation: Saving batch 5...]
[2025-10-28 03:48:08,788: INFO: data_transformation: Saving batch 6...]
[2025-10-28 03:48:32,248: INFO: data_transformation: Saving batch 7...]
[2025-10-28 03:48:56,344: INFO: data_transformation: Saving batch 8...]
[2025-10-28 03:49:18,191: INFO: data_transformation: Saving batch 9...]
[2025-10-28 03:49:38,210: INFO: data_transformation: Saving batch 10...]
[2025-10-28 03:49:46,759: INFO: data_transformation:  Created total 3031290 conversation pairs]
[2025-10-28 03:49:46,806: INFO: data_transformation:  Applying tokenization in batches (memory safe)...]
[2025-10-28 03:49:46,810: INFO: data_transformation: Processing batch 1/11: samples 0-300000]
[2025-10-28 03:49:50,704: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:49:50,704: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:49:50,704: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:49:52,485: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:49:52,485: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:56:49,073: INFO: data_transformation: Processing batch 2/11: samples 300000-600000]
[2025-10-28 03:56:51,887: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:56:51,888: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:56:51,888: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:56:53,251: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:56:53,258: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:05:13,982: INFO: data_transformation: Processing batch 3/11: samples 600000-900000]
[2025-10-28 04:05:18,743: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:05:18,744: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:05:18,746: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:05:20,368: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:05:20,374: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:05:20,382: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:13:53,899: INFO: data_transformation: Processing batch 4/11: samples 900000-1200000]
[2025-10-28 04:13:57,868: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:13:57,869: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:13:57,869: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:13:59,822: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:13:59,834: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:13:59,840: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:33:15,337: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:33:25,110: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:33:59,776: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 04:33:59,789: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 04:33:59,796: INFO: common: created directory at: artifacts]
[2025-10-28 04:33:59,799: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 04:34:05,733: INFO: data_transformation:  Loading dataset from artifacts/data_ingestion]
[2025-10-28 04:34:19,059: INFO: data_transformation:  Dataset loaded with 1000000 samples]
[2025-10-28 04:34:19,099: INFO: data_transformation: Converting conversations into input-target pairs...]
[2025-10-28 04:35:53,419: INFO: data_transformation: Saving batch 1...]
[2025-10-28 04:37:02,449: INFO: data_transformation: Saving batch 2...]
[2025-10-28 04:37:54,406: INFO: data_transformation: Saving batch 3...]
[2025-10-28 04:39:22,074: INFO: data_transformation: Saving batch 4...]
[2025-10-28 04:40:15,152: INFO: data_transformation: Saving batch 5...]
[2025-10-28 04:41:05,264: INFO: data_transformation: Saving batch 6...]
[2025-10-28 04:42:04,945: INFO: data_transformation: Saving batch 7...]
[2025-10-28 04:42:58,239: INFO: data_transformation: Saving batch 8...]
[2025-10-28 04:43:38,293: INFO: data_transformation: Saving batch 9...]
[2025-10-28 04:44:40,800: INFO: data_transformation: Saving batch 10...]
[2025-10-28 04:45:03,705: INFO: data_transformation:  Created total 3031290 conversation pairs]
[2025-10-28 04:45:03,754: INFO: data_transformation:  Applying tokenization in batches (memory safe)...]
[2025-10-28 04:45:03,767: INFO: data_transformation: Processing batch 1/31: samples 0-100000]
[2025-10-28 04:45:10,290: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:45:10,291: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:45:10,293: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:45:12,587: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:45:12,587: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:45:12,587: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:47:15,398: INFO: data_transformation: Processing batch 2/31: samples 100000-200000]
[2025-10-28 04:47:17,819: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:47:17,821: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:47:17,823: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:47:19,521: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:47:19,537: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:47:19,538: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:49:00,930: INFO: data_transformation: Processing batch 3/31: samples 200000-300000]
[2025-10-28 04:49:02,672: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:49:02,673: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:49:04,552: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:49:04,560: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:49:04,562: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:50:50,962: INFO: data_transformation: Processing batch 4/31: samples 300000-400000]
[2025-10-28 04:50:53,314: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:50:53,325: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:50:54,263: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:50:54,268: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:50:54,271: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:53:53,020: INFO: data_transformation: Processing batch 5/31: samples 400000-500000]
[2025-10-28 04:53:55,701: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:53:55,703: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:53:55,705: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:53:57,588: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:53:57,589: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:53:57,762: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:56:20,779: INFO: data_transformation: Processing batch 6/31: samples 500000-600000]
[2025-10-28 04:56:22,608: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:56:22,609: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:56:23,839: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:56:23,845: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:58:22,815: INFO: data_transformation: Processing batch 7/31: samples 600000-700000]
[2025-10-28 04:58:24,486: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:58:24,498: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:58:25,623: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:58:25,624: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:58:25,630: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:00:17,416: INFO: data_transformation: Processing batch 8/31: samples 700000-800000]
[2025-10-28 05:00:19,292: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:00:19,292: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:00:19,295: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:00:20,540: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:00:20,544: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:00:20,570: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:01:59,511: INFO: data_transformation: Processing batch 9/31: samples 800000-900000]
[2025-10-28 05:02:01,359: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:02:01,481: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:02:01,611: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:02:02,352: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:02:02,511: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:02:02,628: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:03:42,975: INFO: data_transformation: Processing batch 10/31: samples 900000-1000000]
[2025-10-28 05:03:44,829: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:03:44,853: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:03:44,894: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:03:45,890: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:03:45,959: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:03:45,977: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:05:29,656: INFO: data_transformation: Processing batch 11/31: samples 1000000-1100000]
[2025-10-28 05:05:31,186: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:05:31,201: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:05:31,256: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:05:32,436: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:05:32,446: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:05:32,452: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:07:12,162: INFO: data_transformation: Processing batch 12/31: samples 1100000-1200000]
[2025-10-28 05:07:13,960: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:07:13,960: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:07:14,004: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:07:15,071: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:07:15,080: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:07:15,088: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:09:05,192: INFO: data_transformation: Processing batch 13/31: samples 1200000-1300000]
[2025-10-28 05:09:07,165: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:09:07,166: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:09:07,166: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:09:08,312: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:09:08,312: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:09:08,312: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:10:56,680: INFO: data_transformation: Processing batch 14/31: samples 1300000-1400000]
[2025-10-28 05:11:00,742: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:11:00,755: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:11:00,756: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:11:01,864: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:11:01,865: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:11:01,865: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:13:08,212: INFO: data_transformation: Processing batch 15/31: samples 1400000-1500000]
[2025-10-28 05:13:09,886: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:13:09,888: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:13:09,889: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:13:11,057: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:13:11,058: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:13:11,060: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:14:55,555: INFO: data_transformation: Processing batch 16/31: samples 1500000-1600000]
[2025-10-28 05:14:57,393: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:14:57,394: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:14:58,724: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:14:58,725: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:14:58,726: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:16:47,409: INFO: data_transformation: Processing batch 17/31: samples 1600000-1700000]
[2025-10-28 05:16:49,280: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:16:49,280: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:16:50,674: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:16:50,674: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:16:50,676: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:18:54,725: INFO: data_transformation: Processing batch 18/31: samples 1700000-1800000]
[2025-10-28 05:18:56,481: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:18:56,485: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:18:56,532: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:18:57,846: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:18:57,849: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:18:57,853: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:20:50,709: INFO: data_transformation: Processing batch 19/31: samples 1800000-1900000]
[2025-10-28 05:20:52,789: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:20:52,802: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:20:52,805: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:20:53,988: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:20:53,988: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:22:38,994: INFO: data_transformation: Processing batch 20/31: samples 1900000-2000000]
[2025-10-28 05:22:41,206: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:22:41,206: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:22:41,207: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:22:42,469: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:22:42,474: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:22:42,476: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:24:21,389: INFO: data_transformation: Processing batch 21/31: samples 2000000-2100000]
[2025-10-28 05:24:23,134: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:24:23,134: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:24:23,141: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:24:24,260: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:24:24,272: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:24:24,276: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:26:04,969: INFO: data_transformation: Processing batch 22/31: samples 2100000-2200000]
[2025-10-28 05:26:06,396: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:26:06,412: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:26:06,480: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:26:07,388: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:26:07,398: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:26:07,478: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 07:39:04,540: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 07:39:10,045: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 07:39:33,503: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 07:39:33,533: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 07:39:33,538: INFO: common: created directory at: artifacts]
[2025-10-28 07:39:33,539: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 07:39:35,751: INFO: data_transformation:  Loading dataset from artifacts/data_ingestion]
[2025-10-28 07:40:57,498: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 07:40:59,506: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 07:41:10,248: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 07:41:10,251: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 07:41:10,252: INFO: common: created directory at: artifacts]
[2025-10-28 07:41:10,253: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 07:41:12,430: INFO: data_transformation:  Loading dataset from artifacts/data_ingestion]
[2025-10-28 07:41:24,481: INFO: data_transformation:  Dataset loaded with 1000000 samples]
[2025-10-28 07:41:24,483: INFO: data_transformation: Converting conversations into input-target pairs...]
[2025-10-28 07:41:44,384: INFO: data_transformation: Saving batch 1...]
[2025-10-28 07:42:22,828: INFO: data_transformation: Saving batch 2...]
[2025-10-28 07:42:59,204: INFO: data_transformation: Saving batch 3...]
[2025-10-28 07:43:34,634: INFO: data_transformation: Saving batch 4...]
[2025-10-28 07:44:09,648: INFO: data_transformation: Saving batch 5...]
[2025-10-28 07:44:39,300: INFO: data_transformation: Saving batch 6...]
[2025-10-28 07:45:08,809: INFO: data_transformation: Saving batch 7...]
[2025-10-28 07:45:40,792: INFO: data_transformation: Saving batch 8...]
[2025-10-28 07:46:10,244: INFO: data_transformation: Saving batch 9...]
[2025-10-28 07:46:42,898: INFO: data_transformation: Saving batch 10...]
[2025-10-28 07:46:52,207: INFO: data_transformation:  Created total 3031290 conversation pairs]
[2025-10-28 07:46:52,237: INFO: data_transformation:  Applying tokenization in batches (memory safe)...]
[2025-10-28 07:46:52,246: INFO: data_transformation: Processing batch 1/61: samples 0-50000]
[2025-10-28 07:46:59,126: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 07:46:59,127: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 07:46:59,129: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 07:47:01,952: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 07:47:01,953: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 07:47:01,954: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 07:48:38,574: INFO: data_transformation: Processing batch 2/61: samples 50000-100000]
[2025-11-01 04:04:14,253: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 04:04:14,258: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 04:04:14,258: INFO: common: created directory at: artifacts]
[2025-11-01 04:04:14,259: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 04:04:14,261: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 04:04:14,262: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-01 04:04:15,202: INFO: model_evaluation:  Loading tokenized batches from artifacts/data_transformation/test]
[2025-11-01 04:04:16,556: INFO: model_evaluation:  Loaded total 304316 samples.]
[2025-11-01 04:04:16,583: INFO: model_evaluation:  Generating predictions...]
[2025-11-01 04:11:25,292: INFO: model_evaluation: Generated 50 samples...]
[2025-11-01 04:18:52,399: INFO: model_evaluation: Generated 100 samples...]
[2025-11-01 04:25:39,315: INFO: model_evaluation: Generated 150 samples...]
[2025-11-01 04:33:27,008: INFO: model_evaluation: Generated 200 samples...]
[2025-11-01 04:41:16,612: INFO: model_evaluation: Generated 250 samples...]
[2025-11-01 04:49:12,155: INFO: model_evaluation: Generated 300 samples...]
[2025-11-01 04:53:13,775: INFO: model_evaluation: Generated 350 samples...]
[2025-11-01 04:56:56,885: INFO: model_evaluation: Generated 400 samples...]
[2025-11-01 05:00:39,431: INFO: model_evaluation: Generated 450 samples...]
[2025-11-01 05:04:25,138: INFO: model_evaluation: Generated 500 samples...]
[2025-11-01 05:30:08,063: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 05:30:08,069: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 05:30:08,070: INFO: common: created directory at: artifacts]
[2025-11-01 05:30:08,071: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 05:30:08,072: INFO: model_evaluation: No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 05:30:08,073: INFO: model_evaluation: Loading model and tokenizer...]
[2025-11-01 05:30:08,766: INFO: model_evaluation: Loading tokenized batches from artifacts/data_transformation/test]
[2025-11-01 05:30:10,577: INFO: model_evaluation: Loaded total 304316 samples.]
[2025-11-01 05:30:10,605: INFO: model_evaluation: Generating predictions...]
[2025-11-01 05:35:44,125: INFO: model_evaluation: Generated 50 samples...]
[2025-11-01 05:43:14,519: INFO: model_evaluation: Generated 100 samples...]
[2025-11-01 05:43:24,349: INFO: model_evaluation: Calculating BLEU and ROUGE scores...]
[2025-11-01 05:57:29,501: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 05:57:29,682: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 05:57:29,712: INFO: common: created directory at: artifacts]
[2025-11-01 05:57:29,714: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 05:57:29,716: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 05:57:29,727: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-01 05:57:30,902: INFO: model_evaluation:  Loading tokenized batches from artifacts/data_transformation/test]
[2025-11-01 05:57:33,560: INFO: model_evaluation:  Loaded total 304316 samples.]
[2025-11-01 05:57:33,585: INFO: model_evaluation:  Generating predictions...]
[2025-11-01 06:00:09,323: INFO: model_evaluation: Generated 50 samples...]
[2025-11-01 06:04:11,777: INFO: model_evaluation: Generated 100 samples...]
[2025-11-01 06:04:19,234: INFO: model_evaluation:  Calculating BLEU and ROUGE scores...]
[2025-11-01 06:27:58,786: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 06:27:58,800: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 06:27:58,801: INFO: common: created directory at: artifacts]
[2025-11-01 06:27:58,803: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 06:27:58,807: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 06:27:58,808: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-01 06:28:00,134: INFO: model_evaluation:  Loading tokenized batches from artifacts/data_transformation/test]
[2025-11-01 06:28:01,714: INFO: model_evaluation:  Loaded total 304316 samples.]
[2025-11-01 06:28:01,739: INFO: model_evaluation:  Generating predictions...]
[2025-11-01 06:29:02,482: INFO: model_evaluation:  Calculating BLEU and ROUGE scores...]
[2025-11-01 06:29:58,343: INFO: rouge_scorer: Using default tokenizer.]
[2025-11-01 06:29:58,838: INFO: model_evaluation:  BLEU Score: 0.1050]
[2025-11-01 06:29:58,839: INFO: model_evaluation:  ROUGE Scores: {'rouge1': np.float64(0.19113276824554878), 'rouge2': np.float64(0.09897850531148564), 'rougeL': np.float64(0.16584568845945236)}]
[2025-11-01 06:29:58,841: INFO: model_evaluation:  BLEU Score: 0.1050]
[2025-11-01 06:29:58,842: INFO: model_evaluation:  ROUGE Scores: {'rouge1': np.float64(0.19113276824554878), 'rouge2': np.float64(0.09897850531148564), 'rougeL': np.float64(0.16584568845945236)}]
[2025-11-01 06:29:58,850: INFO: model_evaluation:  Metrics saved to: artifacts\model_evaluation\metrics.csv]
[2025-11-01 06:29:58,917: INFO: model_evaluation:  Sample predictions saved to: artifacts\model_evaluation\predictions.csv]
[2025-11-01 07:57:34,302: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 07:57:34,313: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 07:57:34,314: INFO: common: created directory at: artifacts]
[2025-11-01 07:57:34,316: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 07:57:38,292: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-01 07:57:38,296: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 07:57:38,354: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-01 07:57:38,769: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-01 07:57:39,130: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-01 07:57:39,459: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-01 07:57:40,320: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-01 07:57:40,798: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-01 07:57:41,144: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-01 07:57:41,474: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-01 07:57:41,855: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-01 07:57:42,278: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-01 07:57:42,711: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-01 07:57:43,132: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-01 07:57:43,759: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-01 07:57:44,215: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-01 07:57:44,601: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-01 07:57:45,013: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-01 07:57:45,325: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-01 07:57:45,751: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-01 07:57:46,170: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-01 07:57:46,568: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-01 07:57:46,969: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-01 07:57:47,392: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-01 07:57:47,739: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-01 07:57:48,147: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-01 07:57:48,573: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-01 07:57:48,971: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-01 07:57:49,359: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-01 07:57:49,732: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-01 07:57:50,094: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-01 07:57:50,415: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-01 07:57:50,700: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-01 07:57:51,134: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-01 07:57:51,437: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-01 07:57:51,756: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-01 07:57:52,064: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-01 07:57:52,449: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-01 07:57:52,815: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-01 07:57:53,156: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-01 07:57:53,469: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-01 07:57:53,867: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-01 07:57:54,281: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-01 07:57:54,673: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-01 07:57:55,219: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-01 07:57:55,607: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-01 07:57:56,007: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-01 07:57:56,297: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-01 07:57:56,798: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-01 07:57:57,227: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-01 07:57:57,621: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-01 07:57:57,972: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-01 07:57:58,352: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 50000 samples]
[2025-11-01 07:57:58,645: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_55 with 26974 samples]
[2025-11-01 07:57:58,960: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-01 07:57:59,314: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-01 07:57:59,671: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-01 07:58:00,020: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-01 07:58:00,203: INFO: model_trainer:  Combined total samples: 2726974]
[2025-11-01 07:58:00,204: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-01 07:58:00,471: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-01 07:58:00,688: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-01 07:58:00,943: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-01 07:58:01,200: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-01 07:58:01,477: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-01 07:58:01,687: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-01 07:58:01,745: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 4316 samples]
[2025-11-01 07:58:01,809: INFO: model_trainer:  Combined total samples: 304316]
[2025-11-01 07:58:02,065: INFO: model_trainer:  Train samples: 1000 | Eval samples: 500]
[2025-11-01 07:58:09,532: INFO: model_trainer:  Training model...]
[2025-11-02 11:37:06,316: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 11:37:06,336: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 11:37:06,337: INFO: common: created directory at: artifacts]
[2025-11-02 11:37:06,339: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 11:37:12,195: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-02 11:37:12,196: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 11:37:12,207: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-02 11:37:12,684: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-02 11:37:13,067: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-02 11:37:13,382: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-02 11:37:13,662: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-02 11:37:13,972: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-02 11:37:14,901: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-02 11:37:15,312: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-02 11:37:15,646: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-02 11:37:15,892: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-02 11:37:16,132: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-02 11:37:16,366: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-02 11:37:16,602: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-02 11:37:16,830: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-02 11:37:17,066: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-02 11:37:17,414: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-02 11:37:17,652: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-02 11:37:17,884: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-02 11:37:18,127: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-02 11:37:18,365: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-02 11:37:18,608: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-02 11:37:18,904: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-02 11:37:19,141: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-02 11:37:19,393: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-02 11:37:19,648: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-02 11:37:19,893: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-02 11:37:20,128: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-02 11:37:20,371: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-02 11:37:20,612: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-02 11:37:20,863: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-02 11:37:21,105: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-02 11:37:21,346: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-02 11:37:21,591: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-02 11:37:21,823: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-02 11:37:22,059: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-02 11:37:22,305: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-02 11:37:22,542: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-02 11:37:22,786: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-02 11:37:23,018: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-02 11:37:23,578: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-02 11:37:23,887: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-02 11:37:24,110: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-02 11:37:24,355: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-02 11:37:24,586: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-02 11:37:24,826: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-02 11:37:25,078: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-02 11:37:25,330: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-02 11:37:25,577: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-02 11:37:25,823: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-02 11:37:26,061: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-02 11:37:26,301: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 50000 samples]
[2025-11-02 11:37:26,444: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_55 with 26974 samples]
[2025-11-02 11:37:26,686: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-02 11:37:26,933: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-02 11:37:27,195: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-02 11:37:27,441: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-02 11:37:27,540: INFO: model_trainer:  Combined total samples: 2726974]
[2025-11-02 11:37:27,541: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-02 11:37:27,773: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-02 11:37:28,020: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-02 11:37:28,270: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-02 11:37:28,570: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-02 11:37:28,801: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-02 11:37:29,054: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-02 11:37:29,183: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 4316 samples]
[2025-11-02 11:37:29,197: INFO: model_trainer:  Combined total samples: 304316]
[2025-11-02 11:37:29,307: INFO: model_trainer:  Train samples: 1000 | Eval samples: 500]
[2025-11-02 11:37:35,259: INFO: model_trainer:  Training model...]
[2025-11-01 12:57:18,284: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 12:57:21,268: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 12:57:36,217: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 12:57:36,234: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 12:57:36,235: INFO: common: created directory at: artifacts]
[2025-11-01 12:57:36,236: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 12:57:37,937: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 12:57:41,706: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 12:57:41,712: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 12:57:44,139: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 12:57:44,140: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 12:57:44,140: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 12:59:19,631: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 12:59:20,886: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 12:59:27,830: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 12:59:27,832: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 12:59:27,832: INFO: common: created directory at: artifacts]
[2025-11-01 12:59:27,833: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 12:59:29,478: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 12:59:33,396: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 12:59:33,397: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 12:59:33,994: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 12:59:33,995: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 12:59:33,996: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:31:40,174: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:31:41,417: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:31:48,283: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 13:31:48,286: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 13:31:48,287: INFO: common: created directory at: artifacts]
[2025-11-01 13:31:48,288: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 13:31:49,544: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 13:31:53,968: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 13:31:53,969: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 13:31:54,744: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 13:31:54,830: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 13:31:54,831: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:32:18,898: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:32:20,177: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:32:27,329: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 13:32:27,331: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 13:32:27,331: INFO: common: created directory at: artifacts]
[2025-11-01 13:32:27,332: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 13:32:28,566: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 13:32:32,483: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 13:32:32,486: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 13:32:33,076: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 13:32:33,077: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 13:32:33,078: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:33:16,491: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:33:49,096: INFO: data_transformation:  Processed 600000 samples so far...]
[2025-11-01 13:34:15,652: INFO: data_transformation:  Processed 900000 samples so far...]
[2025-11-01 13:34:37,753: INFO: data_transformation:  Processed 1200000 samples so far...]
[2025-11-01 13:35:01,752: INFO: data_transformation:  Processed 1500000 samples so far...]
[2025-11-01 13:35:35,423: INFO: data_transformation:  Processed 1800000 samples so far...]
[2025-11-01 13:35:56,645: INFO: data_transformation:  Processed 2100000 samples so far...]
[2025-11-01 13:36:17,156: INFO: data_transformation:  Processed 2400000 samples so far...]
[2025-11-01 13:36:42,117: INFO: data_transformation:  Processed 2700000 samples so far...]
[2025-11-01 13:36:43,839: INFO: data_transformation:  Created 2726974 total conversation pairs]
[2025-11-01 13:36:43,858: INFO: data_transformation:  Preparing test conversation pairs...]
[2025-11-01 13:36:43,859: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:37:01,968: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:37:02,306: INFO: data_transformation:  Created 304316 total conversation pairs]
[2025-11-01 13:37:02,311: INFO: data_transformation:  Starting tokenization for train data...]
[2025-11-01 13:37:02,312: INFO: data_transformation:  Processing train batch 1/55: samples 0-50000]
[2025-11-01 13:37:14,660: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:37:14,661: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:37:17,902: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:37:17,902: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:37:17,903: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:38:45,006: INFO: data_transformation:  Processing train batch 2/55: samples 50000-100000]
[2025-11-01 13:38:47,045: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:38:47,045: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:38:47,052: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:38:48,370: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:38:48,372: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:38:48,373: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:39:57,454: INFO: data_transformation:  Processing train batch 3/55: samples 100000-150000]
[2025-11-01 13:40:00,445: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:40:00,764: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:40:00,853: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:40:03,029: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:40:03,536: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:40:04,308: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:40:20,698: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:40:20,802: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:40:56,708: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:40:57,703: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:41:02,640: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 13:41:02,663: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 13:41:02,666: INFO: common: created directory at: artifacts]
[2025-11-01 13:41:02,667: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 13:41:04,096: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 13:41:08,028: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 13:41:08,029: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 13:41:08,645: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 13:41:08,646: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 13:41:08,650: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:41:24,485: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:41:42,556: INFO: data_transformation:  Processed 600000 samples so far...]
[2025-11-01 13:42:02,274: INFO: data_transformation:  Processed 900000 samples so far...]
[2025-11-01 13:42:26,207: INFO: data_transformation:  Processed 1200000 samples so far...]
[2025-11-01 13:42:51,509: INFO: data_transformation:  Processed 1500000 samples so far...]
[2025-11-01 13:43:19,973: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:43:21,656: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:43:31,656: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 13:43:31,665: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 13:43:31,666: INFO: common: created directory at: artifacts]
[2025-11-01 13:43:31,666: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 13:43:32,936: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 13:43:36,737: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 13:43:36,738: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 13:43:37,301: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 13:43:37,302: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 13:43:37,302: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:43:50,277: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:44:06,106: INFO: data_transformation:  Processed 600000 samples so far...]
[2025-11-01 13:44:23,841: INFO: data_transformation:  Processed 900000 samples so far...]
[2025-11-01 13:44:43,663: INFO: data_transformation:  Processed 1200000 samples so far...]
[2025-11-01 13:45:01,338: INFO: data_transformation:  Processed 1500000 samples so far...]
[2025-11-01 13:45:21,284: INFO: data_transformation:  Processed 1800000 samples so far...]
[2025-11-01 13:45:41,898: INFO: data_transformation:  Processed 2100000 samples so far...]
[2025-11-01 13:46:01,188: INFO: data_transformation:  Processed 2400000 samples so far...]
[2025-11-01 13:46:24,581: INFO: data_transformation:  Processed 2700000 samples so far...]
[2025-11-01 13:46:26,272: INFO: data_transformation:  Created 2726974 total conversation pairs]
[2025-11-01 13:46:26,289: INFO: data_transformation:  Preparing test conversation pairs...]
[2025-11-01 13:46:26,290: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:46:43,716: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:46:44,135: INFO: data_transformation:  Created 304316 total conversation pairs]
[2025-11-01 13:46:44,207: INFO: data_transformation:  Starting tokenization for train data...]
[2025-11-01 13:46:44,208: INFO: data_transformation:  Processing train batch 1/55: samples 0-50000]
[2025-11-01 13:46:50,550: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:46:50,551: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:46:52,185: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:46:52,185: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:47:33,927: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:47:35,188: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:47:41,713: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 13:47:41,723: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 13:47:41,725: INFO: common: created directory at: artifacts]
[2025-11-01 13:47:41,726: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 13:47:42,946: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 13:47:46,532: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 13:47:46,533: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 13:47:47,093: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 13:47:47,094: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 13:47:47,095: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:48:00,729: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:48:16,237: INFO: data_transformation:  Processed 600000 samples so far...]
[2025-11-01 13:48:33,657: INFO: data_transformation:  Processed 900000 samples so far...]
[2025-11-01 13:48:50,911: INFO: data_transformation:  Processed 1200000 samples so far...]
[2025-11-01 13:49:10,373: INFO: data_transformation:  Processed 1500000 samples so far...]
[2025-11-01 13:49:28,815: INFO: data_transformation:  Processed 1800000 samples so far...]
[2025-11-01 13:49:46,095: INFO: data_transformation:  Processed 2100000 samples so far...]
[2025-11-01 13:50:05,851: INFO: data_transformation:  Processed 2400000 samples so far...]
[2025-11-01 13:50:36,519: INFO: data_transformation:  Processed 2700000 samples so far...]
[2025-11-01 13:50:39,703: INFO: data_transformation:  Created 2726974 total conversation pairs]
[2025-11-01 13:50:39,722: INFO: data_transformation:  Preparing test conversation pairs...]
[2025-11-01 13:50:39,735: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:50:57,562: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:50:57,818: INFO: data_transformation:  Created 304316 total conversation pairs]
[2025-11-01 13:50:57,827: INFO: data_transformation:  Starting tokenization for train data...]
[2025-11-01 13:50:57,828: INFO: data_transformation:  Processing train batch 1/55: samples 0-50000]
[2025-11-01 13:51:16,466: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:51:18,111: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:51:28,992: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 13:51:29,000: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 13:51:29,001: INFO: common: created directory at: artifacts]
[2025-11-01 13:51:29,002: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 13:51:30,318: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 13:51:33,509: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 13:51:33,510: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 13:51:33,698: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 13:51:33,699: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 13:51:33,700: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:51:51,481: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:52:07,474: INFO: data_transformation:  Processed 600000 samples so far...]
[2025-11-01 13:52:25,073: INFO: data_transformation:  Processed 900000 samples so far...]
[2025-11-01 13:52:42,459: INFO: data_transformation:  Processed 1200000 samples so far...]
[2025-11-01 13:53:00,210: INFO: data_transformation:  Processed 1500000 samples so far...]
[2025-11-01 13:53:18,416: INFO: data_transformation:  Processed 1800000 samples so far...]
[2025-11-01 13:53:36,543: INFO: data_transformation:  Processed 2100000 samples so far...]
[2025-11-01 13:54:00,555: INFO: data_transformation:  Processed 2400000 samples so far...]
[2025-11-01 13:54:24,546: INFO: data_transformation:  Processed 2700000 samples so far...]
[2025-11-01 13:54:26,729: INFO: data_transformation:  Created 2726974 total conversation pairs]
[2025-11-01 13:54:26,759: INFO: data_transformation:  Preparing test conversation pairs...]
[2025-11-01 13:54:26,759: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:54:44,681: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:54:45,001: INFO: data_transformation:  Created 304316 total conversation pairs]
[2025-11-01 13:54:45,025: INFO: data_transformation:  Starting tokenization for train data...]
[2025-11-01 13:54:45,028: INFO: data_transformation:  Processing train batch 1/55: samples 0-50000]
[2025-11-01 13:54:50,794: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:54:50,794: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:54:50,795: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:54:52,132: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:54:52,132: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:54:52,132: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:58:02,031: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:58:03,400: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:58:10,440: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 13:58:10,451: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 13:58:10,454: INFO: common: created directory at: artifacts]
[2025-11-01 13:58:10,455: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 13:58:11,819: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 13:58:15,687: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 13:58:15,688: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 13:58:16,271: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 13:58:16,272: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 13:58:16,272: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 14:03:49,011: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:03:49,870: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:03:54,475: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 14:03:54,477: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 14:03:54,478: INFO: common: created directory at: artifacts]
[2025-11-01 14:03:54,478: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 14:03:55,521: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 14:03:56,177: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 14:03:56,183: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 14:03:56,258: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 14:03:56,259: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 14:03:56,259: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 14:04:11,623: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 14:04:33,051: INFO: data_transformation:  Processed 600000 samples so far...]
[2025-11-01 14:04:52,189: INFO: data_transformation:  Processed 900000 samples so far...]
[2025-11-01 14:05:11,184: INFO: data_transformation:  Processed 1200000 samples so far...]
[2025-11-01 14:05:30,342: INFO: data_transformation:  Processed 1500000 samples so far...]
[2025-11-01 14:05:52,879: INFO: data_transformation:  Processed 1800000 samples so far...]
[2025-11-01 14:06:10,045: INFO: data_transformation:  Processed 2100000 samples so far...]
[2025-11-01 14:06:32,123: INFO: data_transformation:  Processed 2400000 samples so far...]
[2025-11-01 14:06:55,854: INFO: data_transformation:  Processed 2700000 samples so far...]
[2025-11-01 14:06:57,538: INFO: data_transformation:  Created 2726974 total conversation pairs]
[2025-11-01 14:06:57,562: INFO: data_transformation:  Preparing test conversation pairs...]
[2025-11-01 14:06:57,562: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 14:07:15,229: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 14:07:15,590: INFO: data_transformation:  Created 304316 total conversation pairs]
[2025-11-01 14:07:15,603: INFO: data_transformation:  Starting tokenization for train data...]
[2025-11-01 14:07:15,603: INFO: data_transformation:  Processing train batch 1/55: samples 0-50000]
[2025-11-01 14:07:18,788: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:07:18,788: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:07:20,172: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:07:20,849: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:07:20,851: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:12:49,432: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 14:12:49,437: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 14:12:49,439: INFO: common: created directory at: artifacts]
[2025-11-01 14:12:49,440: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 14:12:52,427: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-01 14:12:52,428: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 14:12:52,435: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-01 14:12:52,714: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-01 14:12:52,937: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-01 14:12:53,166: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-01 14:12:53,399: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-01 14:12:53,627: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-01 14:12:53,854: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-01 14:12:54,080: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-01 14:12:54,308: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-01 14:12:54,540: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-01 14:12:54,765: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-01 14:12:54,994: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-01 14:12:55,221: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-01 14:12:55,455: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-01 14:12:55,680: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-01 14:12:55,914: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-01 14:12:56,152: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-01 14:12:56,395: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-01 14:12:56,636: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-01 14:12:56,867: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-01 14:12:57,107: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-01 14:12:57,345: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-01 14:12:57,587: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-01 14:12:57,821: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-01 14:12:58,055: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-01 14:12:58,296: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-01 14:12:58,534: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-01 14:12:58,766: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-01 14:12:58,992: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-01 14:12:59,221: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-01 14:12:59,461: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-01 14:12:59,701: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-01 14:12:59,937: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-01 14:13:00,166: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-01 14:13:00,390: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-01 14:13:00,625: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-01 14:13:00,870: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-01 14:13:01,101: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-01 14:13:01,357: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-01 14:13:01,583: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-01 14:13:01,929: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-01 14:13:02,189: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-01 14:13:02,433: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-01 14:13:02,665: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-01 14:13:02,891: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-01 14:13:03,117: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-01 14:13:03,347: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-01 14:13:03,585: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-01 14:13:03,818: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-01 14:13:04,047: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-01 14:13:04,273: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 50000 samples]
[2025-11-01 14:13:04,422: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_55 with 26974 samples]
[2025-11-01 14:13:04,654: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-01 14:13:04,878: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-01 14:13:05,106: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-01 14:13:05,329: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-01 14:13:05,390: INFO: model_trainer:  Combined total samples: 2726974]
[2025-11-01 14:13:05,391: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-01 14:13:05,620: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-01 14:13:05,923: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-01 14:13:06,151: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-01 14:13:06,383: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-01 14:13:06,619: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-01 14:13:06,856: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-01 14:13:06,918: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 4316 samples]
[2025-11-01 14:13:06,933: INFO: model_trainer:  Combined total samples: 304316]
[2025-11-01 14:13:07,033: INFO: model_trainer:  Train samples: 1000 | Eval samples: 500]
[2025-11-01 14:13:11,306: INFO: model_trainer:  Training model...]
[2025-11-01 14:29:15,497: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:29:16,497: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:29:23,030: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 14:29:23,032: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 14:29:23,032: INFO: common: created directory at: artifacts]
[2025-11-01 14:29:23,033: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 14:29:24,620: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 14:29:28,416: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 14:29:28,417: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 14:29:28,968: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 14:29:28,968: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 14:29:28,969: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 14:29:51,971: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 14:30:40,011: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:30:41,473: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:30:49,087: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 14:30:49,089: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 14:30:49,090: INFO: common: created directory at: artifacts]
[2025-11-01 14:30:49,091: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 14:30:50,474: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 14:30:54,316: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 14:30:54,325: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 14:30:55,045: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 14:30:55,045: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 14:30:55,046: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 14:31:23,521: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 14:31:48,346: INFO: data_transformation:  Processed 600000 samples so far...]
[2025-11-01 14:32:07,542: INFO: data_transformation:  Processed 900000 samples so far...]
[2025-11-01 14:32:25,896: INFO: data_transformation:  Processed 1200000 samples so far...]
[2025-11-01 14:32:46,164: INFO: data_transformation:  Processed 1500000 samples so far...]
[2025-11-01 14:33:08,861: INFO: data_transformation:  Processed 1800000 samples so far...]
[2025-11-01 14:33:26,073: INFO: data_transformation:  Processed 2100000 samples so far...]
[2025-11-01 14:33:49,299: INFO: data_transformation:  Processed 2400000 samples so far...]
[2025-11-01 14:34:06,340: INFO: data_transformation:   Created 2693391 total conversation pairs]
[2025-11-01 14:34:06,933: INFO: data_transformation:  Preparing test conversation pairs...]
[2025-11-01 14:34:06,934: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 14:34:25,054: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 14:34:25,174: INFO: data_transformation:   Created 300679 total conversation pairs]
[2025-11-01 14:34:25,203: INFO: data_transformation:  Starting tokenization for train data...]
[2025-11-01 14:34:25,207: INFO: data_transformation:  Processing train batch 1/54: samples 0-50000]
[2025-11-01 14:34:29,116: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:34:29,116: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:34:30,576: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:34:30,576: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:36:18,029: INFO: data_transformation:  Processing train batch 2/54: samples 50000-100000]
[2025-11-01 14:36:20,495: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:36:20,496: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:36:20,497: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:36:22,077: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:36:22,107: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:36:22,171: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:37:45,786: INFO: data_transformation:  Processing train batch 3/54: samples 100000-150000]
[2025-11-01 14:37:47,948: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:37:47,963: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:37:47,979: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:37:49,578: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:37:49,578: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:37:49,578: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:38:56,769: INFO: data_transformation:  Processing train batch 4/54: samples 150000-200000]
[2025-11-01 14:38:58,160: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:38:58,164: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:38:58,209: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:38:59,176: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:38:59,197: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:38:59,246: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:39:57,758: INFO: data_transformation:  Processing train batch 5/54: samples 200000-250000]
[2025-11-01 14:39:59,254: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:39:59,255: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:39:59,269: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:40:00,213: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:40:00,220: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:40:00,233: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:40:57,758: INFO: data_transformation:  Processing train batch 6/54: samples 250000-300000]
[2025-11-01 14:40:59,269: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:40:59,283: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:40:59,316: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:41:00,223: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:41:00,240: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:41:00,295: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:42:03,782: INFO: data_transformation:  Processing train batch 7/54: samples 300000-350000]
[2025-11-01 14:42:05,215: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:42:05,332: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:42:05,339: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:42:06,309: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:42:06,479: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:42:06,492: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:43:08,595: INFO: data_transformation:  Processing train batch 8/54: samples 350000-400000]
[2025-11-01 14:43:10,610: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:43:10,622: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:43:10,631: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:43:11,805: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:43:11,806: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:43:11,818: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:44:11,743: INFO: data_transformation:  Processing train batch 9/54: samples 400000-450000]
[2025-11-01 14:44:13,208: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:44:13,228: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:44:13,230: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:44:14,191: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:44:14,208: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:44:14,215: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:45:07,143: INFO: data_transformation:  Processing train batch 10/54: samples 450000-500000]
[2025-11-01 14:45:08,729: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:45:08,747: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:45:08,760: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:45:09,696: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:45:09,712: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:45:09,723: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:46:07,445: INFO: data_transformation:  Processing train batch 11/54: samples 500000-550000]
[2025-11-01 14:46:08,878: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:46:08,885: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:46:08,902: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:46:09,872: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:46:09,874: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:46:09,887: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:47:11,532: INFO: data_transformation:  Processing train batch 12/54: samples 550000-600000]
[2025-11-01 14:47:12,959: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:47:12,977: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:47:13,025: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:47:14,125: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:47:14,208: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:47:14,219: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:48:06,946: INFO: data_transformation:  Processing train batch 13/54: samples 600000-650000]
[2025-11-01 14:48:08,357: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:48:08,365: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:48:08,428: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:48:09,334: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:48:09,337: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:48:09,433: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:49:26,362: INFO: data_transformation:  Processing train batch 14/54: samples 650000-700000]
[2025-11-01 14:49:28,720: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:49:28,720: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:49:30,290: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:49:30,337: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:49:30,341: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:51:17,644: INFO: data_transformation:  Processing train batch 15/54: samples 700000-750000]
[2025-11-01 14:51:20,071: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:51:20,072: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:51:20,080: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:51:21,815: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:51:21,899: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:51:22,384: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:52:54,249: INFO: data_transformation:  Processing train batch 16/54: samples 750000-800000]
[2025-11-01 14:52:57,259: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:52:57,260: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:52:57,262: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:52:59,235: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:52:59,456: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:52:59,490: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:54:33,309: INFO: data_transformation:  Processing train batch 17/54: samples 800000-850000]
[2025-11-01 14:54:36,158: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:54:36,158: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:54:37,706: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:54:37,706: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:54:37,728: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:55:41,492: INFO: data_transformation:  Processing train batch 18/54: samples 850000-900000]
[2025-11-01 14:55:43,251: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:55:43,252: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:55:44,329: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:55:44,334: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:55:44,346: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:56:47,951: INFO: data_transformation:  Processing train batch 19/54: samples 900000-950000]
[2025-11-01 14:56:49,933: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:56:49,955: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:56:50,010: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:56:51,503: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:56:51,610: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:56:51,693: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:58:28,450: INFO: data_transformation:  Processing train batch 20/54: samples 950000-1000000]
[2025-11-01 14:58:31,108: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:58:31,110: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:58:33,028: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:58:33,042: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:58:33,087: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:00:08,539: INFO: data_transformation:  Processing train batch 21/54: samples 1000000-1050000]
[2025-11-01 15:00:11,053: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:00:11,333: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:00:11,353: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:00:12,664: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:00:12,798: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:00:12,865: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:01:52,175: INFO: data_transformation:  Processing train batch 22/54: samples 1050000-1100000]
[2025-11-01 15:01:54,540: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:01:54,541: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:01:54,551: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:01:56,085: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:01:56,086: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:01:56,086: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:03:31,831: INFO: data_transformation:  Processing train batch 23/54: samples 1100000-1150000]
[2025-11-01 15:03:34,114: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:03:34,114: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:03:34,115: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:03:35,565: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:03:35,569: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:03:35,584: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:04:58,487: INFO: data_transformation:  Processing train batch 24/54: samples 1150000-1200000]
[2025-11-01 15:05:00,395: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:05:00,396: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:05:00,396: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:05:02,405: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:05:02,897: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:05:02,990: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:06:35,508: INFO: data_transformation:  Processing train batch 25/54: samples 1200000-1250000]
[2025-11-01 15:06:38,276: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:06:38,278: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:06:38,279: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:06:39,769: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:06:39,770: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:06:39,771: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:08:22,242: INFO: data_transformation:  Processing train batch 26/54: samples 1250000-1300000]
[2025-11-01 15:08:25,220: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:08:25,281: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:08:25,434: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:08:27,383: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:08:27,643: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:08:27,646: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:09:59,813: INFO: data_transformation:  Processing train batch 27/54: samples 1300000-1350000]
[2025-11-01 15:10:02,225: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:10:02,225: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:10:02,225: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:10:03,630: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:10:03,635: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:10:03,638: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:11:12,618: INFO: data_transformation:  Processing train batch 28/54: samples 1350000-1400000]
[2025-11-01 15:11:14,507: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:11:14,508: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:11:14,508: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:11:15,809: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:11:15,809: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:11:15,811: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:12:28,960: INFO: data_transformation:  Processing train batch 29/54: samples 1400000-1450000]
[2025-11-01 15:12:31,208: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:12:31,220: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:12:31,231: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:12:32,493: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:12:32,497: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:12:32,503: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:13:35,229: INFO: data_transformation:  Processing train batch 30/54: samples 1450000-1500000]
[2025-11-01 15:13:37,481: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:13:37,481: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:13:38,827: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:13:38,827: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:13:38,835: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:14:40,991: INFO: data_transformation:  Processing train batch 31/54: samples 1500000-1550000]
[2025-11-01 15:14:42,773: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:14:42,790: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:14:42,790: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:14:43,911: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:14:43,916: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:14:43,920: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:15:44,252: INFO: data_transformation:  Processing train batch 32/54: samples 1550000-1600000]
[2025-11-01 15:15:46,212: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:15:46,215: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:15:47,517: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:15:47,521: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:15:47,527: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:16:45,143: INFO: data_transformation:  Processing train batch 33/54: samples 1600000-1650000]
[2025-11-01 15:16:47,054: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:16:47,055: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:16:47,055: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:16:48,352: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:16:48,354: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:16:48,361: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:17:49,039: INFO: data_transformation:  Processing train batch 34/54: samples 1650000-1700000]
[2025-11-01 15:17:50,441: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:17:50,465: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:17:50,487: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:17:51,450: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:17:51,459: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:17:51,484: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:18:48,772: INFO: data_transformation:  Processing train batch 35/54: samples 1700000-1750000]
[2025-11-01 15:18:50,477: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:18:50,493: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:18:50,495: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:18:51,941: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:18:51,943: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:18:51,943: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:20:01,101: INFO: data_transformation:  Processing train batch 36/54: samples 1750000-1800000]
[2025-11-01 15:20:03,200: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:20:03,200: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:20:03,202: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:20:04,343: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:20:04,347: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:20:04,356: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:21:06,228: INFO: data_transformation:  Processing train batch 37/54: samples 1800000-1850000]
[2025-11-01 15:21:07,683: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:21:07,693: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:21:07,753: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:21:08,667: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:21:08,672: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:21:08,733: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:22:10,764: INFO: data_transformation:  Processing train batch 38/54: samples 1850000-1900000]
[2025-11-01 15:22:12,501: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:22:12,502: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:22:13,836: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:22:13,838: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:23:11,302: INFO: data_transformation:  Processing train batch 39/54: samples 1900000-1950000]
[2025-11-01 15:23:12,756: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:23:12,766: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:23:12,806: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:23:13,699: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:23:13,709: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:23:13,812: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:24:09,336: INFO: data_transformation:  Processing train batch 40/54: samples 1950000-2000000]
[2025-11-01 15:24:10,858: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:24:10,880: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:24:10,913: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:24:11,845: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:24:11,851: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:24:11,919: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:25:06,726: INFO: data_transformation:  Processing train batch 41/54: samples 2000000-2050000]
[2025-11-01 15:25:08,215: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:25:08,242: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:25:08,293: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:25:09,179: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:25:09,208: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:25:09,271: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:26:04,723: INFO: data_transformation:  Processing train batch 42/54: samples 2050000-2100000]
[2025-11-01 15:26:06,185: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:26:06,223: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:26:06,275: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:26:07,160: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:26:07,189: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:26:07,219: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:27:07,510: INFO: data_transformation:  Processing train batch 43/54: samples 2100000-2150000]
[2025-11-01 15:27:08,982: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:27:08,992: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:27:09,058: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:27:09,912: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:27:09,939: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:27:10,012: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:28:17,933: INFO: data_transformation:  Processing train batch 44/54: samples 2150000-2200000]
[2025-11-01 15:28:19,583: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:28:19,584: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:28:19,584: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:28:20,764: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:28:20,764: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:28:20,764: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:29:18,976: INFO: data_transformation:  Processing train batch 45/54: samples 2200000-2250000]
[2025-11-01 15:29:21,905: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:29:22,053: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:29:22,105: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:29:23,342: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:29:23,502: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:29:23,531: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:30:38,354: INFO: data_transformation:  Processing train batch 46/54: samples 2250000-2300000]
[2025-11-01 15:30:40,545: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:30:40,545: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:30:40,546: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:30:41,728: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:30:41,730: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:30:41,734: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:31:50,958: INFO: data_transformation:  Processing train batch 47/54: samples 2300000-2350000]
[2025-11-01 15:31:52,834: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:31:52,834: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:31:52,838: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:31:54,028: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:31:54,029: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:31:54,035: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:33:02,829: INFO: data_transformation:  Processing train batch 48/54: samples 2350000-2400000]
[2025-11-01 15:33:05,032: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:33:05,032: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:33:05,040: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:33:06,198: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:33:06,199: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:33:06,202: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:34:04,575: INFO: data_transformation:  Processing train batch 49/54: samples 2400000-2450000]
[2025-11-01 15:34:06,085: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:34:06,087: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:34:06,122: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:34:07,037: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:34:07,040: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:34:07,065: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:35:06,267: INFO: data_transformation:  Processing train batch 50/54: samples 2450000-2500000]
[2025-11-01 15:35:07,698: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:35:07,728: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:35:07,764: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:35:08,705: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:35:08,726: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:35:08,801: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:36:03,264: INFO: data_transformation:  Processing train batch 51/54: samples 2500000-2550000]
[2025-11-01 15:36:05,140: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:36:05,141: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:36:06,357: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:36:06,360: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:37:02,869: INFO: data_transformation:  Processing train batch 52/54: samples 2550000-2600000]
[2025-11-01 15:37:05,032: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:37:05,033: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:37:05,033: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:37:06,242: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:37:06,246: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:37:06,249: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:38:07,431: INFO: data_transformation:  Processing train batch 53/54: samples 2600000-2650000]
[2025-11-01 15:38:08,893: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:38:08,897: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:38:08,940: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:38:09,898: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:38:09,899: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:38:09,900: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:39:20,358: INFO: data_transformation:  Processing train batch 54/54: samples 2650000-2693391]
[2025-11-01 15:39:22,358: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:39:22,358: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:39:22,360: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:39:23,524: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:39:23,525: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:39:23,526: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:40:26,284: INFO: data_transformation:  Starting tokenization for test data...]
[2025-11-01 15:40:26,285: INFO: data_transformation:  Processing test batch 1/7: samples 0-50000]
[2025-11-01 15:40:28,248: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:40:28,263: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:40:28,311: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:40:29,433: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:40:29,435: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:40:29,477: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:41:59,888: INFO: data_transformation:  Processing test batch 2/7: samples 50000-100000]
[2025-11-01 15:42:02,047: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:42:02,051: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:42:03,339: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:42:03,341: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:42:03,348: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:43:09,650: INFO: data_transformation:  Processing test batch 3/7: samples 100000-150000]
[2025-11-01 15:43:12,184: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:43:12,189: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:43:12,194: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:43:13,308: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:43:13,315: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:43:13,321: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:44:17,707: INFO: data_transformation:  Processing test batch 4/7: samples 150000-200000]
[2025-11-01 15:44:19,431: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:44:19,431: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:44:20,615: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:44:20,618: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:44:20,623: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:45:41,072: INFO: data_transformation:  Processing test batch 5/7: samples 200000-250000]
[2025-11-01 15:45:42,774: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:45:42,780: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:45:42,782: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:45:43,856: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:45:43,861: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:45:43,894: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:47:09,759: INFO: data_transformation:  Processing test batch 6/7: samples 250000-300000]
[2025-11-01 15:47:13,006: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:47:13,009: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:47:13,029: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:47:14,515: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:47:14,516: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:47:14,516: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:48:41,350: INFO: data_transformation:  Processing test batch 7/7: samples 300000-300679]
[2025-11-01 15:48:43,351: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:48:43,352: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:48:43,352: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:48:44,652: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:48:44,652: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:48:44,654: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:48:53,171: INFO: data_transformation:   All train and test batches tokenized and saved successfully!]
[2025-11-01 17:23:59,901: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 17:23:59,915: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 17:23:59,922: INFO: common: created directory at: artifacts]
[2025-11-01 17:23:59,926: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 17:24:04,047: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-01 17:24:04,049: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 17:24:04,141: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-01 17:24:04,204: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-01 17:24:04,470: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-01 17:24:04,752: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-01 17:24:05,135: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-01 17:24:05,426: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-01 17:24:05,720: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-01 17:24:06,017: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-01 17:24:06,306: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-01 17:24:06,592: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-01 17:24:06,887: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-01 17:24:07,208: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-01 17:24:07,606: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-01 17:24:08,020: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-01 17:24:08,377: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-01 17:24:08,751: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-01 17:24:09,130: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-01 17:24:09,867: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-01 17:24:10,561: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-01 17:24:10,882: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-01 17:24:11,415: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-01 17:24:11,894: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-01 17:24:12,425: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-01 17:24:12,775: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-01 17:24:13,048: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-01 17:24:13,397: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-01 17:24:13,758: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-01 17:24:14,276: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-01 17:24:14,708: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-01 17:24:15,064: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-01 17:24:15,418: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-01 17:24:15,698: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-01 17:24:16,089: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-01 17:24:16,469: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-01 17:24:16,910: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-01 17:24:17,393: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-01 17:24:17,795: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-01 17:24:18,075: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-01 17:24:18,463: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-01 17:24:18,860: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-01 17:24:19,265: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-01 17:24:19,672: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-01 17:24:20,118: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-01 17:24:20,385: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-01 17:24:20,808: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-01 17:24:21,206: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-01 17:24:21,897: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-01 17:24:22,224: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-01 17:24:22,618: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-01 17:24:22,922: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-01 17:24:23,308: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-01 17:24:23,711: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-01 17:24:24,077: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-01 17:24:24,425: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-01 17:24:24,819: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-01 17:24:25,121: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-01 17:24:25,124: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-01 17:24:25,448: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-01 17:24:25,800: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-01 17:24:26,125: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-01 17:24:26,475: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-01 17:24:26,856: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-01 17:24:27,235: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-01 17:24:27,409: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-01 17:24:27,524: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-01 17:24:27,529: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-01 17:24:27,866: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 17:24:27,869: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 17:24:27,870: INFO: common: created directory at: artifacts]
[2025-11-01 17:24:27,871: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 17:24:27,876: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 17:24:27,876: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-01 17:26:12,003: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 17:26:12,009: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 17:26:12,011: INFO: common: created directory at: artifacts]
[2025-11-01 17:26:12,016: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 17:26:16,215: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-01 17:26:16,218: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 17:26:16,305: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-01 17:26:16,574: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-01 17:26:16,811: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-01 17:26:17,064: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-01 17:26:17,301: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-01 17:26:17,502: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-01 17:26:17,720: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-01 17:26:17,931: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-01 17:26:18,151: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-01 17:26:18,428: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-01 17:26:18,716: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-01 17:26:18,968: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-01 17:26:19,238: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-01 17:26:19,452: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-01 17:26:19,663: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-01 17:26:19,892: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-01 17:26:20,134: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-01 17:26:20,345: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-01 17:26:20,563: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-01 17:26:20,852: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-01 17:26:21,086: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-01 17:26:21,372: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-01 17:26:21,620: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-01 17:26:21,919: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-01 17:26:22,197: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-01 17:26:22,427: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-01 17:26:22,675: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-01 17:26:22,885: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-01 17:26:23,127: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-01 17:26:23,322: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-01 17:26:23,529: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-01 17:26:23,748: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-01 17:26:24,039: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-01 17:26:24,362: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-01 17:26:24,602: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-01 17:26:24,856: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-01 17:26:25,110: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-01 17:26:25,348: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-01 17:26:25,547: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-01 17:26:25,828: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-01 17:26:26,082: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-01 17:26:26,320: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-01 17:26:26,580: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-01 17:26:26,837: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-01 17:26:27,115: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-01 17:26:27,359: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-01 17:26:27,624: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-01 17:26:27,954: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-01 17:26:28,244: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-01 17:26:28,475: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-01 17:26:28,794: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-01 17:26:29,016: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-01 17:26:29,265: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-01 17:26:29,573: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-01 17:26:29,974: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-01 17:26:30,304: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-01 17:26:30,341: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-01 17:26:30,810: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-01 17:26:31,080: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-01 17:26:31,312: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-01 17:26:31,512: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-01 17:26:31,721: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-01 17:26:31,977: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-01 17:26:32,004: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-01 17:26:32,072: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-01 17:26:32,072: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-01 17:26:32,363: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 17:26:32,368: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 17:26:32,370: INFO: common: created directory at: artifacts]
[2025-11-01 17:26:32,372: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 17:26:32,376: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 17:26:32,384: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-01 17:26:40,971: INFO: model_trainer:  Training model...]
[2025-11-01 19:56:35,393: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 19:56:35,406: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 19:56:35,409: INFO: common: created directory at: artifacts]
[2025-11-01 19:56:35,412: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 19:56:40,183: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-01 19:56:40,186: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 19:56:40,320: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-01 19:56:40,681: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-01 19:56:40,950: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-01 19:56:41,204: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-01 19:56:41,483: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-01 19:56:41,752: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-01 19:56:42,023: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-01 19:56:42,254: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-01 19:56:42,529: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-01 19:56:42,790: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-01 19:56:43,062: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-01 19:56:43,323: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-01 19:56:43,588: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-01 19:56:43,858: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-01 19:56:44,145: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-01 19:56:44,439: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-01 19:56:44,675: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-01 19:56:44,958: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-01 19:56:45,247: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-01 19:56:45,518: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-01 19:56:45,823: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-01 19:56:46,068: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-01 19:56:46,372: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-01 19:56:46,617: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-01 19:56:46,926: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-01 19:56:47,170: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-01 19:56:47,455: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-01 19:56:47,685: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-01 19:56:47,985: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-01 19:56:48,254: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-01 19:56:48,501: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-01 19:56:48,837: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-01 19:56:49,049: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-01 19:56:49,374: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-01 19:56:49,708: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-01 19:56:49,949: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-01 19:56:50,213: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-01 19:56:50,504: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-01 19:56:50,780: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-01 19:56:51,008: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-01 19:56:51,241: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-01 19:56:51,498: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-01 19:56:51,737: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-01 19:56:51,983: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-01 19:56:52,199: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-01 19:56:52,458: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-01 19:56:52,713: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-01 19:56:52,958: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-01 19:56:53,228: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-01 19:56:53,484: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-01 19:56:53,718: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-01 19:56:53,987: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-01 19:56:54,268: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-01 19:56:54,503: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-01 19:56:54,742: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-01 19:56:55,003: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-01 19:56:55,004: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-01 19:56:55,244: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-01 19:56:55,483: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-01 19:56:55,753: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-01 19:56:56,018: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-01 19:56:56,279: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-01 19:56:56,551: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-01 19:56:56,592: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-01 19:56:56,672: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-01 19:56:56,674: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-01 19:56:57,102: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 19:56:57,109: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 19:56:57,112: INFO: common: created directory at: artifacts]
[2025-11-01 19:56:57,116: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 19:56:57,121: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 19:56:57,126: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-01 19:57:04,919: INFO: model_trainer:  Training model...]
[2025-11-01 20:07:14,774: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 20:07:14,781: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 20:07:14,784: INFO: common: created directory at: artifacts]
[2025-11-01 20:07:14,787: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 20:07:17,822: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-01 20:07:17,825: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 20:07:17,884: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-01 20:07:18,216: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-01 20:07:18,447: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-01 20:07:18,706: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-01 20:07:18,960: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-01 20:07:19,199: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-01 20:07:19,445: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-01 20:07:19,692: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-01 20:07:19,960: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-01 20:07:20,221: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-01 20:07:20,466: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-01 20:07:20,726: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-01 20:07:20,935: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-01 20:07:21,145: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-01 20:07:21,419: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-01 20:07:21,656: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-01 20:07:21,894: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-01 20:07:22,118: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-01 20:07:22,347: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-01 20:07:22,590: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-01 20:07:22,845: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-01 20:07:23,091: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-01 20:07:23,300: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-01 20:07:23,541: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-01 20:07:23,778: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-01 20:07:24,017: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-01 20:07:24,259: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-01 20:07:24,511: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-01 20:07:24,762: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-01 20:07:25,019: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-01 20:07:25,241: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-01 20:07:25,496: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-01 20:07:25,700: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-01 20:07:25,939: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-01 20:07:26,197: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-01 20:07:26,460: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-01 20:07:26,710: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-01 20:07:26,976: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-01 20:07:27,233: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-01 20:07:27,490: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-01 20:07:27,747: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-01 20:07:27,998: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-01 20:07:28,223: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-01 20:07:28,481: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-01 20:07:28,741: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-01 20:07:29,014: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-01 20:07:29,279: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-01 20:07:29,533: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-01 20:07:29,792: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-01 20:07:30,051: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-01 20:07:30,298: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-01 20:07:30,536: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-01 20:07:30,772: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-01 20:07:31,032: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-01 20:07:31,289: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-01 20:07:31,531: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-01 20:07:31,532: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-01 20:07:31,752: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-01 20:07:31,979: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-01 20:07:32,207: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-01 20:07:32,438: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-01 20:07:32,668: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-01 20:07:32,898: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-01 20:07:32,923: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-01 20:07:32,984: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-01 20:07:32,985: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-01 20:07:33,177: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 20:07:33,180: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 20:07:33,181: INFO: common: created directory at: artifacts]
[2025-11-01 20:07:33,182: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 20:07:33,184: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 20:07:33,185: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-01 20:07:41,114: INFO: model_trainer:  Training model...]
[2025-11-01 20:08:58,604: INFO: rouge_scorer: Using default tokenizer.]
[2025-11-01 20:14:28,225: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 20:14:28,234: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 20:14:28,237: INFO: common: created directory at: artifacts]
[2025-11-01 20:14:28,240: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 20:14:31,400: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-01 20:14:31,402: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 20:14:31,449: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-01 20:14:31,732: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-01 20:14:31,957: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-01 20:14:32,189: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-01 20:14:32,412: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-01 20:14:32,636: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-01 20:14:32,861: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-01 20:14:33,096: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-01 20:14:33,331: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-01 20:14:33,676: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-01 20:14:33,967: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-01 20:14:34,220: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-01 20:14:34,534: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-01 20:14:34,807: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-01 20:14:35,065: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-01 20:14:35,299: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-01 20:14:35,543: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-01 20:14:35,792: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-01 20:14:36,054: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-01 20:14:36,325: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-01 20:14:36,555: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-01 20:14:36,778: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-01 20:14:37,019: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-01 20:14:37,249: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-01 20:14:37,495: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-01 20:14:37,740: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-01 20:14:37,984: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-01 20:14:38,280: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-01 20:14:38,552: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-01 20:14:38,798: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-01 20:14:39,005: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-01 20:14:39,241: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-01 20:14:39,488: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-01 20:14:39,749: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-01 20:14:40,003: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-01 20:14:40,258: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-01 20:14:40,521: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-01 20:14:40,763: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-01 20:14:40,997: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-01 20:14:41,231: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-01 20:14:41,451: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-01 20:14:41,728: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-01 20:14:41,948: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-01 20:14:42,216: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-01 20:14:42,510: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-01 20:14:42,761: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-01 20:14:43,043: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-01 20:14:43,257: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-01 20:14:43,475: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-01 20:14:43,689: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-01 20:14:43,885: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-01 20:14:44,122: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-01 20:14:44,433: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-01 20:14:44,689: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-01 20:14:44,934: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-01 20:14:45,109: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-01 20:14:45,111: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-01 20:14:45,325: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-01 20:14:45,579: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-01 20:14:45,833: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-01 20:14:46,051: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-01 20:14:46,258: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-01 20:14:46,517: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-01 20:14:46,593: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-01 20:14:46,643: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-01 20:14:46,644: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-01 20:14:47,060: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 20:14:47,069: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 20:14:47,072: INFO: common: created directory at: artifacts]
[2025-11-01 20:14:47,074: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 20:14:47,083: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 20:14:47,092: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-01 20:14:53,461: INFO: model_trainer:  Training model...]
[2025-11-01 20:15:45,296: INFO: rouge_scorer: Using default tokenizer.]
[2025-11-01 20:16:33,227: INFO: rouge_scorer: Using default tokenizer.]
[2025-11-01 20:17:26,004: INFO: rouge_scorer: Using default tokenizer.]
[2025-11-01 20:26:14,243: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 20:26:14,265: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 20:26:14,267: INFO: common: created directory at: artifacts]
[2025-11-01 20:26:14,270: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 20:26:19,015: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-01 20:26:19,018: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 20:26:19,107: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-01 20:26:19,498: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-01 20:26:19,896: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-01 20:26:20,290: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-01 20:26:20,681: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-01 20:26:21,082: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-01 20:26:21,500: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-01 20:26:21,794: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-01 20:26:22,192: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-01 20:26:22,600: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-01 20:26:22,991: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-01 20:26:23,386: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-01 20:26:23,794: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-01 20:26:24,181: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-01 20:26:24,625: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-01 20:26:25,002: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-01 20:26:25,383: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-01 20:26:25,722: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-01 20:26:26,071: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-01 20:26:26,378: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-01 20:26:26,681: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-01 20:26:27,052: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-01 20:26:27,450: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-01 20:26:27,850: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-01 20:26:28,229: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-01 20:26:28,552: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-01 20:26:28,928: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-01 20:26:29,298: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-01 20:26:29,709: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-01 20:26:30,103: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-01 20:26:30,500: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-01 20:26:30,908: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-01 20:26:31,315: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-01 20:26:31,611: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-01 20:26:31,947: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-01 20:26:32,332: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-01 20:26:32,704: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-01 20:26:33,057: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-01 20:26:33,452: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-01 20:26:33,840: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-01 20:26:34,179: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-01 20:26:34,545: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-01 20:26:35,149: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-01 20:26:35,436: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-01 20:26:35,744: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-01 20:26:36,068: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-01 20:26:36,383: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-01 20:26:36,621: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-01 20:26:36,896: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-01 20:26:37,212: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-01 20:26:37,501: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-01 20:26:37,817: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-01 20:26:38,127: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-01 20:26:38,453: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-01 20:26:38,757: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-01 20:26:38,950: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-01 20:26:38,951: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-01 20:26:39,208: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-01 20:26:39,480: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-01 20:26:39,739: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-01 20:26:40,019: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-01 20:26:40,306: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-01 20:26:40,588: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-01 20:26:40,671: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-01 20:26:40,707: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-01 20:26:40,708: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-01 20:26:40,951: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 20:26:40,955: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 20:26:40,956: INFO: common: created directory at: artifacts]
[2025-11-01 20:26:40,957: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 20:26:40,960: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 20:26:40,961: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-02 02:42:50,858: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 02:42:50,889: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 02:42:50,895: INFO: common: created directory at: artifacts]
[2025-11-02 02:42:50,897: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 02:42:59,256: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-02 02:42:59,271: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 02:42:59,339: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-02 02:42:59,876: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-02 02:43:00,225: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-02 02:43:00,709: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-02 02:43:01,055: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-02 02:43:01,325: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-02 02:43:01,614: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-02 02:43:01,905: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-02 02:43:02,234: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-02 02:43:02,571: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-02 02:43:02,900: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-02 02:43:03,212: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-02 02:43:03,522: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-02 02:43:03,853: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-02 02:43:04,260: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-02 02:43:04,729: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-02 02:43:05,185: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-02 02:43:05,523: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-02 02:43:05,959: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-02 02:43:06,283: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-02 02:43:06,728: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-02 02:43:07,108: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-02 02:43:07,453: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-02 02:43:07,873: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-02 02:43:08,258: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-02 02:43:08,584: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-02 02:43:08,843: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-02 02:43:09,158: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-02 02:43:09,419: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-02 02:43:09,685: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-02 02:43:09,978: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-02 02:43:10,260: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-02 02:43:10,637: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-02 02:43:11,038: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-02 02:43:11,383: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-02 02:43:11,735: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-02 02:43:12,353: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-02 02:43:12,749: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-02 02:43:13,092: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-02 02:43:13,418: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-02 02:43:13,705: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-02 02:43:14,008: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-02 02:43:14,323: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-02 02:43:14,662: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-02 02:43:15,103: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-02 02:43:15,441: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-02 02:43:15,745: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-02 02:43:16,091: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-02 02:43:16,345: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-02 02:43:16,568: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-02 02:43:16,776: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-02 02:43:17,012: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-02 02:43:17,260: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-02 02:43:17,675: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-02 02:43:17,907: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-02 02:43:18,102: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-02 02:43:18,160: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-02 02:43:18,460: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-02 02:43:18,871: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-02 02:43:19,185: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-02 02:43:19,543: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-02 02:43:19,924: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-02 02:43:20,323: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-02 02:43:20,407: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-02 02:43:20,426: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-02 02:43:20,426: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-02 02:43:20,734: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 02:43:20,738: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 02:43:20,740: INFO: common: created directory at: artifacts]
[2025-11-02 02:43:20,741: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-02 02:43:20,743: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-02 02:43:20,744: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-02 02:43:28,395: INFO: model_trainer:  Training model...]
[2025-11-02 03:01:43,522: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 03:01:43,539: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 03:01:43,542: INFO: common: created directory at: artifacts]
[2025-11-02 03:01:43,545: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 03:01:49,586: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-02 03:01:49,588: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 03:01:49,689: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-02 03:01:50,000: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-02 03:01:50,302: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-02 03:01:50,568: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-02 03:01:50,850: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-02 03:01:51,222: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-02 03:01:51,527: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-02 03:01:51,831: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-02 03:01:52,084: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-02 03:01:52,314: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-02 03:01:52,562: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-02 03:01:52,816: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-02 03:01:53,067: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-02 03:01:53,312: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-02 03:01:53,581: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-02 03:01:53,872: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-02 03:01:54,164: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-02 03:01:54,442: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-02 03:01:54,733: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-02 03:01:55,013: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-02 03:01:55,276: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-02 03:01:55,513: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-02 03:01:55,747: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-02 03:01:55,996: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-02 03:01:56,244: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-02 03:01:56,461: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-02 03:01:56,691: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-02 03:01:56,943: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-02 03:01:57,197: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-02 03:01:57,449: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-02 03:01:57,715: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-02 03:01:57,969: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-02 03:01:58,209: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-02 03:01:58,443: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-02 03:01:58,677: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-02 03:01:58,897: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-02 03:01:59,188: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-02 03:01:59,409: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-02 03:01:59,659: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-02 03:01:59,905: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-02 03:02:00,156: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-02 03:02:00,402: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-02 03:02:00,629: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-02 03:02:00,905: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-02 03:02:01,136: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-02 03:02:01,370: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-02 03:02:01,634: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-02 03:02:01,853: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-02 03:02:02,069: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-02 03:02:02,287: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-02 03:02:02,473: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-02 03:02:02,686: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-02 03:02:02,956: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-02 03:02:03,217: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-02 03:02:03,483: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-02 03:02:03,660: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-02 03:02:03,662: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-02 03:02:03,959: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-02 03:02:04,249: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-02 03:02:04,524: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-02 03:02:04,820: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-02 03:02:05,124: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-02 03:02:05,407: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-02 03:02:05,449: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-02 03:02:05,515: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-02 03:02:05,516: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-02 03:02:05,896: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 03:02:05,902: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 03:02:05,905: INFO: common: created directory at: artifacts]
[2025-11-02 03:02:05,911: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-02 03:02:05,920: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-02 03:02:05,922: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-02 03:02:14,956: INFO: model_trainer:  Training model...]
[2025-11-02 03:02:21,634: INFO: model_trainer:  Model saved at: artifacts\model_trainer\t5-small-trained]
[2025-11-02 04:07:53,101: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 04:07:53,107: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 04:07:53,108: INFO: common: created directory at: artifacts]
[2025-11-02 04:07:53,110: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 04:07:58,231: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-02 04:07:58,248: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 04:07:58,368: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-02 04:07:58,662: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-02 04:07:58,919: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-02 04:07:59,140: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-02 04:07:59,352: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-02 04:07:59,575: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-02 04:07:59,810: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-02 04:08:00,010: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-02 04:08:00,313: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-02 04:08:00,536: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-02 04:08:00,763: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-02 04:08:00,972: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-02 04:08:01,174: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-02 04:08:01,380: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-02 04:08:01,591: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-02 04:08:01,808: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-02 04:08:02,019: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-02 04:08:02,240: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-02 04:08:02,461: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-02 04:08:02,744: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-02 04:08:03,040: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-02 04:08:03,280: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-02 04:08:03,540: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-02 04:08:03,748: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-02 04:08:04,011: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-02 04:08:04,261: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-02 04:08:04,526: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-02 04:08:04,782: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-02 04:08:04,990: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-02 04:08:05,255: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-02 04:08:05,534: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-02 04:08:05,801: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-02 04:08:06,046: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-02 04:08:06,314: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-02 04:08:06,535: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-02 04:08:06,778: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-02 04:08:07,042: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-02 04:08:07,288: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-02 04:08:07,532: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-02 04:08:07,862: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-02 04:08:08,093: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-02 04:08:08,344: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-02 04:08:08,573: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-02 04:08:08,808: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-02 04:08:09,043: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-02 04:08:09,279: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-02 04:08:09,518: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-02 04:08:09,755: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-02 04:08:09,956: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-02 04:08:10,184: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-02 04:08:10,424: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-02 04:08:10,704: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-02 04:08:10,954: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-02 04:08:11,193: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-02 04:08:11,421: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-02 04:08:11,666: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-02 04:08:11,668: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-02 04:08:11,902: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-02 04:08:12,122: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-02 04:08:12,348: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-02 04:08:12,590: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-02 04:08:12,861: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-02 04:08:13,114: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-02 04:08:13,146: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-02 04:08:13,225: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-02 04:08:13,229: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-02 04:08:13,726: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 04:08:13,733: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 04:08:13,735: INFO: common: created directory at: artifacts]
[2025-11-02 04:08:13,739: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-02 04:08:13,756: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-02 04:08:13,757: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-02 04:08:20,676: INFO: model_trainer:  Training model...]
[2025-11-02 04:08:24,949: INFO: model_trainer:  Model saved at: artifacts\model_trainer\t5-small-trained]
[2025-11-02 06:24:28,790: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 06:24:28,792: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 06:24:28,793: INFO: common: created directory at: artifacts]
[2025-11-02 06:24:28,794: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 06:24:28,814: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/tokenizer_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E5EC980>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 57b22ab4-1585-44b2-804a-38e54661f3fb)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/tokenizer_config.json]
[2025-11-02 06:24:28,818: WARNING: _http: Retrying in 1s [Retry 1/5].]
[2025-11-02 06:24:29,825: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/tokenizer_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E50F750>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: a5ed21e3-0912-46f6-9b47-a69bd29e955b)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/tokenizer_config.json]
[2025-11-02 06:24:29,826: WARNING: _http: Retrying in 2s [Retry 2/5].]
[2025-11-02 06:24:31,862: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/tokenizer_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E50FD90>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 2329f8fb-8435-4237-8fd8-dd835cc257ae)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/tokenizer_config.json]
[2025-11-02 06:24:31,864: WARNING: _http: Retrying in 4s [Retry 3/5].]
[2025-11-02 06:24:35,902: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/tokenizer_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E600190>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: e4c6cc06-d0a4-4460-a66a-e880950664fb)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/tokenizer_config.json]
[2025-11-02 06:24:35,911: WARNING: _http: Retrying in 8s [Retry 4/5].]
[2025-11-02 06:24:43,920: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/tokenizer_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E600550>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 37609ede-b8c3-4006-9c38-5d4186a74939)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/tokenizer_config.json]
[2025-11-02 06:24:43,921: WARNING: _http: Retrying in 8s [Retry 5/5].]
[2025-11-02 06:24:51,981: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/tokenizer_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E600910>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 3b498074-ad79-4f62-bb11-cff1388bfc3d)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/tokenizer_config.json]
[2025-11-02 06:24:52,212: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E50FD90>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 153c1c8b-88bd-4315-889e-c706616279ef)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json]
[2025-11-02 06:24:52,213: WARNING: _http: Retrying in 1s [Retry 1/5].]
[2025-11-02 06:24:53,223: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E50F610>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 247afbdb-6910-49c8-a5e1-f833d9bc1672)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json]
[2025-11-02 06:24:53,227: WARNING: _http: Retrying in 2s [Retry 2/5].]
[2025-11-02 06:24:55,240: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E50F890>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 47af62e9-29f6-401f-911f-7c36b1a68ab0)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json]
[2025-11-02 06:24:55,242: WARNING: _http: Retrying in 4s [Retry 3/5].]
[2025-11-02 06:24:59,250: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E50F390>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 00557614-8764-4c29-8653-e7a0f089902b)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json]
[2025-11-02 06:24:59,253: WARNING: _http: Retrying in 8s [Retry 4/5].]
[2025-11-02 06:25:07,269: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E50F250>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 1a0dc925-cebb-4ef4-bd40-0eab9e9227af)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json]
[2025-11-02 06:25:07,270: WARNING: _http: Retrying in 8s [Retry 5/5].]
[2025-11-02 06:25:15,273: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E6142D0>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: cf105b6d-10e1-4ae9-b350-8f57d20055f8)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json]
[2025-11-02 06:25:15,658: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E617890>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 4208376f-0570-4e16-bd11-982b30f95d41)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json]
[2025-11-02 06:25:15,660: WARNING: _http: Retrying in 1s [Retry 1/5].]
[2025-11-02 06:25:16,664: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E617C50>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 55789764-4cbb-4434-8356-42889027052b)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json]
[2025-11-02 06:25:16,665: WARNING: _http: Retrying in 2s [Retry 2/5].]
[2025-11-02 06:25:18,669: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7EFE0050>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 904d174f-6fda-476f-81d2-fae872789c61)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json]
[2025-11-02 06:25:18,671: WARNING: _http: Retrying in 4s [Retry 3/5].]
[2025-11-02 06:25:22,673: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7EFE0410>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: c3d49dca-8030-4070-8bfb-9e0728c58c36)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json]
[2025-11-02 06:25:22,675: WARNING: _http: Retrying in 8s [Retry 4/5].]
[2025-11-02 06:25:30,678: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7EFE07D0>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 536f0d8e-a17c-4358-955f-c6ce338caefa)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json]
[2025-11-02 06:25:30,679: WARNING: _http: Retrying in 8s [Retry 5/5].]
[2025-11-02 06:25:38,684: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7EFE0B90>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: f83827f3-500a-4165-9e4a-4be955238f6e)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json]
[2025-11-02 06:25:38,687: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-02 06:25:38,687: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 06:25:38,702: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-02 06:25:38,940: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-02 06:25:39,148: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-02 06:25:39,356: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-02 06:25:39,573: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-02 06:25:39,786: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-02 06:25:39,987: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-02 06:25:40,201: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-02 06:25:40,410: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-02 06:25:40,608: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-02 06:25:40,822: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-02 06:25:41,033: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-02 06:25:41,247: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-02 06:25:41,464: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-02 06:25:41,687: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-02 06:25:41,908: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-02 06:25:42,119: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-02 06:25:42,336: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-02 06:25:42,557: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-02 06:25:42,755: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-02 06:25:42,978: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-02 06:25:43,197: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-02 06:25:43,414: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-02 06:25:43,628: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-02 06:25:43,851: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-02 06:25:44,067: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-02 06:25:44,284: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-02 06:25:44,504: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-02 06:25:44,887: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-02 06:25:45,126: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-02 06:25:45,416: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-02 06:25:45,787: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-02 06:25:46,165: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-02 06:25:46,482: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-02 06:25:46,854: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-02 06:25:47,191: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-02 06:25:47,517: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-02 06:25:47,751: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-02 06:25:48,101: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-02 06:25:48,682: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-02 06:25:49,045: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-02 06:25:49,400: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-02 06:25:49,696: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-02 06:25:50,058: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-02 06:25:50,301: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-02 06:25:50,594: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-02 06:25:50,850: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-02 06:25:51,128: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-02 06:25:51,411: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-02 06:25:51,690: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-02 06:25:51,949: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-02 06:25:52,224: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-02 06:25:52,526: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-02 06:25:52,741: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-02 06:25:53,075: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-02 06:25:53,386: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-02 06:25:53,389: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-02 06:25:53,644: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-02 06:25:53,926: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-02 06:25:54,306: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-02 06:25:54,613: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-02 06:25:54,944: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-02 06:25:55,194: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-02 06:25:55,212: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-02 06:25:55,244: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-02 06:25:55,245: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-02 06:25:55,635: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 06:25:55,661: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 06:25:55,677: INFO: common: created directory at: artifacts]
[2025-11-02 06:25:55,693: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-02 06:25:55,708: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-02 06:25:55,726: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-02 06:26:04,713: INFO: model_trainer:  Training model...]
[2025-11-04 04:00:48,770: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-04 04:00:48,797: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-04 04:00:48,804: INFO: common: created directory at: artifacts]
[2025-11-04 04:00:48,806: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-04 04:00:52,754: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-04 04:00:52,757: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-04 04:00:52,825: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-04 04:00:53,387: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-04 04:00:53,704: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-04 04:00:54,017: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-04 04:00:54,305: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-04 04:00:54,571: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-04 04:00:54,886: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-04 04:00:55,163: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-04 04:00:55,452: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-04 04:00:55,799: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-04 04:00:56,132: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-04 04:00:56,496: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-04 04:00:56,921: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-04 04:00:57,226: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-04 04:00:57,602: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-04 04:00:57,887: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-04 04:00:58,233: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-04 04:00:58,983: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-04 04:00:59,259: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-04 04:00:59,521: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-04 04:00:59,786: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-04 04:01:00,066: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-04 04:01:00,319: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-04 04:01:00,580: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-04 04:01:00,855: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-04 04:01:01,204: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-04 04:01:01,555: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-04 04:01:01,920: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-04 04:01:02,727: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-04 04:01:03,150: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-04 04:01:03,439: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-04 04:01:03,852: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-04 04:01:04,144: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-04 04:01:04,485: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-04 04:01:04,911: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-04 04:01:05,323: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-04 04:01:05,657: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-04 04:01:05,925: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-04 04:01:06,187: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-04 04:01:06,469: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-04 04:01:06,752: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-04 04:01:07,094: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-04 04:01:07,363: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-04 04:01:07,661: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-04 04:01:07,973: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-04 04:01:08,272: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-04 04:01:08,573: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-04 04:01:08,901: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-04 04:01:09,207: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-04 04:01:09,529: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-04 04:01:09,813: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-04 04:01:10,100: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-04 04:01:10,386: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-04 04:01:10,671: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-04 04:01:10,965: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-04 04:01:11,172: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-04 04:01:11,174: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-04 04:01:11,518: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-04 04:01:11,874: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-04 04:01:12,203: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-04 04:01:12,504: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-04 04:01:12,872: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-04 04:01:13,279: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-04 04:01:13,485: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-04 04:01:13,541: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-04 04:01:13,554: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-04 04:01:14,921: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-04 04:01:14,936: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-04 04:01:14,969: INFO: common: created directory at: artifacts]
[2025-11-04 04:01:15,006: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-04 04:01:15,024: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-04 04:01:15,053: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-04 04:01:23,221: INFO: model_trainer:  Training model...]
[2025-11-04 04:19:01,471: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-04 04:19:01,479: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-04 04:19:01,481: INFO: common: created directory at: artifacts]
[2025-11-04 04:19:01,483: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-04 04:19:07,180: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-04 04:19:07,182: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-04 04:19:07,227: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-04 04:19:07,560: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-04 04:19:07,826: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-04 04:19:08,103: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-04 04:19:08,360: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-04 04:19:08,622: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-04 04:19:08,903: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-04 04:19:09,125: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-04 04:19:09,327: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-04 04:19:09,552: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-04 04:19:09,752: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-04 04:19:10,046: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-04 04:19:10,294: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-04 04:19:10,652: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-04 04:19:10,906: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-04 04:19:11,146: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-04 04:19:11,360: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-04 04:19:11,569: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-04 04:19:11,788: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-04 04:19:12,007: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-04 04:19:12,497: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-04 04:19:12,837: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-04 04:19:13,056: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-04 04:19:13,301: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-04 04:19:13,537: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-04 04:19:13,753: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-04 04:19:14,051: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-04 04:19:14,271: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-04 04:19:14,485: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-04 04:19:14,724: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-04 04:19:14,974: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-04 04:19:15,218: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-04 04:19:15,439: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-04 04:19:15,698: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-04 04:19:15,924: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-04 04:19:16,174: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-04 04:19:16,458: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-04 04:19:16,705: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-04 04:19:16,954: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-04 04:19:17,224: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-04 04:19:17,493: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-04 04:19:17,759: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-04 04:19:18,043: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-04 04:19:18,315: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-04 04:19:18,606: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-04 04:19:18,832: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-04 04:19:19,086: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-04 04:19:19,354: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-04 04:19:19,650: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-04 04:19:19,861: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-04 04:19:20,063: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-04 04:19:20,414: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-04 04:19:20,651: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-04 04:19:20,983: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-04 04:19:21,326: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-04 04:19:21,556: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-04 04:19:21,559: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-04 04:19:21,944: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-04 04:19:22,249: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-04 04:19:22,505: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-04 04:19:22,789: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-04 04:19:23,047: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-04 04:19:23,388: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-04 04:19:23,447: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-04 04:19:23,474: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-04 04:19:23,475: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-04 04:19:31,396: INFO: model_trainer:  Training model...]
[2025-11-04 04:20:59,128: INFO: model_trainer:  Model saved at: artifacts\model_trainer\t5-small-trained]
[2025-11-04 04:23:40,077: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-04 04:23:40,092: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-04 04:23:40,094: INFO: common: created directory at: artifacts]
[2025-11-04 04:23:40,096: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-04 04:23:40,099: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-04 04:23:40,100: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-04 04:23:40,101: INFO: model_evaluation:  Loading tokenized batches from artifacts/data_transformation/test]
[2025-11-04 04:23:41,606: INFO: model_evaluation:  Loaded total 300679 samples.]
[2025-11-04 04:23:41,628: INFO: model_evaluation:  Generating predictions...]
[2025-11-04 04:25:04,304: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-04 04:25:04,308: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-04 04:25:04,309: INFO: common: created directory at: artifacts]
[2025-11-04 04:25:04,309: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-04 04:25:04,311: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-04 04:25:04,311: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-04 04:25:05,215: INFO: model_evaluation:  Loading tokenized batches from artifacts/data_transformation/test]
[2025-11-04 04:25:05,479: INFO: model_evaluation:  Loaded total 300679 samples.]
[2025-11-04 04:25:05,493: INFO: model_evaluation:  Generating predictions...]
[2025-11-04 04:25:34,762: INFO: model_evaluation: Generated 10 samples...]
[2025-11-04 04:25:48,325: INFO: model_evaluation: Generated 20 samples...]
[2025-11-04 04:25:48,634: INFO: model_evaluation:  Calculating BLEU and ROUGE scores...]
[2025-11-04 04:26:13,628: INFO: rouge_scorer: Using default tokenizer.]
[2025-11-04 04:26:13,948: INFO: model_evaluation:  BLEU Score: 0.1067]
[2025-11-04 04:26:13,948: INFO: model_evaluation:  ROUGE Scores: {'rouge1': np.float64(0.21185766509411302), 'rouge2': np.float64(0.11671939246151347), 'rougeL': np.float64(0.16757374515276446)}]
[2025-11-04 04:26:13,949: INFO: model_evaluation:  BLEU Score: 0.1067]
[2025-11-04 04:26:13,949: INFO: model_evaluation:  ROUGE Scores: {'rouge1': np.float64(0.21185766509411302), 'rouge2': np.float64(0.11671939246151347), 'rougeL': np.float64(0.16757374515276446)}]
[2025-11-04 04:26:13,950: INFO: model_evaluation:  Metrics saved to: artifacts\model_evaluation\metrics.csv]
[2025-11-04 04:26:13,971: INFO: model_evaluation:  Sample predictions saved to: artifacts\model_evaluation\predictions.csv]
[2025-11-04 10:35:55,834: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:35:57,429: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:36:18,850: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-04 10:36:18,911: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-04 10:36:18,913: INFO: common: created directory at: artifacts]
[2025-11-04 10:36:18,915: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-04 10:36:46,567: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-04 10:36:49,817: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-04 10:36:49,818: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-04 10:36:50,659: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-04 10:36:50,664: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-04 10:36:50,666: INFO: data_transformation:  Converting conversations into single prompt-response pairs...]
[2025-11-04 10:37:28,135: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-04 10:38:06,469: INFO: data_transformation:  Processed 600000 samples so far...]
[2025-11-04 10:38:39,537: INFO: data_transformation:  Processed 900000 samples so far...]
[2025-11-04 10:39:11,624: INFO: data_transformation:  Processed 1200000 samples so far...]
[2025-11-04 10:39:56,179: INFO: data_transformation:  Processed 1500000 samples so far...]
[2025-11-04 10:40:29,909: INFO: data_transformation:  Processed 1800000 samples so far...]
[2025-11-04 10:41:07,695: INFO: data_transformation:  Processed 2100000 samples so far...]
[2025-11-04 10:41:40,491: INFO: data_transformation:  Processed 2400000 samples so far...]
[2025-11-04 10:42:13,723: INFO: data_transformation:  Created 2693391 total conversation samples]
[2025-11-04 10:42:15,707: INFO: data_transformation:  Preparing test conversation pairs...]
[2025-11-04 10:42:15,714: INFO: data_transformation:  Converting conversations into single prompt-response pairs...]
[2025-11-04 10:42:48,415: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-04 10:42:48,529: INFO: data_transformation:  Created 300679 total conversation samples]
[2025-11-04 10:42:48,555: INFO: data_transformation:  Starting tokenization for train data...]
[2025-11-04 10:42:48,559: INFO: data_transformation:  Processing train batch 1/54: samples 0-50000]
[2025-11-04 10:42:59,389: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:42:59,390: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:42:59,391: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:43:01,650: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:43:01,650: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:43:01,650: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:46:05,686: INFO: data_transformation:  Processing train batch 2/54: samples 50000-100000]
[2025-11-04 10:46:10,906: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:46:10,919: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:46:10,934: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:46:13,285: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:46:13,287: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:46:13,306: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:49:22,995: INFO: data_transformation:  Processing train batch 3/54: samples 100000-150000]
[2025-11-04 10:49:29,490: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:49:29,492: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:49:29,501: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:49:31,729: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:49:31,734: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:49:31,748: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:52:29,815: INFO: data_transformation:  Processing train batch 4/54: samples 150000-200000]
[2025-11-04 10:52:37,595: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:52:37,635: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:52:37,639: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:52:40,041: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:52:40,464: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:52:40,525: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:55:16,600: INFO: data_transformation:  Processing train batch 5/54: samples 200000-250000]
[2025-11-04 10:55:24,846: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:55:24,849: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:55:24,865: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:55:28,450: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:55:28,885: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:55:28,921: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:58:28,830: INFO: data_transformation:  Processing train batch 6/54: samples 250000-300000]
[2025-11-04 10:58:34,887: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:58:34,887: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:58:34,887: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 10:58:37,094: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:58:37,095: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 10:58:37,095: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:01:53,394: INFO: data_transformation:  Processing train batch 7/54: samples 300000-350000]
[2025-11-04 11:01:58,434: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:01:58,435: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:01:59,987: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:01:59,987: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:02:00,005: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:05:01,687: INFO: data_transformation:  Processing train batch 8/54: samples 350000-400000]
[2025-11-04 11:05:07,572: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:05:07,573: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:05:07,590: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:05:09,133: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:05:09,142: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:05:09,144: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:08:48,342: INFO: data_transformation:  Processing train batch 9/54: samples 400000-450000]
[2025-11-04 11:08:55,208: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:08:55,225: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:08:55,372: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:08:57,422: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:08:57,453: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:08:57,657: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:13:06,596: INFO: data_transformation:  Processing train batch 10/54: samples 450000-500000]
[2025-11-04 11:13:17,166: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:13:17,488: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:13:17,543: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:13:20,239: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:13:20,661: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:13:20,747: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:15:41,208: INFO: data_transformation:  Processing train batch 11/54: samples 500000-550000]
[2025-11-04 11:15:48,897: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:15:49,030: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:15:49,268: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:15:50,774: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:15:50,822: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:15:51,019: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:19:40,917: INFO: data_transformation:  Processing train batch 12/54: samples 550000-600000]
[2025-11-04 11:19:50,625: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:19:50,641: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:19:50,712: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:19:53,325: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:19:53,558: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:19:54,104: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:24:19,460: INFO: data_transformation:  Processing train batch 13/54: samples 600000-650000]
[2025-11-04 11:24:29,035: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:24:29,337: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:24:29,500: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:24:31,676: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:24:32,956: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:24:33,581: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:29:19,749: INFO: data_transformation:  Processing train batch 14/54: samples 650000-700000]
[2025-11-04 11:29:29,703: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:29:29,703: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:29:29,723: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:29:32,067: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:29:32,067: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:29:32,072: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:33:25,937: INFO: data_transformation:  Processing train batch 15/54: samples 700000-750000]
[2025-11-04 11:33:32,896: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:33:32,997: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:33:33,182: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:33:35,145: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:33:35,148: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:33:35,293: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:37:18,907: INFO: data_transformation:  Processing train batch 16/54: samples 750000-800000]
[2025-11-04 11:37:22,581: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:37:24,040: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:37:24,041: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:37:24,044: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:39:11,771: INFO: data_transformation:  Processing train batch 17/54: samples 800000-850000]
[2025-11-04 11:39:16,085: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:39:16,085: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:39:17,452: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:39:17,453: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:39:17,499: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:41:06,071: INFO: data_transformation:  Processing train batch 18/54: samples 850000-900000]
[2025-11-04 11:41:09,832: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:41:09,836: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:41:09,838: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:41:11,188: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:43:11,071: INFO: data_transformation:  Processing train batch 19/54: samples 900000-950000]
[2025-11-04 11:43:14,954: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:43:14,954: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:43:14,954: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:43:16,371: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:43:16,371: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:45:17,658: INFO: data_transformation:  Processing train batch 20/54: samples 950000-1000000]
[2025-11-04 11:45:21,160: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:45:21,217: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:45:22,513: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:45:22,513: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:45:22,513: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:47:13,457: INFO: data_transformation:  Processing train batch 21/54: samples 1000000-1050000]
[2025-11-04 11:47:16,976: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:47:16,976: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:47:18,314: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:47:18,333: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:49:10,822: INFO: data_transformation:  Processing train batch 22/54: samples 1050000-1100000]
[2025-11-04 11:49:13,577: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:49:13,596: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:49:13,599: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:49:14,674: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:49:14,679: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:49:14,682: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:50:49,711: INFO: data_transformation:  Processing train batch 23/54: samples 1100000-1150000]
[2025-11-04 11:50:52,708: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:50:52,714: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:50:52,714: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:50:53,787: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:50:53,789: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:50:53,801: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:52:34,620: INFO: data_transformation:  Processing train batch 24/54: samples 1150000-1200000]
[2025-11-04 11:52:37,697: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:52:37,698: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:52:38,755: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:52:38,768: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:52:38,773: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:54:08,129: INFO: data_transformation:  Processing train batch 25/54: samples 1200000-1250000]
[2025-11-04 11:54:11,579: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:54:11,588: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:54:11,598: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:54:12,772: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:54:13,694: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:54:13,698: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:56:01,840: INFO: data_transformation:  Processing train batch 26/54: samples 1250000-1300000]
[2025-11-04 11:56:05,557: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:56:05,619: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:56:05,667: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:56:06,748: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:56:06,757: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:56:06,758: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:58:01,823: INFO: data_transformation:  Processing train batch 27/54: samples 1300000-1350000]
[2025-11-04 11:58:06,207: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:58:06,210: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:58:06,218: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 11:58:07,860: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:58:07,877: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 11:58:07,903: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:00:20,929: INFO: data_transformation:  Processing train batch 28/54: samples 1350000-1400000]
[2025-11-04 12:00:24,862: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:00:24,862: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:00:24,869: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:00:26,181: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:00:26,182: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:00:26,190: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:03:57,870: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:03:59,219: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:04:19,943: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-04 12:04:19,953: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-04 12:04:19,957: INFO: common: created directory at: artifacts]
[2025-11-04 12:04:19,958: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-04 12:04:21,460: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-04 12:04:25,592: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-04 12:04:25,593: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-04 12:04:26,151: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-04 12:04:26,151: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-04 12:13:46,335: INFO: data_transformation:  Preparing test conversation pairs...]
[2025-11-04 12:14:39,718: INFO: data_transformation:  Starting tokenization for train data...]
[2025-11-04 12:14:39,720: INFO: data_transformation:  Processing train batch 1/36: samples 0-50000]
[2025-11-04 12:14:44,323: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:14:44,324: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:14:44,324: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:14:45,596: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:14:45,598: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:14:45,602: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:16:38,368: INFO: data_transformation:  Processing train batch 2/36: samples 50000-100000]
[2025-11-04 12:16:41,192: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:16:41,199: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:16:41,201: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:16:42,305: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:16:42,316: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:16:42,321: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:18:19,478: INFO: data_transformation:  Processing train batch 3/36: samples 100000-150000]
[2025-11-04 12:18:22,182: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:18:22,183: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:18:22,187: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:18:23,222: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:18:23,229: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:18:23,232: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:20:05,683: INFO: data_transformation:  Processing train batch 4/36: samples 150000-200000]
[2025-11-04 12:20:08,889: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:20:08,889: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:20:08,893: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:20:10,136: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:20:10,139: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:20:10,195: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:22:28,269: INFO: data_transformation:  Processing train batch 5/36: samples 200000-250000]
[2025-11-04 12:22:32,612: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:22:32,739: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:22:32,784: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:22:34,112: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:22:34,114: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:22:34,115: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:24:46,705: INFO: data_transformation:  Processing train batch 6/36: samples 250000-300000]
[2025-11-04 12:24:50,827: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:24:50,835: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:24:50,960: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:24:52,221: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:24:52,221: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:24:52,337: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:26:58,633: INFO: data_transformation:  Processing train batch 7/36: samples 300000-350000]
[2025-11-04 12:27:02,748: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:27:02,869: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:27:03,047: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:27:04,147: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:27:04,190: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:27:04,332: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:29:24,314: INFO: data_transformation:  Processing train batch 8/36: samples 350000-400000]
[2025-11-04 12:29:28,609: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:29:28,610: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:29:28,617: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:29:29,983: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:29:29,983: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:31:37,769: INFO: data_transformation:  Processing train batch 9/36: samples 400000-450000]
[2025-11-04 12:31:42,046: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:31:42,061: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:31:42,086: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:31:43,529: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:31:43,531: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:31:43,531: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:33:53,300: INFO: data_transformation:  Processing train batch 10/36: samples 450000-500000]
[2025-11-04 12:33:57,343: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:33:57,424: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:33:57,439: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:33:58,780: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:33:58,788: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:33:58,812: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:36:12,043: INFO: data_transformation:  Processing train batch 11/36: samples 500000-550000]
[2025-11-04 12:36:15,843: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:36:15,991: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:36:16,031: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:36:17,279: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:36:17,376: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:36:17,381: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:38:30,303: INFO: data_transformation:  Processing train batch 12/36: samples 550000-600000]
[2025-11-04 12:38:34,387: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:38:34,442: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:38:34,505: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:38:35,825: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:38:35,826: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:38:35,826: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:40:39,144: INFO: data_transformation:  Processing train batch 13/36: samples 600000-650000]
[2025-11-04 12:40:41,950: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:40:41,951: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:40:41,957: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:40:43,007: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:40:43,015: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:40:43,029: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:42:26,627: INFO: data_transformation:  Processing train batch 14/36: samples 650000-700000]
[2025-11-04 12:42:29,506: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:42:29,506: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:42:30,680: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:42:30,682: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:42:30,686: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:44:08,841: INFO: data_transformation:  Processing train batch 15/36: samples 700000-750000]
[2025-11-04 12:44:11,942: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:44:11,944: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:44:11,944: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:44:13,156: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:44:13,156: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:44:13,156: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:45:53,843: INFO: data_transformation:  Processing train batch 16/36: samples 750000-800000]
[2025-11-04 12:46:01,544: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:46:01,544: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:46:01,544: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:46:02,699: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:46:02,701: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:46:02,705: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:47:38,939: INFO: data_transformation:  Processing train batch 17/36: samples 800000-850000]
[2025-11-04 12:47:41,997: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:47:41,998: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:47:42,002: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:47:43,058: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:47:43,065: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:47:43,074: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:49:29,339: INFO: data_transformation:  Processing train batch 18/36: samples 850000-900000]
[2025-11-04 12:49:32,427: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:49:33,508: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:49:33,515: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:49:33,532: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:51:05,946: INFO: data_transformation:  Processing train batch 19/36: samples 900000-950000]
[2025-11-04 12:51:09,942: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:51:09,943: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:51:09,947: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:51:10,988: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:51:10,991: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:51:10,994: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:52:54,435: INFO: data_transformation:  Processing train batch 20/36: samples 950000-1000000]
[2025-11-04 12:52:57,205: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:52:57,223: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:52:57,226: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:52:59,160: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:52:59,161: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:52:59,162: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:54:35,668: INFO: data_transformation:  Processing train batch 21/36: samples 1000000-1050000]
[2025-11-04 12:54:38,543: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:54:38,544: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:54:38,578: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:54:39,615: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:54:39,615: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:54:39,615: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:56:11,057: INFO: data_transformation:  Processing train batch 22/36: samples 1050000-1100000]
[2025-11-04 12:56:14,197: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:56:14,197: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:56:14,204: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:56:15,277: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:56:15,278: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:56:15,279: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:58:09,314: INFO: data_transformation:  Processing train batch 23/36: samples 1100000-1150000]
[2025-11-04 12:58:12,777: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:58:12,777: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:58:12,789: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 12:58:14,018: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 12:58:14,020: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:00:14,219: INFO: data_transformation:  Processing train batch 24/36: samples 1150000-1200000]
[2025-11-04 13:00:18,359: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:00:18,360: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:00:18,360: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:00:19,600: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:00:19,600: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:00:19,601: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:02:15,499: INFO: data_transformation:  Processing train batch 25/36: samples 1200000-1250000]
[2025-11-04 13:02:20,033: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:02:20,034: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:02:20,055: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:02:21,328: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:02:21,329: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:02:21,331: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:04:25,258: INFO: data_transformation:  Processing train batch 26/36: samples 1250000-1300000]
[2025-11-04 13:04:29,516: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:04:29,537: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:04:29,583: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:04:30,631: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:04:30,636: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:04:30,636: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:06:28,198: INFO: data_transformation:  Processing train batch 27/36: samples 1300000-1350000]
[2025-11-04 13:06:31,330: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:06:31,331: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:06:31,335: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:06:32,418: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:06:32,420: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:06:32,428: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:08:27,321: INFO: data_transformation:  Processing train batch 28/36: samples 1350000-1400000]
[2025-11-04 13:08:30,762: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:08:30,763: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:08:31,890: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:08:31,895: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:08:31,900: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:10:40,840: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:10:42,119: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:10:48,371: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-04 13:10:48,381: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-04 13:10:48,384: INFO: common: created directory at: artifacts]
[2025-11-04 13:10:48,385: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-04 13:27:18,395: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:27:18,396: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:27:20,654: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:27:20,654: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:27:20,655: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:29:19,409: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:29:19,420: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:29:19,432: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:29:21,267: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:29:21,282: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:29:21,441: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:31:13,053: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:31:13,057: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:31:13,070: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:31:15,157: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:31:15,159: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:31:15,172: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:33:52,487: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:33:52,542: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:33:52,542: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:33:54,639: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:36:32,754: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:36:32,788: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:36:32,819: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:36:35,038: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:36:35,060: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:36:35,092: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:39:07,914: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:39:07,938: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:39:08,043: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:39:10,043: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:39:10,052: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:39:10,096: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:41:33,173: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:41:33,245: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:41:33,275: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:41:35,225: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:41:35,258: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:41:35,350: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:47:28,449: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:47:28,450: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:47:28,705: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:47:30,631: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:47:30,672: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:47:31,008: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:49:57,253: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:49:57,356: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:49:57,530: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:49:59,603: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:49:59,700: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:50:00,015: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:52:28,929: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:52:29,034: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:52:29,288: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:52:31,153: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:52:31,193: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:52:31,410: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:54:50,504: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:54:50,625: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:54:50,642: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:54:52,862: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:54:52,926: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:54:53,066: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:56:28,270: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:56:28,271: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:56:28,330: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:56:30,518: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:56:30,519: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:56:30,522: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:58:12,080: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:58:12,081: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:58:12,171: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 13:58:13,761: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:58:13,762: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 13:58:13,763: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:00:23,347: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:00:23,348: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:00:23,403: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:00:25,140: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:00:25,143: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:00:25,147: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:02:07,732: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:02:07,738: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:02:07,835: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:02:09,468: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:02:09,472: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:02:09,480: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:03:57,836: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:03:57,836: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:03:57,886: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:03:59,506: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:03:59,511: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:03:59,514: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:05:37,599: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:05:37,603: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:05:37,793: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:05:39,073: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:05:39,079: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:05:39,226: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:07:30,840: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:07:30,854: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:07:30,964: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:07:32,606: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:07:32,648: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:07:32,663: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:09:07,032: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:09:07,039: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:09:07,080: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:09:08,774: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:09:08,775: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:09:08,792: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:10:53,981: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:10:53,994: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:10:54,007: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:10:55,694: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:10:55,694: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:10:55,717: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:12:31,639: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:12:31,697: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:12:31,755: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:12:33,173: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:12:33,175: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:12:33,248: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:14:03,890: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:14:03,902: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:14:03,952: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:14:05,621: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:14:05,685: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:14:05,711: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:15:41,234: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:15:41,236: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:15:41,298: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:15:42,742: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:15:42,757: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:15:42,858: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:17:17,079: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:17:17,116: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:17:17,156: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:17:18,806: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:17:18,831: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:17:18,848: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:19:06,417: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:19:06,483: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:19:06,552: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:19:08,360: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:19:08,413: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:19:08,543: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:20:48,028: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:20:48,069: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:20:48,071: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:20:49,671: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:20:49,693: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:20:49,752: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:22:42,304: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:22:42,336: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:22:42,401: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:22:43,946: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:22:43,989: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:22:44,034: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:31:31,373: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:31:33,353: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:31:41,468: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-04 14:31:41,480: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-04 14:31:41,482: INFO: common: created directory at: artifacts]
[2025-11-04 14:31:41,482: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-04 14:32:10,841: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:32:11,621: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:32:15,770: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-04 14:32:15,774: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-04 14:32:15,775: INFO: common: created directory at: artifacts]
[2025-11-04 14:32:15,776: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-04 14:40:32,817: INFO: data_transformation:  Created 1790028 total conversation samples]
[2025-11-04 14:41:27,290: INFO: data_transformation:  Created 199575 total conversation samples]
[2025-11-04 14:41:32,096: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:41:32,096: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:41:33,588: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:41:33,588: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:41:33,588: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:43:08,556: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:43:08,557: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:43:08,558: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:43:09,839: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:43:09,844: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:43:09,845: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:44:25,096: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:44:25,096: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:44:25,097: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:44:26,239: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:44:26,248: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:44:26,251: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:45:43,751: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:45:43,780: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:45:43,789: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:45:44,747: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:45:44,755: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:45:44,758: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:47:08,998: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:47:09,008: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:47:09,092: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:47:09,982: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:47:09,991: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:47:10,078: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:48:42,740: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:48:42,741: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:48:42,741: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:48:43,838: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:48:43,848: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:48:43,848: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:50:03,117: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:50:03,121: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:50:03,141: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:50:04,016: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:50:04,017: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:50:04,025: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:51:32,874: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:51:32,874: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:51:32,915: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:51:33,945: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:51:33,948: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:51:33,958: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:52:52,996: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:52:53,002: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:52:53,049: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:52:55,857: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:52:55,859: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:52:55,860: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:54:34,450: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:54:34,483: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:54:34,510: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:54:35,523: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:54:35,525: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:54:35,567: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:56:22,654: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:56:22,729: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:56:22,765: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:56:24,622: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:56:24,659: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:56:24,837: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:58:10,096: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:58:10,096: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:58:10,119: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 14:58:11,295: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 14:58:12,006: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:00:05,432: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:00:05,432: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:00:05,433: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:00:07,010: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:00:07,087: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:00:07,191: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:01:44,050: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:01:44,050: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:01:44,051: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:01:45,554: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:01:45,557: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:01:45,558: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:03:28,565: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:03:28,565: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:03:28,581: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:03:29,745: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:03:29,748: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:03:29,753: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:05:04,886: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:05:04,898: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:05:04,915: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:05:05,983: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:05:05,991: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:05:05,996: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:06:36,596: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:06:36,596: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:06:36,630: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:06:37,698: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:06:37,704: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:06:37,705: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:08:15,391: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:08:15,392: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:08:15,406: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:08:16,588: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:08:16,590: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:08:16,592: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:09:44,939: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:09:44,940: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:09:45,029: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:09:46,059: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:09:46,061: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:09:46,061: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:11:25,746: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:11:25,746: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:11:25,750: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-04 15:11:26,854: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-04 15:11:26,856: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:13:56,801: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:13:57,012: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:13:57,122: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:13:58,644: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:13:58,895: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:13:58,989: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:16:44,972: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:16:44,972: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:16:44,972: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:16:48,188: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:16:48,188: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:18:31,124: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:18:31,125: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:18:31,172: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:18:32,917: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:18:33,155: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:18:33,310: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:20:28,340: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:20:28,341: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:20:28,341: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:20:29,650: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:20:29,651: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:20:29,655: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:22:38,721: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:22:38,722: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:22:38,723: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:22:40,078: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:22:40,080: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:22:40,082: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:24:36,732: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:24:36,733: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:24:36,796: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:24:38,287: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:24:38,287: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:24:38,287: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:26:33,568: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:26:33,569: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:26:34,845: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:26:34,845: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:26:34,846: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:28:19,627: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:28:19,629: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:28:19,733: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:28:20,852: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:28:20,853: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:28:20,853: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:30:09,834: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:30:09,835: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:30:09,842: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:30:10,958: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:30:10,959: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:30:10,960: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:31:50,785: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:31:50,785: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:31:50,787: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:31:52,291: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:31:52,292: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:31:52,294: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:34:41,577: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:34:41,584: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:34:41,588: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:34:43,023: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:34:43,030: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:34:43,037: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:37:27,732: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:37:27,764: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:37:29,381: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:37:29,385: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:37:29,389: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:40:06,468: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:40:06,470: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:40:06,511: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:40:08,034: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:40:08,133: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:40:08,180: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:42:43,991: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:42:44,014: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:42:44,034: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:42:45,407: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:42:45,410: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:42:45,411: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:45:20,575: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:45:20,605: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:45:20,610: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:45:21,804: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:45:21,831: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:45:21,835: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:47:55,967: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:47:55,968: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:47:56,007: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:47:57,381: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:47:57,382: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:47:57,405: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:50:13,638: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:50:13,641: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:50:13,642: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:50:17,873: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:50:17,873: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:53:01,159: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:53:01,178: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:53:01,180: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:53:02,418: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:53:02,447: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:53:02,469: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:55:24,165: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:55:24,211: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:55:24,212: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:55:25,450: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:55:25,450: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:55:25,480: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:57:56,308: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:57:56,328: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:57:56,335: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-05 15:57:57,588: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:57:57,596: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 15:57:57,601: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-05 16:35:56,692: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-05 16:35:56,718: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-05 16:35:56,722: INFO: common: created directory at: artifacts]
[2025-11-05 16:35:56,724: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-05 16:36:04,915: WARNING: file_download: Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`]
[2025-11-05 16:38:21,745: WARNING: file_download: Error while downloading from https://huggingface.co/gpt2/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.
Trying to resume download...]
[2025-11-05 16:46:06,279: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-05 16:46:06,281: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-05 16:46:06,341: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-05 16:46:06,791: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-05 16:46:07,296: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-05 16:46:07,735: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-05 16:46:08,290: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-05 16:46:08,745: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-05 16:46:09,221: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-05 16:46:09,743: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-05 16:46:10,301: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-05 16:46:10,752: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-05 16:46:11,287: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-05 16:46:11,668: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-05 16:46:12,118: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-05 16:46:12,654: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-05 16:46:13,117: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-05 16:46:13,661: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-05 16:46:14,162: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-05 16:46:14,656: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-05 16:46:15,159: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-05 16:46:15,629: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-05 16:46:16,173: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-05 16:46:16,703: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-05 16:46:17,182: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-05 16:46:17,671: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-05 16:46:18,094: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-05 16:46:18,620: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-05 16:46:19,130: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-05 16:46:19,663: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-05 16:46:20,162: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-05 16:46:20,562: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-05 16:46:21,006: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 40028 samples]
[2025-11-05 16:46:21,534: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-05 16:46:22,014: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-05 16:46:22,528: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-05 16:46:22,996: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-05 16:46:23,511: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-05 16:46:24,044: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-05 16:46:24,632: INFO: model_trainer:  Combined total samples: 1790028]
[2025-11-05 16:46:24,634: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-05 16:46:25,103: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-05 16:46:25,579: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-05 16:46:26,069: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-05 16:46:26,588: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 49575 samples]
[2025-11-05 16:46:26,619: INFO: model_trainer:  Combined total samples: 199575]
[2025-11-05 16:46:26,620: INFO: model_trainer:  Train samples: 1790028 | Eval samples: 199575]
[2025-11-05 16:46:36,099: INFO: model_trainer:  Training model...]
[2025-11-05 20:25:20,106: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-05 20:25:20,247: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-05 20:25:20,256: INFO: common: created directory at: artifacts]
[2025-11-05 20:25:20,257: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-05 20:25:32,826: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-05 20:25:32,827: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-05 20:25:32,844: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-05 20:25:33,373: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-05 20:25:33,831: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-05 20:25:34,256: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-05 20:25:34,651: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-05 20:25:34,991: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-05 20:25:35,512: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-05 20:25:36,094: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-05 20:25:36,563: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-05 20:25:37,036: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-05 20:25:37,462: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-05 20:25:37,881: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-05 20:25:38,345: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-05 20:25:38,808: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-05 20:25:39,292: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-05 20:25:39,707: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-05 20:25:40,194: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-05 20:25:40,630: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-05 20:25:41,060: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-05 20:25:41,523: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-05 20:25:42,070: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-05 20:25:42,692: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-05 20:25:43,311: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-05 20:25:43,770: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-05 20:25:44,209: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-05 20:25:44,796: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-05 20:25:45,389: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-05 20:25:45,837: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-05 20:25:46,271: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-05 20:25:46,688: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-05 20:25:47,062: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 40028 samples]
[2025-11-05 20:25:47,481: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-05 20:25:47,910: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-05 20:25:48,320: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-05 20:25:48,737: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-05 20:25:49,145: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-05 20:25:49,637: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-05 20:25:50,313: INFO: model_trainer:  Combined total samples: 1790028]
[2025-11-05 20:25:50,317: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-05 20:25:50,888: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-05 20:25:51,506: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-05 20:25:51,979: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-05 20:25:52,830: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 49575 samples]
[2025-11-05 20:25:53,547: INFO: model_trainer:  Combined total samples: 199575]
[2025-11-05 20:25:53,560: INFO: model_trainer:  Train samples: 1790028 | Eval samples: 199575]
[2025-11-05 20:26:02,434: INFO: model_trainer:  Training model...]
[2025-11-05 20:28:30,193: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-05 20:28:30,201: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-05 20:28:30,203: INFO: common: created directory at: artifacts]
[2025-11-05 20:28:30,205: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-05 20:28:41,949: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-05 20:28:41,950: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-05 20:28:41,959: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-05 20:28:42,347: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-05 20:28:42,712: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-05 20:28:43,078: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-05 20:28:43,398: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-05 20:28:43,692: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-05 20:28:44,025: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-05 20:28:44,386: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-05 20:28:44,725: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-05 20:28:45,052: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-05 20:28:45,415: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-05 20:28:45,758: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-05 20:28:46,088: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-05 20:28:46,443: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-05 20:28:46,806: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-05 20:28:47,161: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-05 20:28:47,546: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-05 20:28:47,914: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-05 20:28:48,484: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-05 20:28:48,865: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-05 20:28:49,234: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-05 20:28:49,579: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-05 20:28:49,926: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-05 20:28:50,260: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-05 20:28:50,594: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-05 20:28:50,943: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-05 20:28:51,278: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-05 20:28:51,610: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-05 20:28:51,952: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-05 20:28:52,312: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-05 20:28:52,579: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 40028 samples]
[2025-11-05 20:28:52,924: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-05 20:28:53,284: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-05 20:28:53,620: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-05 20:28:53,957: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-05 20:28:54,315: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-05 20:28:54,648: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-05 20:28:55,235: INFO: model_trainer:  Combined total samples: 1790028]
[2025-11-05 20:28:55,236: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-05 20:28:55,607: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-05 20:28:55,950: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-05 20:28:56,288: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-05 20:28:56,637: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 49575 samples]
[2025-11-05 20:28:56,691: INFO: model_trainer:  Combined total samples: 199575]
[2025-11-05 20:28:56,693: INFO: model_trainer:  Train samples: 1790028 | Eval samples: 199575]
[2025-11-05 20:29:02,094: INFO: model_trainer:  Training model...]
[2025-11-05 20:32:11,330: INFO: model_trainer:  Model saved at: artifacts\model_trainer\gpt2-trained]
[2025-11-05 20:50:55,816: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-05 20:50:55,859: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-05 20:50:55,861: INFO: common: created directory at: artifacts]
[2025-11-05 20:50:55,863: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-05 20:50:55,866: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\gpt2-trained]
[2025-11-05 20:50:55,868: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-05 20:50:56,760: INFO: model_evaluation:  Loading tokenized batches from artifacts/data_transformation/test]
[2025-11-05 20:50:58,828: INFO: model_evaluation:  Loaded total 199575 samples.]
[2025-11-05 20:50:58,878: INFO: model_evaluation:  Generating predictions...]
[2025-11-06 03:39:31,491: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 03:39:31,497: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 03:39:31,498: INFO: common: created directory at: artifacts]
[2025-11-06 03:39:31,499: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-06 03:39:31,500: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\gpt2-trained]
[2025-11-06 03:39:31,501: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-06 03:39:32,545: INFO: model_evaluation:  Loading tokenized batches from artifacts/data_transformation/test]
[2025-11-06 03:39:33,444: INFO: model_evaluation:  Loaded total 199575 samples.]
[2025-11-06 03:39:33,522: INFO: model_evaluation:  Generating predictions...]
[2025-11-06 03:43:36,206: INFO: model_evaluation: Generated 10 samples...]
[2025-11-06 03:47:51,401: INFO: model_evaluation: Generated 20 samples...]
[2025-11-06 03:48:08,440: INFO: model_evaluation:  Calculating BLEU and ROUGE scores...]
[2025-11-06 03:48:36,567: INFO: rouge_scorer: Using default tokenizer.]
[2025-11-06 03:48:37,147: INFO: model_evaluation:  BLEU Score: 0.0643]
[2025-11-06 03:48:37,148: INFO: model_evaluation:  ROUGE Scores: {'rouge1': np.float64(0.15953793394898563), 'rouge2': np.float64(0.057768727872662795), 'rougeL': np.float64(0.11872108037043587)}]
[2025-11-06 03:48:37,149: INFO: model_evaluation:  BLEU Score: 0.0643]
[2025-11-06 03:48:37,149: INFO: model_evaluation:  ROUGE Scores: {'rouge1': np.float64(0.15953793394898563), 'rouge2': np.float64(0.057768727872662795), 'rougeL': np.float64(0.11872108037043587)}]
[2025-11-06 03:48:37,151: INFO: model_evaluation:  Metrics saved to: artifacts\model_evaluation\metrics.csv]
[2025-11-06 03:48:37,228: INFO: model_evaluation:  Sample predictions saved to: artifacts\model_evaluation\predictions.csv]
[2025-11-06 04:02:40,164: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:02:40,971: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:03:02,629: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:03:04,055: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:03:04,062: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:03:04,068: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:03:04,069: INFO: common: created directory at: artifacts]
[2025-11-06 04:03:04,070: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:03:04,071: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:03:16,815: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:03:17,045: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:07:54,844: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 04:08:08,330: INFO: data_ingestion:  Dataset successfully downloaded, split, and saved in Arrow format.]
[2025-11-06 04:08:08,595: INFO: main: Stage Data Ingestion stage Completed]
[2025-11-06 04:08:08,596: INFO: main: stage Data Transformation stage initiated]
[2025-11-06 04:08:08,605: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:08:08,608: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:08:08,609: INFO: common: created directory at: artifacts]
[2025-11-06 04:08:08,611: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-06 04:16:15,033: INFO: data_transformation:  Created 1790028 total conversation samples]
[2025-11-06 04:17:06,651: INFO: data_transformation:  Created 199575 total conversation samples]
[2025-11-06 04:17:11,792: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:17:11,793: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:17:11,793: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:17:13,357: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:17:13,357: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:17:13,782: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:17:54,824: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:17:55,194: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:17:55,339: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:17:58,214: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:17:58,220: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:17:58,221: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:17:58,226: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:17:58,228: INFO: common: created directory at: artifacts]
[2025-11-06 04:17:58,229: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:17:58,230: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:17:58,231: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:17:58,232: INFO: common: created directory at: artifacts]
[2025-11-06 04:17:58,232: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:17:58,233: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:17:58,233: INFO: common: created directory at: artifacts]
[2025-11-06 04:17:58,233: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:17:58,234: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:17:58,235: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:18:15,715: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:18:15,718: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:18:15,718: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:18:16,035: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:18:16,037: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:18:17,968: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:18:17,973: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:18:22,011: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:18:22,011: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:18:23,109: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:18:52,079: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:18:52,095: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:18:54,707: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:18:54,710: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:18:54,710: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:18:54,713: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:18:54,713: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:18:54,714: INFO: common: created directory at: artifacts]
[2025-11-06 04:18:54,714: INFO: common: created directory at: artifacts]
[2025-11-06 04:18:54,715: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:18:54,715: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:18:54,716: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:18:54,716: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:19:13,834: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:19:13,838: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:19:14,175: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:19:14,175: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:19:15,861: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:19:19,484: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:19:19,484: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:19:20,583: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:19:20,583: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:19:50,723: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:19:50,744: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:19:53,384: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:19:53,390: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:19:53,392: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:19:53,393: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:19:53,393: INFO: common: created directory at: artifacts]
[2025-11-06 04:19:53,393: INFO: common: created directory at: artifacts]
[2025-11-06 04:19:53,394: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:19:53,395: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:19:53,396: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:19:53,397: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:20:05,017: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:20:05,017: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:20:05,188: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:20:06,940: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:20:06,945: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:20:10,464: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:20:10,464: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:20:11,525: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:20:11,525: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:20:43,744: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:20:43,982: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:20:46,629: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:20:46,634: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:20:46,634: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:20:46,636: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:20:46,638: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:20:46,639: INFO: common: created directory at: artifacts]
[2025-11-06 04:20:46,640: INFO: common: created directory at: artifacts]
[2025-11-06 04:20:46,641: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:20:46,641: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:20:46,642: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:20:46,642: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:20:57,616: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:20:57,626: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:20:58,152: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:20:58,153: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:21:00,372: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:21:00,370: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:21:04,781: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:21:04,781: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:21:06,001: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:21:06,004: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:21:38,287: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:21:38,327: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:21:41,042: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:21:41,046: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:21:41,046: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:21:41,048: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:21:41,050: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:21:41,051: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:21:41,051: INFO: common: created directory at: artifacts]
[2025-11-06 04:21:41,051: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:21:41,052: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:21:41,052: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:21:52,428: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:21:52,430: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:21:52,625: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:21:52,625: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:21:54,945: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:21:58,423: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:21:59,543: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:21:59,543: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:22:29,204: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:22:29,264: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:22:31,837: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:22:31,840: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:22:31,840: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:22:31,843: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:22:31,844: INFO: common: created directory at: artifacts]
[2025-11-06 04:22:31,844: INFO: common: created directory at: artifacts]
[2025-11-06 04:22:31,845: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:22:31,846: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:22:31,846: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:22:31,848: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:22:42,793: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:22:42,817: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:22:43,135: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:22:43,137: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:22:45,570: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:22:45,604: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:22:49,758: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:22:49,759: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:22:50,888: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:22:50,888: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:23:24,745: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:23:24,748: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:23:27,826: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:23:27,828: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:23:27,834: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:23:27,835: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:23:27,841: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:23:27,842: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:23:27,844: INFO: common: created directory at: artifacts]
[2025-11-06 04:23:27,845: INFO: common: created directory at: artifacts]
[2025-11-06 04:23:27,846: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:23:27,846: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:23:27,847: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:23:27,847: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:23:39,431: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:23:39,796: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:23:39,800: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:23:41,517: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:23:41,507: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:23:45,353: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:23:45,360: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:23:46,550: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:23:46,554: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:24:21,956: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:24:21,961: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:24:24,514: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:24:24,520: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:24:24,523: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:24:24,524: INFO: common: created directory at: artifacts]
[2025-11-06 04:24:24,525: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:24:24,526: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:24:24,527: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:24:24,528: INFO: common: created directory at: artifacts]
[2025-11-06 04:24:24,529: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:24:24,530: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:24:35,181: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:24:35,182: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:24:35,425: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:24:35,426: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:24:37,406: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:24:37,407: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:24:41,089: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:24:41,089: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:24:42,298: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:24:42,299: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:25:15,960: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:25:15,993: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:25:18,783: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:25:18,788: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:25:18,791: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:25:18,792: INFO: common: created directory at: artifacts]
[2025-11-06 04:25:18,793: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:25:18,793: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:25:18,794: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:25:18,795: INFO: common: created directory at: artifacts]
[2025-11-06 04:25:18,796: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:25:18,797: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:25:29,650: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:25:29,652: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:25:30,041: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:25:31,709: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:25:35,540: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:25:36,581: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:25:36,725: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:26:08,611: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:26:08,676: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:26:11,633: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:26:11,637: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:26:11,637: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:26:11,639: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:26:11,639: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:26:11,640: INFO: common: created directory at: artifacts]
[2025-11-06 04:26:11,640: INFO: common: created directory at: artifacts]
[2025-11-06 04:26:11,641: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:26:11,641: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:26:11,642: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:26:11,643: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:26:23,418: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:26:23,419: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:26:23,812: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:26:23,813: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:26:25,901: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:26:25,907: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:27:45,647: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:27:46,999: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:28:17,719: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:28:19,125: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:28:19,129: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:28:19,131: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:28:19,132: INFO: common: created directory at: artifacts]
[2025-11-06 04:28:19,133: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:28:19,133: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:28:31,592: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:28:31,917: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:32:27,241: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 04:32:38,625: INFO: data_ingestion:  Dataset successfully downloaded, split, and saved in Arrow format.]
[2025-11-06 04:32:38,993: INFO: main: Stage Data Ingestion stage Completed]
[2025-11-06 04:32:38,993: INFO: main: stage Data Transformation stage initiated]
[2025-11-06 04:32:39,040: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:32:39,043: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:32:39,044: INFO: common: created directory at: artifacts]
[2025-11-06 04:32:39,045: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-06 04:40:52,518: INFO: data_transformation:  Created 1790028 total conversation samples]
[2025-11-06 04:41:45,607: INFO: data_transformation:  Created 199575 total conversation samples]
[2025-11-06 04:41:50,151: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:41:50,152: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:41:50,152: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:41:51,464: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:41:51,464: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:41:51,471: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:42:30,348: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:42:30,661: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:42:30,752: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:42:33,927: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:42:33,927: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:42:33,931: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:42:33,933: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:42:33,937: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:42:33,937: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:42:33,938: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:42:33,940: INFO: common: created directory at: artifacts]
[2025-11-06 04:42:33,940: INFO: common: created directory at: artifacts]
[2025-11-06 04:42:33,941: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:42:33,942: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:42:33,943: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:42:33,943: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:42:33,943: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:42:33,945: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:42:49,429: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:42:49,430: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:42:49,432: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:42:49,696: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:42:49,696: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:42:49,697: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:42:52,249: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:42:52,248: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:42:52,261: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:42:56,049: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:42:56,050: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:42:57,147: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:42:57,147: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:43:25,621: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:43:25,663: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:43:28,182: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:43:28,187: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:43:28,191: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:43:28,192: INFO: common: created directory at: artifacts]
[2025-11-06 04:43:28,193: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:43:28,194: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:43:28,194: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:43:28,196: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:43:28,196: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:43:42,568: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:43:45,586: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:43:46,223: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:43:47,401: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:43:50,961: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:43:50,961: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:43:52,032: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:43:52,032: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:44:21,233: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:44:21,260: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:44:23,770: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:44:23,773: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:44:23,774: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:44:23,776: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:44:23,776: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:44:23,777: INFO: common: created directory at: artifacts]
[2025-11-06 04:44:23,777: INFO: common: created directory at: artifacts]
[2025-11-06 04:44:23,778: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:44:23,778: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:44:23,779: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:44:23,779: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:44:38,708: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:44:39,101: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:44:41,394: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:44:44,712: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:44:45,746: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:44:45,746: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:45:14,858: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:45:14,896: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:45:17,438: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:45:17,442: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:45:17,442: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:45:17,445: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:45:17,446: INFO: common: created directory at: artifacts]
[2025-11-06 04:45:17,446: INFO: common: created directory at: artifacts]
[2025-11-06 04:45:17,447: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:45:17,447: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:45:17,448: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:45:17,448: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:45:28,387: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:45:28,388: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:45:28,805: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:45:28,808: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:45:30,565: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:45:30,565: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:45:33,817: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:45:33,817: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:45:34,876: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:46:04,608: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:46:04,626: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:46:07,300: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:46:07,303: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:46:07,303: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:46:07,306: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:46:07,307: INFO: common: created directory at: artifacts]
[2025-11-06 04:46:07,307: INFO: common: created directory at: artifacts]
[2025-11-06 04:46:07,308: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:46:07,308: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:46:07,308: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:46:07,309: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:46:19,077: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:46:19,078: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:46:19,518: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:46:19,522: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:46:21,707: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:46:21,708: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:46:25,181: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:46:25,181: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:46:26,260: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:46:56,940: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:46:57,050: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:46:59,619: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:46:59,624: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:46:59,627: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:46:59,628: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:46:59,628: INFO: common: created directory at: artifacts]
[2025-11-06 04:46:59,629: INFO: common: created directory at: artifacts]
[2025-11-06 04:46:59,629: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:46:59,630: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:46:59,630: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:46:59,631: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:47:12,491: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:47:12,764: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:47:12,764: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:47:15,782: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:47:15,782: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:47:19,127: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:47:19,127: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:47:20,171: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:47:20,172: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:47:48,548: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:47:48,548: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:47:51,092: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:47:51,096: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:47:51,096: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:47:51,098: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:47:51,099: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:47:51,099: INFO: common: created directory at: artifacts]
[2025-11-06 04:47:51,100: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:47:51,100: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:47:51,100: INFO: common: created directory at: artifacts]
[2025-11-06 04:47:51,101: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:47:51,102: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:48:03,882: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:48:03,882: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:48:04,261: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:48:04,265: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:48:06,072: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:48:06,071: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:48:09,532: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:48:10,573: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:48:10,963: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:48:39,183: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:48:39,185: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:48:41,840: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:48:41,843: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:48:41,846: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:48:41,847: INFO: common: created directory at: artifacts]
[2025-11-06 04:48:41,847: INFO: common: created directory at: artifacts]
[2025-11-06 04:48:41,848: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:48:41,848: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:48:41,849: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:48:41,849: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:48:54,526: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:48:54,526: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:48:54,777: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:48:54,777: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:48:57,134: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:49:00,525: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:49:00,525: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:49:01,550: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:49:31,576: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:49:31,632: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:49:34,152: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:49:34,156: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:49:34,156: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:49:34,158: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:49:34,159: INFO: common: created directory at: artifacts]
[2025-11-06 04:49:34,159: INFO: common: created directory at: artifacts]
[2025-11-06 04:49:34,160: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:49:34,160: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:49:34,161: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:49:34,161: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:49:45,755: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:49:45,755: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:49:46,032: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:49:47,817: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:49:47,816: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:49:51,294: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:49:52,363: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:49:52,363: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:50:20,605: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:50:20,607: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:50:23,177: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:50:23,181: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:50:23,181: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:50:23,183: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:50:23,184: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:50:23,184: INFO: common: created directory at: artifacts]
[2025-11-06 04:50:23,185: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:50:23,185: INFO: common: created directory at: artifacts]
[2025-11-06 04:50:23,185: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:50:23,186: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:50:23,187: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:50:38,314: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:50:38,540: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:50:38,540: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:50:41,165: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:50:41,165: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:50:44,571: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:50:45,617: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:50:45,617: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:51:14,397: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:51:14,447: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:51:16,962: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:51:16,968: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:51:16,971: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:51:16,972: INFO: common: created directory at: artifacts]
[2025-11-06 04:51:16,972: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:51:16,973: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:51:16,974: INFO: common: created directory at: artifacts]
[2025-11-06 04:51:16,974: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:51:16,975: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:51:16,976: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:51:29,955: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:51:29,956: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:51:30,296: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:51:30,296: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:51:31,886: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:51:35,304: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:51:36,316: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:52:05,803: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:52:05,818: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:52:08,388: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:52:08,392: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:52:08,392: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:52:08,394: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:52:08,394: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:52:08,394: INFO: common: created directory at: artifacts]
[2025-11-06 04:52:08,395: INFO: common: created directory at: artifacts]
[2025-11-06 04:52:08,396: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:52:08,396: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:52:08,396: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:52:08,397: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:52:20,910: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:52:21,195: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:52:23,159: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:52:23,173: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:52:26,618: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:52:28,125: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:52:28,280: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:52:56,694: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:52:56,730: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:52:59,299: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:52:59,303: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:52:59,304: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:52:59,305: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:52:59,306: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:52:59,306: INFO: common: created directory at: artifacts]
[2025-11-06 04:52:59,307: INFO: common: created directory at: artifacts]
[2025-11-06 04:52:59,308: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:52:59,308: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:52:59,308: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:52:59,309: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:53:14,833: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:53:14,836: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:53:15,021: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:53:15,022: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:53:16,802: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:53:20,099: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:53:21,095: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:53:21,095: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:53:49,297: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:53:49,334: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:53:51,899: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:53:51,902: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:53:51,902: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:53:51,905: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:53:51,906: INFO: common: created directory at: artifacts]
[2025-11-06 04:53:51,906: INFO: common: created directory at: artifacts]
[2025-11-06 04:53:51,907: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:53:51,908: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:54:22,984: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:54:22,987: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:54:23,298: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:54:23,298: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:54:25,037: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:54:25,036: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:54:28,423: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:54:29,467: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:54:29,467: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:54:57,744: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:54:57,764: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:55:00,299: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:55:00,303: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:55:00,303: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:55:00,305: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:55:00,306: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:55:00,306: INFO: common: created directory at: artifacts]
[2025-11-06 04:55:00,307: INFO: common: created directory at: artifacts]
[2025-11-06 04:55:00,307: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:55:00,308: INFO: common: created directory at: artifacts/data_ingestion]
 Hugging Face Hub...]
[2025-11-06 04:55:00,308: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:55:11,748: WARNING: _http: '(ReadTimeoutError("HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)"), '(Request ID: bdc7881e-28d5-49b5-ad78-c10e5dcd101d)')' thrown while requesting HEAD https://huggingface.co/datasets/lmsys/lmsys-chat-1m/resolve/main/README.md]
[2025-11-06 04:55:11,752: WARNING: _http: Retrying in 1s [Retry 1/5].]
[2025-11-06 04:55:17,914: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:55:18,156: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:55:20,851: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:55:24,823: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:55:24,872: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:55:24,996: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:55:25,004: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:55:26,091: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:55:27,616: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:55:28,496: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:55:55,023: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:55:55,097: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:55:57,602: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:55:57,605: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:55:57,605: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:55:57,608: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:55:57,609: INFO: common: created directory at: artifacts]
[2025-11-06 04:55:57,610: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:55:57,610: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:55:57,611: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:55:57,611: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:56:08,850: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:56:08,850: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:56:09,119: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:56:09,123: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:56:11,212: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:56:11,212: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:56:14,828: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:56:14,828: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:56:15,872: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:56:15,872: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:56:43,823: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:56:46,348: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:56:46,353: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:56:46,355: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:56:46,356: INFO: common: created directory at: artifacts]
[2025-11-06 04:56:46,357: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:56:46,357: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:56:50,785: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:56:52,724: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:56:52,727: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:56:52,730: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:56:52,731: INFO: common: created directory at: artifacts]
[2025-11-06 04:56:52,732: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:56:52,733: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:56:57,681: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:56:57,922: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:57:03,654: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:57:03,921: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:57:04,099: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:57:04,096: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:57:07,583: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:57:08,603: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:57:36,850: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:57:36,894: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:57:39,467: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:57:39,473: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:57:39,474: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:57:39,479: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:57:39,481: INFO: common: created directory at: artifacts]
[2025-11-06 04:57:39,481: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:57:39,482: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:57:39,483: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:57:39,483: INFO: common: created directory at: artifacts]
[2025-11-06 04:57:39,484: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:57:39,484: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:57:50,859: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:57:51,021: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:57:51,022: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:57:52,776: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:57:56,094: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:57:56,094: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:57:57,104: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:58:26,361: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:58:26,373: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:58:28,996: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:58:28,999: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:58:28,999: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:58:29,002: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:58:29,003: INFO: common: created directory at: artifacts]
[2025-11-06 04:58:29,003: INFO: common: created directory at: artifacts]
[2025-11-06 04:58:29,004: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:58:29,004: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:58:29,004: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:58:29,005: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:58:44,124: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:58:44,125: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:58:44,382: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:58:44,383: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:58:46,385: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:58:49,908: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:58:50,914: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:58:50,914: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:59:19,149: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:59:19,208: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 04:59:21,845: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 04:59:21,849: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 04:59:21,851: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:59:21,851: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 04:59:21,852: INFO: common: created directory at: artifacts]
[2025-11-06 04:59:21,852: INFO: common: created directory at: artifacts]
[2025-11-06 04:59:21,853: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:59:21,853: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 04:59:21,854: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:59:21,854: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 04:59:34,803: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 04:59:35,066: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:59:35,067: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 04:59:36,538: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:59:36,538: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 04:59:39,987: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 04:59:41,023: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 04:59:41,023: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:00:09,324: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:00:09,359: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:00:11,868: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:00:11,872: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:00:11,872: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:00:11,874: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:00:11,874: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:00:11,876: INFO: common: created directory at: artifacts]
ta_ingestion]
[2025-11-06 05:00:11,877: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:00:11,877: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:00:11,879: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:00:22,963: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:00:22,963: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:00:23,543: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:00:25,507: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:00:28,825: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:00:28,825: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:00:29,833: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:00:30,263: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:00:58,245: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:00:58,258: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:01:00,813: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:01:00,817: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:01:00,819: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:01:00,820: INFO: common: created directory at: artifacts]
[2025-11-06 05:01:00,820: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:01:00,821: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:01:00,821: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:01:00,822: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:01:00,824: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:01:13,370: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:01:13,372: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:01:13,551: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:01:13,551: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:01:15,856: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:01:19,338: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:01:20,385: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:01:20,386: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:01:48,544: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:01:51,203: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:01:51,207: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:01:51,209: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:01:51,210: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:01:51,210: INFO: common: created directory at: artifacts]
[2025-11-06 05:01:51,211: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:01:51,211: INFO: common: created directory at: artifacts]
[2025-11-06 05:01:51,213: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:01:51,216: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:01:51,217: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:02:09,215: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:02:09,217: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:02:09,547: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:02:09,549: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:02:11,387: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:02:11,419: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:02:14,833: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:02:14,856: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:02:15,867: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:02:15,870: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:02:44,172: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:02:44,178: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:02:46,821: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:02:46,824: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:02:46,828: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:02:46,828: INFO: common: created directory at: artifacts]
[2025-11-06 05:02:46,830: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:02:46,830: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:02:46,866: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:02:57,418: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:02:57,418: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:02:57,684: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:02:59,433: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:03:02,929: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:03:03,931: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:03:03,931: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:03:32,323: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:03:32,347: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:03:34,896: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:03:34,900: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:03:34,900: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:03:34,902: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:03:34,903: INFO: common: created directory at: artifacts]
[2025-11-06 05:03:34,903: INFO: common: created directory at: artifacts]
[2025-11-06 05:03:34,904: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:03:34,904: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:03:34,905: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:03:34,905: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:03:46,400: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:03:46,400: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:03:46,637: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:03:46,638: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:03:48,565: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:03:48,565: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:03:52,001: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:03:52,001: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:03:53,065: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:03:53,065: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:04:22,961: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:04:22,983: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:04:25,165: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:04:25,166: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:04:25,169: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:04:25,169: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:04:25,171: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:04:25,171: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:04:25,172: INFO: common: created directory at: artifacts]
[2025-11-06 05:04:25,172: INFO: common: created directory at: artifacts]
[2025-11-06 05:04:25,173: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:04:25,173: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:04:25,174: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:04:25,174: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:04:35,346: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:04:35,386: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:04:35,938: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:04:35,940: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:04:38,337: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:04:38,346: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:04:42,823: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:04:42,917: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:04:44,230: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:04:44,306: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:05:19,889: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:05:20,180: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:05:23,444: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:05:23,456: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:05:23,465: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:05:23,467: INFO: common: created directory at: artifacts]
[2025-11-06 05:05:23,471: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:05:23,472: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:05:24,702: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:05:24,706: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:05:24,709: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:05:24,710: INFO: common: created directory at: artifacts]
[2025-11-06 05:05:24,711: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:05:24,711: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:05:36,267: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:05:36,267: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:05:36,493: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:05:36,493: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:05:37,430: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:05:41,510: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:05:41,510: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:05:42,671: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:05:42,671: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:06:18,188: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:06:19,131: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:06:44,527: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:06:45,865: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:06:45,872: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:06:45,877: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:06:45,879: INFO: common: created directory at: artifacts]
[2025-11-06 05:06:45,881: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:06:45,882: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:06:56,602: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:06:56,638: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:09:36,078: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 05:09:48,443: INFO: data_ingestion:  Dataset successfully downloaded, split, and saved in Arrow format.]
[2025-11-06 05:09:48,699: INFO: main: Stage Data Ingestion stage Completed]
[2025-11-06 05:09:48,700: INFO: main: stage Data Transformation stage initiated]
[2025-11-06 05:09:48,711: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:09:48,715: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:09:48,716: INFO: common: created directory at: artifacts]
[2025-11-06 05:09:48,718: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-06 05:18:01,130: INFO: data_transformation:  Created 1790028 total conversation samples]
[2025-11-06 05:18:57,477: INFO: data_transformation:  Created 199575 total conversation samples]
[2025-11-06 05:19:02,155: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:19:02,156: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:19:02,156: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:19:03,400: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:19:03,400: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:19:03,400: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:19:40,481: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:19:41,157: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:19:41,651: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:19:43,702: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:19:43,708: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:19:43,709: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:19:43,714: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:19:43,714: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:19:43,715: INFO: common: created directory at: artifacts]
[2025-11-06 05:19:43,716: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:19:43,717: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:19:43,717: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:19:43,717: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:19:43,926: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:19:43,929: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:19:43,931: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:19:43,932: INFO: common: created directory at: artifacts]
[2025-11-06 05:19:43,933: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:19:43,933: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:19:54,949: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /datasets/lmsys/lmsys-chat-1m/resolve/main/README.md (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000028F462DA7B0>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: e7e771a2-f912-4a84-9ea0-af523bd0ebb2)')' thrown while requesting HEAD https://huggingface.co/datasets/lmsys/lmsys-chat-1m/resolve/main/README.md]
[2025-11-06 05:19:54,949: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /datasets/lmsys/lmsys-chat-1m/resolve/main/README.md (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x000001A08581A7B0>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: d847f31e-2fa5-43f2-ad2d-d4b1067c7132)')' thrown while requesting HEAD https://huggingface.co/datasets/lmsys/lmsys-chat-1m/resolve/main/README.md]
[2025-11-06 05:19:54,949: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /datasets/lmsys/lmsys-chat-1m/resolve/main/README.md (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x000001490B85A7B0>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 594a2ada-ec65-472f-9d95-d09d05ab0c7b)')' thrown while requesting HEAD https://huggingface.co/datasets/lmsys/lmsys-chat-1m/resolve/main/README.md]
[2025-11-06 05:19:54,955: WARNING: _http: Retrying in 1s [Retry 1/5].]
[2025-11-06 05:19:54,956: WARNING: _http: Retrying in 1s [Retry 1/5].]
[2025-11-06 05:19:54,955: WARNING: _http: Retrying in 1s [Retry 1/5].]
[2025-11-06 05:20:12,823: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:20:12,826: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:20:12,828: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:20:13,029: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:20:13,033: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:20:14,538: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:20:14,533: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:20:14,542: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:20:18,103: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:20:18,103: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:20:19,127: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:20:19,127: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:20:48,172: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:20:48,181: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:20:50,844: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:20:50,847: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:20:50,849: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:20:50,850: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:20:50,850: INFO: common: created directory at: artifacts]
[2025-11-06 05:20:50,851: INFO: common: created directory at: artifacts]
ta_ingestion]
[2025-11-06 05:20:50,852: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:20:50,852: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:20:50,853: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:21:01,345: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:21:01,345: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:21:01,562: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:21:01,562: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:21:03,906: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:21:07,364: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:21:07,364: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:21:08,411: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:21:08,412: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:21:37,741: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:21:37,741: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:21:40,445: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:21:40,448: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:21:40,448: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:21:40,451: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:21:40,452: INFO: common: created directory at: artifacts]
[2025-11-06 05:21:40,452: INFO: common: created directory at: artifacts]
[2025-11-06 05:21:40,453: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:21:40,453: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:21:40,453: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:21:40,454: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:21:52,205: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:21:52,208: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:21:52,514: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:21:55,147: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:21:55,147: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:21:58,641: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:21:58,641: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:21:59,626: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:21:59,626: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:22:29,378: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:22:29,378: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:22:32,028: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:22:32,032: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:22:32,035: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:22:32,035: INFO: common: created directory at: artifacts]
[2025-11-06 05:22:32,036: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:22:32,037: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:22:32,037: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:22:32,038: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:22:44,540: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:22:44,540: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:22:44,750: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:22:46,647: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:22:50,315: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:22:51,334: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:22:51,335: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:23:23,024: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:23:23,074: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:23:25,802: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:23:25,808: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:23:25,808: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:23:25,812: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:23:25,812: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:23:25,813: INFO: common: created directory at: artifacts]
[2025-11-06 05:23:25,814: INFO: common: created directory at: artifacts]
[2025-11-06 05:23:25,814: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:23:25,814: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:23:25,815: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:23:25,815: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:23:37,005: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:23:37,005: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:23:37,239: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:23:37,247: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:23:39,541: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:23:43,482: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:23:43,482: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:23:44,646: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:23:44,648: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:24:20,393: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:24:20,476: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:24:23,390: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:24:23,393: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:24:23,397: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:24:23,402: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:24:23,404: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:24:23,405: INFO: common: created directory at: artifacts]
[2025-11-06 05:24:23,406: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:24:23,407: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:24:23,407: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:24:23,409: INFO: common: created directory at: artifacts]
[2025-11-06 05:24:23,410: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:24:23,412: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:24:34,627: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:24:34,627: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:24:35,497: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:24:35,511: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:24:37,068: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:24:37,064: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 46, in extract_data
    dataset["train"].save_to_disk(str(train_dir.resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:24:41,045: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:24:41,045: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:24:42,310: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:24:42,311: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:32:51,892: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:32:52,889: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:33:12,414: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:33:13,872: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:33:13,875: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:33:13,877: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:33:13,878: INFO: common: created directory at: artifacts]
[2025-11-06 05:33:13,879: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:33:13,881: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:33:22,572: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:33:22,611: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:33:22,611: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 05:37:07,084: INFO: data_ingestion:  Dataset successfully downloaded, split, and saved in Arrow format.]
[2025-11-06 05:37:07,372: INFO: main: Stage Data Ingestion stage Completed]
[2025-11-06 05:37:07,372: INFO: main: stage Data Transformation stage initiated]
[2025-11-06 05:37:07,376: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:37:07,378: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:37:07,378: INFO: common: created directory at: artifacts]
[2025-11-06 05:37:07,379: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-06 05:45:52,574: INFO: data_transformation:  Created 1790028 total conversation samples]
[2025-11-06 05:46:46,360: INFO: data_transformation:  Created 199575 total conversation samples]
[2025-11-06 05:46:51,314: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:46:51,314: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:46:51,314: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:46:52,762: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:46:52,764: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:46:52,765: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:47:32,860: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:47:33,128: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:47:33,506: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:47:36,354: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:47:36,354: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:47:36,358: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:47:36,359: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:47:36,360: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:47:36,365: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:47:36,365: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:47:36,365: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:47:36,367: INFO: common: created directory at: artifacts]
[2025-11-06 05:47:36,368: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:47:36,369: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:47:36,369: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:47:36,369: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:47:36,370: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:47:50,649: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:47:50,662: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:47:50,665: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:47:50,896: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:47:50,898: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 05:47:50,901: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:47:50,901: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:47:50,901: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 05:47:50,902: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 05:47:52,967: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 54, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:47:52,960: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 54, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:47:52,961: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 54, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:47:57,071: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:47:57,071: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:47:58,187: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:48:31,861: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:48:31,915: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:48:35,421: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:48:35,427: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:48:35,430: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:48:35,432: INFO: common: created directory at: artifacts]
[2025-11-06 05:48:35,432: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:48:35,433: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:48:35,434: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:48:35,435: INFO: common: created directory at: artifacts]
[2025-11-06 05:48:35,436: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:48:35,436: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:48:46,359: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:48:46,360: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:48:46,788: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:48:46,788: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 05:48:46,790: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:48:46,791: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 05:48:48,496: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 54, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:48:48,499: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 54, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:48:52,357: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:48:52,357: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:48:53,531: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:48:53,533: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:49:26,277: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:49:26,418: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:49:29,663: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:49:29,672: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:49:29,675: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:49:29,677: INFO: common: created directory at: artifacts]
[2025-11-06 05:49:29,679: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:49:29,684: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:49:29,848: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:49:29,854: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:49:29,858: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:49:29,859: INFO: common: created directory at: artifacts]
[2025-11-06 05:49:29,859: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:49:29,860: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:49:41,434: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:49:41,445: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:49:41,944: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:49:41,946: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 05:49:41,949: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:49:41,950: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 05:49:43,814: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 54, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:49:43,818: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 54, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:49:47,595: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:49:48,875: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:49:48,876: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:50:21,935: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:50:22,017: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:50:24,731: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:50:24,736: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:50:24,736: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:50:24,738: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:50:24,739: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:50:24,739: INFO: common: created directory at: artifacts]
[2025-11-06 05:50:24,740: INFO: common: created directory at: artifacts]
ta_ingestion]
[2025-11-06 05:50:24,742: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:50:24,742: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:50:24,743: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:50:37,043: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:50:37,044: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:50:37,358: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:50:37,359: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:50:37,360: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 05:50:37,361: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 05:50:39,302: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 54, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:50:39,314: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 54, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:50:43,407: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:50:43,407: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 05:50:44,589: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:50:44,591: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 05:51:17,973: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:51:18,049: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 05:51:20,928: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 05:51:20,933: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:51:20,935: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:51:20,936: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 05:51:20,937: INFO: common: created directory at: artifacts]
[2025-11-06 05:51:20,940: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:51:20,940: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:51:20,943: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 05:51:20,944: INFO: common: created directory at: artifacts]
[2025-11-06 05:51:20,945: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 05:51:20,947: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 05:51:32,474: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:51:32,482: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 05:51:32,759: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:51:32,760: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 05:51:32,761: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 05:51:34,224: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 54, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 05:51:34,255: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 54, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:00:31,269: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:00:34,161: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:01:26,628: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:01:28,407: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:01:28,425: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:01:28,446: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:01:28,447: INFO: common: created directory at: artifacts]
[2025-11-06 06:01:28,448: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:01:28,448: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:01:39,386: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:01:39,745: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:01:39,746: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:06:10,307: INFO: data_ingestion:  Dataset successfully downloaded, split, and saved in Arrow format.]
[2025-11-06 06:06:10,586: INFO: main: Stage Data Ingestion stage Completed]
[2025-11-06 06:06:10,586: INFO: main: stage Data Transformation stage initiated]
[2025-11-06 06:06:10,602: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:06:10,612: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:06:10,616: INFO: common: created directory at: artifacts]
[2025-11-06 06:06:10,617: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-06 06:14:24,851: INFO: data_transformation:  Created 1790028 total conversation samples]
[2025-11-06 06:15:19,200: INFO: data_transformation:  Created 199575 total conversation samples]
[2025-11-06 06:15:24,563: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:15:24,563: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:15:24,564: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:15:26,098: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:15:26,100: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:15:26,102: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:16:08,074: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:16:08,163: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:16:08,579: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:16:11,621: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:16:11,621: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:16:11,626: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:16:11,627: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:16:11,628: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:16:11,633: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:16:11,633: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:16:11,634: INFO: common: created directory at: artifacts]
[2025-11-06 06:16:11,634: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:16:11,635: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:16:11,635: INFO: common: created directory at: artifacts]
[2025-11-06 06:16:11,636: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:16:11,637: INFO: common: created directory at: artifacts]
[2025-11-06 06:16:11,637: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:16:11,638: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:16:11,639: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:16:11,641: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:16:25,601: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:16:25,821: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:16:25,822: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:16:33,127: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:16:33,130: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:16:33,331: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:16:33,331: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:16:33,332: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:16:33,332: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:16:27,836: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:16:33,457: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:16:37,928: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:16:37,928: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:16:38,973: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:16:38,973: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:17:11,084: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:17:11,120: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:17:13,815: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:17:13,820: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:17:13,823: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:17:13,824: INFO: common: created directory at: artifacts]
[2025-11-06 06:17:13,824: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:17:13,826: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:17:13,826: INFO: common: created directory at: artifacts]
[2025-11-06 06:17:13,827: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:17:13,828: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:17:13,829: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:17:24,636: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:17:24,637: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:17:24,850: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:17:24,850: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:17:24,851: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:17:26,625: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:17:26,639: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:17:30,083: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:17:31,163: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:18:00,731: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:18:00,872: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:18:03,408: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:18:03,411: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:18:03,412: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:18:03,414: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:18:03,415: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:18:03,415: INFO: common: created directory at: artifacts]
[2025-11-06 06:18:03,416: INFO: common: created directory at: artifacts]
[2025-11-06 06:18:03,416: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:18:03,417: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:18:03,418: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:18:03,418: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:18:14,088: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:18:14,361: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:18:14,362: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:18:14,362: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:18:14,363: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:18:17,052: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:18:20,550: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:18:20,550: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:18:21,617: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:18:21,618: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:18:54,204: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:18:54,260: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:18:56,904: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:18:56,908: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:18:56,911: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:18:56,911: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:18:56,912: INFO: common: created directory at: artifacts]
[2025-11-06 06:18:56,912: INFO: common: created directory at: artifacts]
[2025-11-06 06:18:56,913: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:18:56,913: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:18:56,914: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:19:08,775: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:19:09,015: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:19:09,015: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:19:09,015: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:19:09,016: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:19:11,771: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:19:15,226: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:19:16,269: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:19:16,269: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:19:50,548: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:19:50,703: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:19:53,187: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:19:53,192: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:19:53,195: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:19:53,196: INFO: common: created directory at: artifacts]
[2025-11-06 06:19:53,196: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:19:53,196: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:19:53,197: INFO: common: created directory at: artifacts]
[2025-11-06 06:19:53,197: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:19:53,198: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:19:53,199: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:20:04,861: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:20:04,861: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:20:05,087: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:20:05,087: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:20:05,088: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:20:05,088: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:20:07,759: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:20:07,755: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:20:11,158: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:20:12,206: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:20:41,693: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:20:41,856: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:20:44,426: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:20:44,432: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:20:44,434: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:20:44,435: INFO: common: created directory at: artifacts]
[2025-11-06 06:20:44,435: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:20:44,436: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:20:44,437: INFO: common: created directory at: artifacts]
[2025-11-06 06:20:44,437: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:20:44,437: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:20:44,438: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:20:57,811: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:20:57,816: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:20:58,170: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:20:58,170: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:20:58,170: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:20:58,171: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:21:00,425: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:21:00,425: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:21:03,820: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:21:03,820: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:21:04,893: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:21:04,893: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:21:34,994: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:21:35,151: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:21:37,592: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:21:37,599: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:21:37,603: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:21:37,604: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:21:37,604: INFO: common: created directory at: artifacts]
[2025-11-06 06:21:37,605: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:21:37,605: INFO: common: created directory at: artifacts]
[2025-11-06 06:21:37,606: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:21:37,607: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:21:37,607: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:21:55,187: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:21:55,535: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:21:55,535: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:21:55,539: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:21:55,539: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:21:58,517: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:22:01,931: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:22:01,931: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:22:02,987: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:22:02,988: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:22:32,206: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:22:32,496: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:22:34,803: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:22:34,803: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:22:34,807: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:22:34,807: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:22:34,810: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:22:34,810: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:22:34,812: INFO: common: created directory at: artifacts]
[2025-11-06 06:22:34,813: INFO: common: created directory at: artifacts]
[2025-11-06 06:22:34,813: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:22:34,814: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:22:34,814: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:22:34,815: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:22:47,686: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:22:47,938: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:22:47,939: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:22:47,939: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:22:50,992: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:22:54,445: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:22:54,446: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:22:55,500: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:23:28,088: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:23:28,219: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:23:30,786: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:23:30,792: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:23:30,795: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:23:30,796: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:23:30,796: INFO: common: created directory at: artifacts]
[2025-11-06 06:23:30,796: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:23:30,797: INFO: common: created directory at: artifacts]
[2025-11-06 06:23:30,797: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:23:30,797: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:23:30,798: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:23:44,244: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:23:44,246: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:23:44,592: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:23:44,592: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:23:44,594: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:23:44,594: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:23:47,921: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:23:52,325: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:23:53,410: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:24:23,199: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:24:23,358: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:24:25,778: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:24:25,783: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:24:25,787: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:24:25,788: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:24:25,788: INFO: common: created directory at: artifacts]
[2025-11-06 06:24:25,789: INFO: common: created directory at: artifacts]
[2025-11-06 06:24:25,789: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:24:25,790: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:24:25,790: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:24:25,791: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:24:35,952: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:24:35,954: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:24:36,137: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:24:36,138: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:24:36,138: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:24:36,138: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:24:38,572: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:24:38,573: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:24:42,279: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:24:42,286: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:24:43,465: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:24:43,465: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:25:17,374: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:25:17,477: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:25:20,103: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:25:20,107: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:25:20,107: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:25:20,109: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:25:20,109: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:25:20,110: INFO: common: created directory at: artifacts]
[2025-11-06 06:25:20,110: INFO: common: created directory at: artifacts]
[2025-11-06 06:25:20,111: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:25:20,111: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:25:20,113: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:25:33,332: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:25:33,333: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:25:33,542: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:25:33,543: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:25:33,543: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:25:35,478: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:25:35,477: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:25:39,221: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:25:39,221: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:25:40,292: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:25:40,292: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:26:13,417: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:26:13,448: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:26:16,061: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:26:16,065: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:26:16,065: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:26:16,068: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:26:16,069: INFO: common: created directory at: artifacts]
[2025-11-06 06:26:16,068: INFO: common: created directory at: artifacts]
[2025-11-06 06:26:16,070: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:26:16,070: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:26:16,070: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:26:16,071: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:26:26,358: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:26:26,558: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:26:26,558: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:26:29,562: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:26:34,745: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:26:35,035: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:26:35,036: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:26:36,664: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:26:36,727: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:26:37,803: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:26:39,295: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:26:40,159: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:27:08,112: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:27:08,520: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:27:10,785: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:27:10,791: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:27:10,793: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:27:10,794: INFO: common: created directory at: artifacts]
[2025-11-06 06:27:10,794: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:27:10,795: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:27:10,795: INFO: common: created directory at: artifacts]
[2025-11-06 06:27:10,796: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:27:10,796: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:27:10,797: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:27:23,943: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:27:23,943: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:27:24,232: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:27:24,232: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:27:24,233: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:27:24,233: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:27:27,095: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:27:30,573: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:27:31,633: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:27:31,633: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:28:03,837: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:28:03,865: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:28:06,641: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:28:06,646: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:28:06,650: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:28:06,652: INFO: common: created directory at: artifacts]
d successfully]
[2025-11-06 06:28:06,653: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:28:06,654: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:28:06,654: INFO: common: created directory at: artifacts]
[2025-11-06 06:28:06,655: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:28:06,655: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:28:18,035: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:28:18,035: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:28:18,242: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:28:18,242: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:28:18,243: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:28:18,243: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:28:20,185: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:28:23,528: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:28:23,528: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:28:24,568: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:28:24,568: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:28:54,358: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:28:54,397: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:28:57,003: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:28:57,006: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:28:57,007: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:28:57,009: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:28:57,009: INFO: common: created directory at: artifacts]
[2025-11-06 06:28:57,010: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:28:57,011: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:28:57,011: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:28:57,012: INFO: common: created directory at: artifacts]
[2025-11-06 06:28:57,014: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:28:57,015: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:29:08,232: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:29:08,232: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:29:08,475: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:29:08,475: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:29:08,475: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:29:08,475: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:29:11,481: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:29:11,477: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:29:14,956: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:29:14,956: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:29:16,014: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:29:48,640: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:29:48,763: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:29:51,336: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:29:51,339: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:29:51,339: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:29:51,341: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:29:51,341: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:29:51,342: INFO: common: created directory at: artifacts]
[2025-11-06 06:29:51,342: INFO: common: created directory at: artifacts]
[2025-11-06 06:29:51,343: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:29:51,344: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:29:51,344: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:29:51,344: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:30:02,758: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:30:03,022: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:30:03,022: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:30:03,022: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:30:03,023: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:30:04,945: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:30:04,948: ERROR: main: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow']
Traceback (most recent call last):
  File "E:\work\project_mlops\Chatbot1\Chatbot\main.py", line 12, in <module>
    data_ingestion_pipeline.initiate_data_ingestion()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\pipeline\data_ingestion_pipeline.py", line 15, in initiate_data_ingestion
    data_ingestion.extract_data()
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "E:\work\project_mlops\Chatbot1\Chatbot\src\Chatbot\components\data_ingestion.py", line 55, in extract_data
    dataset["train"].save_to_disk(str(Path(train_dir).resolve()))
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1632, in save_to_disk
    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):
                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_dataset.py", line 1656, in _save_to_disk_single
    writer = ArrowWriter(
        features=shard.features,
    ...<2 lines>...
        embed_local_files=True,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\datasets\arrow_writer.py", line 451, in __init__
    self.stream = self._fs.open(path, "wb")
                  ~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\spec.py", line 1310, in open
    f = self._open(
        path,
    ...<4 lines>...
        **kwargs,
    )
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 201, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 365, in __init__
    self._open()
    ~~~~~~~~~~^^
  File "C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\fsspec\implementations\local.py", line 370, in _open
    self.f = open(self.path, mode=self.mode)
             ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 22] Invalid argument: 'E:/work/project_mlops/Chatbot1/Chatbot/artifacts/data_ingestion/train/data-00000-of-00005.arrow'
[2025-11-06 06:30:08,398: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 06:30:09,494: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 06:30:47,240: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:30:47,375: WARNING: module_wrapper: From C:\Users\Abdullah Ali\AppData\Roaming\Python\Python313\site-packages\tf_keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.
]
[2025-11-06 06:30:50,109: INFO: main: stage Data Ingestion stage initiated]
[2025-11-06 06:30:50,113: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:30:50,113: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:30:50,115: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:30:50,116: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:30:50,116: INFO: common: created directory at: artifacts]
[2025-11-06 06:30:50,117: INFO: common: created directory at: artifacts]
[2025-11-06 06:30:50,117: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:30:50,119: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:30:50,119: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:30:50,120: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:31:04,788: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:31:04,790: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:31:05,043: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:31:05,044: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:31:05,045: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:31:05,046: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 06:58:44,761: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 06:58:44,775: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 06:58:44,776: INFO: common: created directory at: artifacts]
[2025-11-06 06:58:44,776: INFO: common: created directory at: artifacts/data_ingestion]
[2025-11-06 06:58:44,777: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face Hub...]
[2025-11-06 06:59:03,002: INFO: data_ingestion:  No explicit train/test found  creating 90/10 split...]
[2025-11-06 06:59:03,246: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train ...]
[2025-11-06 06:59:03,247: INFO: data_ingestion:  Saving test split to artifacts\data_ingestion\test ...]
[2025-11-06 07:04:28,737: INFO: data_ingestion:  Dataset successfully downloaded, split, and saved in Arrow format.]
[2025-11-06 07:12:19,079: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:12:20,504: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:12:27,031: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 07:12:27,033: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 07:12:27,033: INFO: common: created directory at: artifacts]
[2025-11-06 07:12:27,034: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-06 07:26:08,929: INFO: data_transformation:  Created 1790028 total conversation samples]
[2025-11-06 07:28:45,572: INFO: data_transformation:  Created 199575 total conversation samples]
[2025-11-06 07:28:53,982: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:28:53,983: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:28:53,983: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:28:56,208: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:28:56,208: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:28:56,222: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:32:18,817: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:32:18,844: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:32:18,847: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:32:20,296: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:32:20,299: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:32:20,306: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:35:08,377: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:35:08,398: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:35:08,486: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:35:09,945: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:35:10,015: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:38:03,281: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:38:03,281: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:38:05,150: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:38:05,153: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:38:05,155: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:40:01,425: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:40:01,455: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:40:01,679: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:40:02,988: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:40:02,989: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:40:03,132: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:41:57,097: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:41:57,098: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:41:57,103: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:41:58,651: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:41:58,651: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:41:58,655: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:43:42,780: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:43:42,786: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:43:42,882: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:43:44,072: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:43:44,072: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:43:44,082: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:45:47,669: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:45:47,670: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:45:47,680: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:45:49,407: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:45:49,435: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:45:49,437: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:47:39,828: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:47:39,880: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:47:39,902: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:47:41,186: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:47:41,192: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:47:41,243: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:49:24,466: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:49:24,484: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:49:24,509: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:49:26,338: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:49:26,388: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:49:26,414: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:50:57,958: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:50:57,983: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:50:58,001: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:50:59,583: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:50:59,586: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:50:59,592: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:53:00,678: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:53:00,832: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:53:02,575: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:53:02,581: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:53:02,757: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:54:50,556: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:54:50,558: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:54:50,571: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:54:51,989: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:54:51,994: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:54:52,006: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:56:38,020: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:56:38,025: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:56:38,026: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:56:39,489: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:56:39,493: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:56:39,495: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:58:40,878: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:58:40,878: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:58:40,878: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 07:58:42,437: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:58:42,437: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 07:58:42,439: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:00:26,888: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:00:26,889: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:00:26,895: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:00:28,164: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:00:28,165: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:00:28,189: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:01:55,854: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:01:55,854: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:01:56,867: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:01:56,876: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:01:56,893: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:03:53,018: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:03:53,018: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:03:53,019: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:03:54,355: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:03:54,356: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:03:54,364: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:05:18,980: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:05:18,980: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:05:20,147: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:05:20,150: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:05:20,151: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:06:52,262: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:06:52,263: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:06:52,272: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:06:53,543: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:06:53,543: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:06:53,544: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:08:18,079: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:08:18,101: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:08:18,102: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:08:19,175: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:08:19,176: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:08:19,177: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:09:32,434: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:09:32,455: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:09:32,463: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:09:33,355: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:09:33,362: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:09:33,370: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:10:58,294: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:10:58,378: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:10:58,379: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:10:59,231: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:10:59,302: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:10:59,353: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:12:23,106: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:12:23,111: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:12:23,128: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:12:24,007: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:12:24,021: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:12:24,057: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:13:57,084: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:13:57,105: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:13:57,130: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:13:57,994: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:13:58,007: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:13:58,075: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:15:29,573: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:15:29,594: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:15:29,742: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:15:30,533: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:15:30,544: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:15:30,664: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:16:57,144: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:16:57,145: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:16:58,178: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:16:58,184: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:16:58,186: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:18:19,231: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:18:19,252: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:18:19,252: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:18:20,375: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:18:20,384: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:18:20,387: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:19:43,503: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:19:43,509: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:19:43,519: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:19:44,547: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:19:44,551: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:19:44,557: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:21:14,011: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:21:14,026: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:21:14,031: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:21:15,056: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:21:15,068: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:21:15,069: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:22:41,384: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:22:41,399: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:22:41,420: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:22:42,265: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:22:42,270: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:22:42,276: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:24:12,341: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:24:12,354: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:24:12,360: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:24:13,546: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:24:13,556: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:24:13,565: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:25:33,981: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:25:33,983: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:25:34,865: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:25:34,865: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:25:34,886: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:26:54,823: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:26:54,833: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:26:54,837: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:26:55,829: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:26:55,837: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:26:55,843: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:28:19,569: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:28:19,608: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:28:19,624: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:28:20,492: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:28:20,539: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:28:20,549: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:29:40,004: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:29:40,006: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:29:40,009: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:29:41,049: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:29:41,053: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:29:41,054: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:30:50,082: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:30:50,083: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:30:50,083: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:30:51,312: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:30:51,315: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:30:51,319: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:32:10,760: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:32:10,770: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:32:10,799: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:32:11,636: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:32:11,648: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:32:11,708: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:33:29,259: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:33:29,290: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:33:29,328: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:33:30,180: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:33:30,195: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:33:30,253: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:36:19,412: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:36:19,429: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:36:19,503: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-06 08:36:20,688: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:36:20,701: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 08:36:20,777: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-06 09:01:06,668: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-06 09:01:06,682: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-06 09:01:06,685: INFO: common: created directory at: artifacts]
[2025-11-06 09:01:06,688: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-06 09:01:20,069: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-06 09:01:20,071: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-06 09:01:20,106: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-06 09:01:20,677: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-06 09:01:21,256: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-06 09:01:21,765: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-06 09:01:22,275: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-06 09:01:22,766: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-06 09:01:23,292: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-06 09:01:23,745: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-06 09:01:24,252: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-06 09:01:24,680: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-06 09:01:25,190: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-06 09:01:25,696: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-06 09:01:26,193: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-06 09:01:26,698: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-06 09:01:27,178: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-06 09:01:27,667: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-06 09:01:28,164: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-06 09:01:28,613: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-06 09:01:29,139: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-06 09:01:29,562: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-06 09:01:29,961: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-06 09:01:30,476: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-06 09:01:31,017: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-06 09:01:31,445: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-06 09:01:31,967: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-06 09:01:32,479: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-06 09:01:32,996: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-06 09:01:33,529: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-06 09:01:34,176: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-06 09:01:34,601: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-06 09:01:35,004: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 40028 samples]
[2025-11-06 09:01:35,514: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-06 09:01:35,927: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-06 09:01:36,442: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-06 09:01:36,962: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-06 09:01:37,409: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-06 09:01:37,912: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-06 09:01:38,441: INFO: model_trainer:  Combined total samples: 1790028]
[2025-11-06 09:01:38,442: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-06 09:01:38,903: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-06 09:01:39,345: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-06 09:01:39,795: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-06 09:01:40,242: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 49575 samples]
[2025-11-06 09:01:40,298: INFO: model_trainer:  Combined total samples: 199575]
[2025-11-06 09:01:40,301: INFO: model_trainer:  Train samples: 1790028 | Eval samples: 199575]
[2025-11-06 09:01:48,973: INFO: model_trainer:  Training model...]
[2025-11-07 05:46:04,930: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-07 05:46:04,955: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-07 05:46:04,958: INFO: common: created directory at: artifacts]
[2025-11-07 05:46:04,959: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-07 05:46:09,498: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-07 05:46:09,499: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-07 05:46:09,517: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-07 05:46:09,982: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-07 05:46:10,374: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-07 05:46:10,784: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-07 05:46:11,142: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-07 05:46:11,525: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-07 05:46:45,600: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-07 05:46:45,603: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-07 05:46:45,603: INFO: common: created directory at: artifacts]
[2025-11-07 05:46:45,604: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-07 05:46:48,398: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-07 05:46:48,399: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-07 05:46:48,407: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-07 05:46:48,779: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-07 05:46:49,127: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-07 05:46:49,465: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-07 05:46:49,811: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-07 05:46:50,159: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-07 05:46:50,504: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-07 05:46:51,684: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-07 05:46:52,078: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-07 05:46:52,466: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-07 05:46:52,861: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-07 05:46:53,250: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-07 05:46:53,639: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-07 05:46:54,040: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-07 05:46:54,431: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-07 05:46:54,827: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-07 05:46:55,230: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-07 05:46:55,623: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-07 05:46:56,075: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-07 05:46:56,471: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-07 05:46:56,857: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-07 05:46:57,242: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-07 05:46:57,640: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-07 05:46:58,043: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-07 05:46:58,439: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-07 05:46:58,821: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-07 05:46:59,217: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-07 05:46:59,640: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-07 05:47:00,036: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-07 05:47:00,423: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-07 05:47:00,746: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 40028 samples]
[2025-11-07 05:47:01,284: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-07 05:47:01,677: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-07 05:47:02,071: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-07 05:47:02,458: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-07 05:47:02,868: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-07 05:47:03,304: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-07 05:47:03,359: INFO: model_trainer:  Combined total samples: 1790028]
[2025-11-07 05:47:03,359: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-07 05:47:03,766: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-07 05:47:04,142: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-07 05:47:04,532: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-07 05:47:04,939: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 49575 samples]
[2025-11-07 05:47:04,946: INFO: model_trainer:  Combined total samples: 199575]
[2025-11-07 05:47:04,947: INFO: model_trainer:  Train samples: 1790028 | Eval samples: 199575]

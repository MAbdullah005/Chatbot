[2025-10-23 02:23:26,092: INFO: template: Creating Directory:.github\workflow for the .gitkeep]
[2025-10-23 02:23:26,093: INFO: template: Creating empty file : .github\workflow\.gitkeep]
[2025-10-23 02:23:26,094: INFO: template: Creating Directory:src\Chatbot for the __init__.py]
[2025-10-23 02:23:26,095: INFO: template: Creating empty file : src\Chatbot\__init__.py]
[2025-10-23 02:23:26,095: INFO: template: Creating Directory:src\Chatbot\components for the __init__.py]
[2025-10-23 02:23:26,096: INFO: template: Creating empty file : src\Chatbot\components\__init__.py]
[2025-10-23 02:23:26,098: INFO: template: Creating Directory:src\Chatbot\utils\common.pysrc\Chatbot\research\research.ipynbsrc\Chatbot\utils for the __init__.py]
[2025-10-23 02:23:26,105: INFO: template: Creating empty file : src\Chatbot\utils\common.pysrc\Chatbot\research\research.ipynbsrc\Chatbot\utils\__init__.py]
[2025-10-23 02:23:26,106: INFO: template: Creating Directory:src\Chatbot\logging for the __init__.py]
[2025-10-23 02:23:26,107: INFO: template: __init__.py is Already exists]
[2025-10-23 02:23:26,107: INFO: template: Creating Directory:src\Chatbot\config for the __init__.py]
[2025-10-23 02:23:26,108: INFO: template: Creating empty file : src\Chatbot\config\__init__.py]
[2025-10-23 02:23:26,109: INFO: template: Creating Directory:src\Chatbot\config for the configration.py]
[2025-10-23 02:23:26,110: INFO: template: Creating empty file : src\Chatbot\config\configration.py]
[2025-10-23 02:23:26,111: INFO: template: Creating Directory:src\Chatbot\pipeline for the __init__.py]
[2025-10-23 02:23:26,112: INFO: template: Creating empty file : src\Chatbot\pipeline\__init__.py]
[2025-10-23 02:23:26,113: INFO: template: Creating Directory:src\Chatbot\entity for the __init__.py]
[2025-10-23 02:23:26,114: INFO: template: Creating empty file : src\Chatbot\entity\__init__.py]
[2025-10-23 02:23:26,114: INFO: template: Creating Directory:src\Chatbot\constants for the __init__.py]
[2025-10-23 02:23:26,115: INFO: template: Creating empty file : src\Chatbot\constants\__init__.py]
[2025-10-23 02:23:26,120: INFO: template: Creating Directory:config for the config.yaml]
[2025-10-23 02:23:26,121: INFO: template: Creating empty file : config\config.yaml]
[2025-10-23 02:23:26,122: INFO: template: Creating Directory:param for the params.yaml]
[2025-10-23 02:23:26,123: INFO: template: Creating empty file : param\params.yaml]
[2025-10-23 02:23:26,123: INFO: template: Creating empty file : app.py]
[2025-10-23 02:23:26,124: INFO: template: Creating empty file : main.py]
[2025-10-23 02:23:26,126: INFO: template: Creating empty file : .gitignore]
[2025-10-23 02:23:26,127: INFO: template: Creating empty file : Dockerfile]
[2025-10-23 02:23:26,128: INFO: template: Creating empty file : .dockerignore]
[2025-10-23 02:23:26,129: INFO: template: Creating empty file : requirements.txt]
[2025-10-23 02:23:26,133: INFO: template: Creating empty file : setup.py]
[2025-10-28 01:36:45,559: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 01:36:50,856: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 01:38:38,851: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 01:38:40,251: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 01:40:17,298: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 01:40:18,738: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 01:43:18,956: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 01:43:20,239: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 01:43:28,833: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 01:43:28,849: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 01:43:28,852: INFO: common: created directory at: artifacts]
[2025-10-28 01:43:28,854: INFO: common: created directory at: artifacts/data_ingestion]
[2025-10-28 01:43:28,856: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face...]
[2025-10-28 01:46:41,296: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 01:46:43,492: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 01:46:52,120: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 01:46:52,127: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 01:46:52,129: INFO: common: created directory at: artifacts]
[2025-10-28 01:46:52,130: INFO: common: created directory at: artifacts/data_ingestion]
[2025-10-28 01:46:52,131: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face...]
[2025-10-28 01:47:05,268: INFO: data_ingestion:  Saving train split to artifacts\data_ingestion\train.parquet]
[2025-10-28 01:48:49,710: INFO: data_ingestion:  Dataset downloaded and saved successfully!]
[2025-10-28 01:48:50,067: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 01:48:51,598: INFO: data_transformation:  Loading dataset from artifacts/data_ingestion]
[2025-10-28 02:13:48,134: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 02:13:50,482: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 02:14:02,782: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 02:14:02,792: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 02:14:02,793: INFO: common: created directory at: artifacts]
[2025-10-28 02:14:02,794: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 03:13:17,221: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:13:22,441: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:13:44,405: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 03:13:44,423: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 03:13:44,424: INFO: common: created directory at: artifacts]
[2025-10-28 03:13:44,426: INFO: common: created directory at: artifacts/data_ingestion]
[2025-10-28 03:13:44,426: INFO: data_ingestion:  Loading dataset lmsys/lmsys-chat-1m from Hugging Face...]
[2025-10-28 03:14:02,207: INFO: data_ingestion:  Saving dataset to artifacts\data_ingestion in Arrow format...]
[2025-10-28 03:15:04,270: INFO: data_ingestion:  Dataset downloaded and saved successfully in Arrow format!]
[2025-10-28 03:15:04,679: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 03:15:06,283: INFO: data_transformation:  Loading dataset from artifacts/data_ingestion]
[2025-10-28 03:15:34,954: INFO: data_transformation:  Dataset loaded with 1000000 samples]
[2025-10-28 03:15:34,956: INFO: data_transformation: Converting conversations into input-target pairs...]
[2025-10-28 03:16:20,670: INFO: data_transformation: Saving batch 1...]
[2025-10-28 03:18:00,492: INFO: data_transformation: Saving batch 2...]
[2025-10-28 03:19:17,057: INFO: data_transformation: Saving batch 3...]
[2025-10-28 03:20:22,552: INFO: data_transformation: Saving batch 4...]
[2025-10-28 03:21:25,289: INFO: data_transformation: Saving batch 5...]
[2025-10-28 03:22:35,621: INFO: data_transformation: Saving batch 6...]
[2025-10-28 03:22:58,952: INFO: data_transformation:  Created total 3031290 conversation pairs]
[2025-10-28 03:22:59,020: INFO: data_transformation:  Applying tokenization in batches (memory safe)...]
[2025-10-28 03:26:47,186: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:26:50,088: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:27:08,147: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 03:27:08,158: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 03:27:08,160: INFO: common: created directory at: artifacts]
[2025-10-28 03:27:08,161: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 03:27:09,921: INFO: data_transformation:  Loading dataset from artifacts/data_ingestion]
[2025-10-28 03:27:16,739: INFO: data_transformation:  Dataset loaded with 1000000 samples]
[2025-10-28 03:27:16,741: INFO: data_transformation: Converting conversations into input-target pairs...]
[2025-10-28 03:27:34,844: INFO: data_transformation: Saving batch 1...]
[2025-10-28 03:27:59,416: INFO: data_transformation: Saving batch 2...]
[2025-10-28 03:28:28,025: INFO: data_transformation: Saving batch 3...]
[2025-10-28 03:28:56,002: INFO: data_transformation: Saving batch 4...]
[2025-10-28 03:29:26,646: INFO: data_transformation: Saving batch 5...]
[2025-10-28 03:29:57,595: INFO: data_transformation: Saving batch 6...]
[2025-10-28 03:30:29,032: INFO: data_transformation: Saving batch 7...]
[2025-10-28 03:31:02,989: INFO: data_transformation: Saving batch 8...]
[2025-10-28 03:31:36,365: INFO: data_transformation: Saving batch 9...]
[2025-10-28 03:32:06,093: INFO: data_transformation: Saving batch 10...]
[2025-10-28 03:32:14,436: INFO: data_transformation:  Created total 3031290 conversation pairs]
[2025-10-28 03:32:14,465: INFO: data_transformation:  Applying tokenization in batches (memory safe)...]
[2025-10-28 03:36:57,623: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:37:00,705: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:37:17,583: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 03:37:17,598: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 03:37:17,601: INFO: common: created directory at: artifacts]
[2025-10-28 03:37:17,603: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 03:37:19,304: INFO: data_transformation:  Loading dataset from artifacts/data_ingestion]
[2025-10-28 03:37:25,098: INFO: data_transformation:  Dataset loaded with 1000000 samples]
[2025-10-28 03:37:25,100: INFO: data_transformation: Converting conversations into input-target pairs...]
[2025-10-28 03:37:42,925: INFO: data_transformation: Saving batch 1...]
[2025-10-28 03:38:07,119: INFO: data_transformation: Saving batch 2...]
[2025-10-28 03:38:31,887: INFO: data_transformation: Saving batch 3...]
[2025-10-28 03:38:47,637: INFO: data_transformation: Saving batch 4...]
[2025-10-28 03:39:04,853: INFO: data_transformation: Saving batch 5...]
[2025-10-28 03:39:27,093: INFO: data_transformation: Saving batch 6...]
[2025-10-28 03:39:44,075: INFO: data_transformation: Saving batch 7...]
[2025-10-28 03:40:11,113: INFO: data_transformation: Saving batch 8...]
[2025-10-28 03:40:36,680: INFO: data_transformation: Saving batch 9...]
[2025-10-28 03:40:56,750: INFO: data_transformation: Saving batch 10...]
[2025-10-28 03:41:07,917: INFO: data_transformation:  Created total 3031290 conversation pairs]
[2025-10-28 03:41:07,943: INFO: data_transformation:  Applying tokenization in batches (memory safe)...]
[2025-10-28 03:41:07,955: INFO: data_transformation: Processing batch 1/61: samples 0-50000]
[2025-10-28 03:41:11,901: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:41:11,901: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:41:14,720: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:41:14,720: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:41:14,736: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:42:32,775: INFO: data_transformation: Processing batch 2/61: samples 50000-100000]
[2025-10-28 03:42:35,019: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:42:35,019: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:42:35,019: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:42:36,638: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:42:36,640: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:42:36,684: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:45:40,984: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:45:42,114: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:45:47,020: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 03:45:47,026: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 03:45:47,027: INFO: common: created directory at: artifacts]
[2025-10-28 03:45:47,028: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 03:45:48,315: INFO: data_transformation:  Loading dataset from artifacts/data_ingestion]
[2025-10-28 03:45:54,902: INFO: data_transformation:  Dataset loaded with 1000000 samples]
[2025-10-28 03:45:54,932: INFO: data_transformation: Converting conversations into input-target pairs...]
[2025-10-28 03:46:08,347: INFO: data_transformation: Saving batch 1...]
[2025-10-28 03:46:29,255: INFO: data_transformation: Saving batch 2...]
[2025-10-28 03:46:51,811: INFO: data_transformation: Saving batch 3...]
[2025-10-28 03:47:14,672: INFO: data_transformation: Saving batch 4...]
[2025-10-28 03:47:41,852: INFO: data_transformation: Saving batch 5...]
[2025-10-28 03:48:08,788: INFO: data_transformation: Saving batch 6...]
[2025-10-28 03:48:32,248: INFO: data_transformation: Saving batch 7...]
[2025-10-28 03:48:56,344: INFO: data_transformation: Saving batch 8...]
[2025-10-28 03:49:18,191: INFO: data_transformation: Saving batch 9...]
[2025-10-28 03:49:38,210: INFO: data_transformation: Saving batch 10...]
[2025-10-28 03:49:46,759: INFO: data_transformation:  Created total 3031290 conversation pairs]
[2025-10-28 03:49:46,806: INFO: data_transformation:  Applying tokenization in batches (memory safe)...]
[2025-10-28 03:49:46,810: INFO: data_transformation: Processing batch 1/11: samples 0-300000]
[2025-10-28 03:49:50,704: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:49:50,704: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:49:50,704: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:49:52,485: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:49:52,485: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:56:49,073: INFO: data_transformation: Processing batch 2/11: samples 300000-600000]
[2025-10-28 03:56:51,887: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:56:51,888: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:56:51,888: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 03:56:53,251: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 03:56:53,258: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:05:13,982: INFO: data_transformation: Processing batch 3/11: samples 600000-900000]
[2025-10-28 04:05:18,743: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:05:18,744: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:05:18,746: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:05:20,368: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:05:20,374: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:05:20,382: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:13:53,899: INFO: data_transformation: Processing batch 4/11: samples 900000-1200000]
[2025-10-28 04:13:57,868: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:13:57,869: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:13:57,869: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:13:59,822: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:13:59,834: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:13:59,840: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:33:15,337: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:33:25,110: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:33:59,776: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 04:33:59,789: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 04:33:59,796: INFO: common: created directory at: artifacts]
[2025-10-28 04:33:59,799: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 04:34:05,733: INFO: data_transformation:  Loading dataset from artifacts/data_ingestion]
[2025-10-28 04:34:19,059: INFO: data_transformation:  Dataset loaded with 1000000 samples]
[2025-10-28 04:34:19,099: INFO: data_transformation: Converting conversations into input-target pairs...]
[2025-10-28 04:35:53,419: INFO: data_transformation: Saving batch 1...]
[2025-10-28 04:37:02,449: INFO: data_transformation: Saving batch 2...]
[2025-10-28 04:37:54,406: INFO: data_transformation: Saving batch 3...]
[2025-10-28 04:39:22,074: INFO: data_transformation: Saving batch 4...]
[2025-10-28 04:40:15,152: INFO: data_transformation: Saving batch 5...]
[2025-10-28 04:41:05,264: INFO: data_transformation: Saving batch 6...]
[2025-10-28 04:42:04,945: INFO: data_transformation: Saving batch 7...]
[2025-10-28 04:42:58,239: INFO: data_transformation: Saving batch 8...]
[2025-10-28 04:43:38,293: INFO: data_transformation: Saving batch 9...]
[2025-10-28 04:44:40,800: INFO: data_transformation: Saving batch 10...]
[2025-10-28 04:45:03,705: INFO: data_transformation:  Created total 3031290 conversation pairs]
[2025-10-28 04:45:03,754: INFO: data_transformation:  Applying tokenization in batches (memory safe)...]
[2025-10-28 04:45:03,767: INFO: data_transformation: Processing batch 1/31: samples 0-100000]
[2025-10-28 04:45:10,290: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:45:10,291: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:45:10,293: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:45:12,587: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:45:12,587: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:45:12,587: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:47:15,398: INFO: data_transformation: Processing batch 2/31: samples 100000-200000]
[2025-10-28 04:47:17,819: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:47:17,821: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:47:17,823: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:47:19,521: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:47:19,537: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:47:19,538: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:49:00,930: INFO: data_transformation: Processing batch 3/31: samples 200000-300000]
[2025-10-28 04:49:02,672: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:49:02,673: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:49:04,552: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:49:04,560: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:49:04,562: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:50:50,962: INFO: data_transformation: Processing batch 4/31: samples 300000-400000]
[2025-10-28 04:50:53,314: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:50:53,325: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:50:54,263: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:50:54,268: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:50:54,271: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:53:53,020: INFO: data_transformation: Processing batch 5/31: samples 400000-500000]
[2025-10-28 04:53:55,701: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:53:55,703: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:53:55,705: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:53:57,588: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:53:57,589: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:53:57,762: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:56:20,779: INFO: data_transformation: Processing batch 6/31: samples 500000-600000]
[2025-10-28 04:56:22,608: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:56:22,609: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:56:23,839: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:56:23,845: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:58:22,815: INFO: data_transformation: Processing batch 7/31: samples 600000-700000]
[2025-10-28 04:58:24,486: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:58:24,498: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 04:58:25,623: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:58:25,624: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 04:58:25,630: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:00:17,416: INFO: data_transformation: Processing batch 8/31: samples 700000-800000]
[2025-10-28 05:00:19,292: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:00:19,292: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:00:19,295: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:00:20,540: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:00:20,544: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:00:20,570: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:01:59,511: INFO: data_transformation: Processing batch 9/31: samples 800000-900000]
[2025-10-28 05:02:01,359: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:02:01,481: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:02:01,611: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:02:02,352: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:02:02,511: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:02:02,628: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:03:42,975: INFO: data_transformation: Processing batch 10/31: samples 900000-1000000]
[2025-10-28 05:03:44,829: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:03:44,853: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:03:44,894: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:03:45,890: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:03:45,959: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:03:45,977: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:05:29,656: INFO: data_transformation: Processing batch 11/31: samples 1000000-1100000]
[2025-10-28 05:05:31,186: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:05:31,201: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:05:31,256: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:05:32,436: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:05:32,446: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:05:32,452: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:07:12,162: INFO: data_transformation: Processing batch 12/31: samples 1100000-1200000]
[2025-10-28 05:07:13,960: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:07:13,960: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:07:14,004: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:07:15,071: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:07:15,080: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:07:15,088: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:09:05,192: INFO: data_transformation: Processing batch 13/31: samples 1200000-1300000]
[2025-10-28 05:09:07,165: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:09:07,166: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:09:07,166: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:09:08,312: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:09:08,312: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:09:08,312: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:10:56,680: INFO: data_transformation: Processing batch 14/31: samples 1300000-1400000]
[2025-10-28 05:11:00,742: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:11:00,755: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:11:00,756: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:11:01,864: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:11:01,865: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:11:01,865: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:13:08,212: INFO: data_transformation: Processing batch 15/31: samples 1400000-1500000]
[2025-10-28 05:13:09,886: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:13:09,888: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:13:09,889: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:13:11,057: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:13:11,058: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:13:11,060: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:14:55,555: INFO: data_transformation: Processing batch 16/31: samples 1500000-1600000]
[2025-10-28 05:14:57,393: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:14:57,394: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:14:58,724: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:14:58,725: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:14:58,726: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:16:47,409: INFO: data_transformation: Processing batch 17/31: samples 1600000-1700000]
[2025-10-28 05:16:49,280: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:16:49,280: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:16:50,674: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:16:50,674: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:16:50,676: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:18:54,725: INFO: data_transformation: Processing batch 18/31: samples 1700000-1800000]
[2025-10-28 05:18:56,481: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:18:56,485: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:18:56,532: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:18:57,846: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:18:57,849: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:18:57,853: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:20:50,709: INFO: data_transformation: Processing batch 19/31: samples 1800000-1900000]
[2025-10-28 05:20:52,789: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:20:52,802: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:20:52,805: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:20:53,988: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:20:53,988: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:22:38,994: INFO: data_transformation: Processing batch 20/31: samples 1900000-2000000]
[2025-10-28 05:22:41,206: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:22:41,206: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:22:41,207: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:22:42,469: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:22:42,474: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:22:42,476: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:24:21,389: INFO: data_transformation: Processing batch 21/31: samples 2000000-2100000]
[2025-10-28 05:24:23,134: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:24:23,134: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:24:23,141: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:24:24,260: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:24:24,272: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:24:24,276: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:26:04,969: INFO: data_transformation: Processing batch 22/31: samples 2100000-2200000]
[2025-10-28 05:26:06,396: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:26:06,412: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:26:06,480: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 05:26:07,388: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:26:07,398: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 05:26:07,478: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 07:39:04,540: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 07:39:10,045: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 07:39:33,503: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 07:39:33,533: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 07:39:33,538: INFO: common: created directory at: artifacts]
[2025-10-28 07:39:33,539: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 07:39:35,751: INFO: data_transformation:  Loading dataset from artifacts/data_ingestion]
[2025-10-28 07:40:57,498: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 07:40:59,506: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 07:41:10,248: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-10-28 07:41:10,251: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-10-28 07:41:10,252: INFO: common: created directory at: artifacts]
[2025-10-28 07:41:10,253: INFO: common: created directory at: artifacts/data_transformation]
[2025-10-28 07:41:12,430: INFO: data_transformation:  Loading dataset from artifacts/data_ingestion]
[2025-10-28 07:41:24,481: INFO: data_transformation:  Dataset loaded with 1000000 samples]
[2025-10-28 07:41:24,483: INFO: data_transformation: Converting conversations into input-target pairs...]
[2025-10-28 07:41:44,384: INFO: data_transformation: Saving batch 1...]
[2025-10-28 07:42:22,828: INFO: data_transformation: Saving batch 2...]
[2025-10-28 07:42:59,204: INFO: data_transformation: Saving batch 3...]
[2025-10-28 07:43:34,634: INFO: data_transformation: Saving batch 4...]
[2025-10-28 07:44:09,648: INFO: data_transformation: Saving batch 5...]
[2025-10-28 07:44:39,300: INFO: data_transformation: Saving batch 6...]
[2025-10-28 07:45:08,809: INFO: data_transformation: Saving batch 7...]
[2025-10-28 07:45:40,792: INFO: data_transformation: Saving batch 8...]
[2025-10-28 07:46:10,244: INFO: data_transformation: Saving batch 9...]
[2025-10-28 07:46:42,898: INFO: data_transformation: Saving batch 10...]
[2025-10-28 07:46:52,207: INFO: data_transformation:  Created total 3031290 conversation pairs]
[2025-10-28 07:46:52,237: INFO: data_transformation:  Applying tokenization in batches (memory safe)...]
[2025-10-28 07:46:52,246: INFO: data_transformation: Processing batch 1/61: samples 0-50000]
[2025-10-28 07:46:59,126: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 07:46:59,127: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 07:46:59,129: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-10-28 07:47:01,952: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 07:47:01,953: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 07:47:01,954: INFO: config: TensorFlow version 2.20.0 available.]
[2025-10-28 07:48:38,574: INFO: data_transformation: Processing batch 2/61: samples 50000-100000]
[2025-11-01 04:04:14,253: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 04:04:14,258: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 04:04:14,258: INFO: common: created directory at: artifacts]
[2025-11-01 04:04:14,259: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 04:04:14,261: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 04:04:14,262: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-01 04:04:15,202: INFO: model_evaluation:  Loading tokenized batches from artifacts/data_transformation/test]
[2025-11-01 04:04:16,556: INFO: model_evaluation:  Loaded total 304316 samples.]
[2025-11-01 04:04:16,583: INFO: model_evaluation:  Generating predictions...]
[2025-11-01 04:11:25,292: INFO: model_evaluation: Generated 50 samples...]
[2025-11-01 04:18:52,399: INFO: model_evaluation: Generated 100 samples...]
[2025-11-01 04:25:39,315: INFO: model_evaluation: Generated 150 samples...]
[2025-11-01 04:33:27,008: INFO: model_evaluation: Generated 200 samples...]
[2025-11-01 04:41:16,612: INFO: model_evaluation: Generated 250 samples...]
[2025-11-01 04:49:12,155: INFO: model_evaluation: Generated 300 samples...]
[2025-11-01 04:53:13,775: INFO: model_evaluation: Generated 350 samples...]
[2025-11-01 04:56:56,885: INFO: model_evaluation: Generated 400 samples...]
[2025-11-01 05:00:39,431: INFO: model_evaluation: Generated 450 samples...]
[2025-11-01 05:04:25,138: INFO: model_evaluation: Generated 500 samples...]
[2025-11-01 05:30:08,063: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 05:30:08,069: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 05:30:08,070: INFO: common: created directory at: artifacts]
[2025-11-01 05:30:08,071: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 05:30:08,072: INFO: model_evaluation: No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 05:30:08,073: INFO: model_evaluation: Loading model and tokenizer...]
[2025-11-01 05:30:08,766: INFO: model_evaluation: Loading tokenized batches from artifacts/data_transformation/test]
[2025-11-01 05:30:10,577: INFO: model_evaluation: Loaded total 304316 samples.]
[2025-11-01 05:30:10,605: INFO: model_evaluation: Generating predictions...]
[2025-11-01 05:35:44,125: INFO: model_evaluation: Generated 50 samples...]
[2025-11-01 05:43:14,519: INFO: model_evaluation: Generated 100 samples...]
[2025-11-01 05:43:24,349: INFO: model_evaluation: Calculating BLEU and ROUGE scores...]
[2025-11-01 05:57:29,501: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 05:57:29,682: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 05:57:29,712: INFO: common: created directory at: artifacts]
[2025-11-01 05:57:29,714: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 05:57:29,716: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 05:57:29,727: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-01 05:57:30,902: INFO: model_evaluation:  Loading tokenized batches from artifacts/data_transformation/test]
[2025-11-01 05:57:33,560: INFO: model_evaluation:  Loaded total 304316 samples.]
[2025-11-01 05:57:33,585: INFO: model_evaluation:  Generating predictions...]
[2025-11-01 06:00:09,323: INFO: model_evaluation: Generated 50 samples...]
[2025-11-01 06:04:11,777: INFO: model_evaluation: Generated 100 samples...]
[2025-11-01 06:04:19,234: INFO: model_evaluation:  Calculating BLEU and ROUGE scores...]
[2025-11-01 06:27:58,786: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 06:27:58,800: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 06:27:58,801: INFO: common: created directory at: artifacts]
[2025-11-01 06:27:58,803: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 06:27:58,807: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 06:27:58,808: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-01 06:28:00,134: INFO: model_evaluation:  Loading tokenized batches from artifacts/data_transformation/test]
[2025-11-01 06:28:01,714: INFO: model_evaluation:  Loaded total 304316 samples.]
[2025-11-01 06:28:01,739: INFO: model_evaluation:  Generating predictions...]
[2025-11-01 06:29:02,482: INFO: model_evaluation:  Calculating BLEU and ROUGE scores...]
[2025-11-01 06:29:58,343: INFO: rouge_scorer: Using default tokenizer.]
[2025-11-01 06:29:58,838: INFO: model_evaluation:  BLEU Score: 0.1050]
[2025-11-01 06:29:58,839: INFO: model_evaluation:  ROUGE Scores: {'rouge1': np.float64(0.19113276824554878), 'rouge2': np.float64(0.09897850531148564), 'rougeL': np.float64(0.16584568845945236)}]
[2025-11-01 06:29:58,841: INFO: model_evaluation:  BLEU Score: 0.1050]
[2025-11-01 06:29:58,842: INFO: model_evaluation:  ROUGE Scores: {'rouge1': np.float64(0.19113276824554878), 'rouge2': np.float64(0.09897850531148564), 'rougeL': np.float64(0.16584568845945236)}]
[2025-11-01 06:29:58,850: INFO: model_evaluation:  Metrics saved to: artifacts\model_evaluation\metrics.csv]
[2025-11-01 06:29:58,917: INFO: model_evaluation:  Sample predictions saved to: artifacts\model_evaluation\predictions.csv]
[2025-11-01 07:57:34,302: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 07:57:34,313: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 07:57:34,314: INFO: common: created directory at: artifacts]
[2025-11-01 07:57:34,316: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 07:57:38,292: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-01 07:57:38,296: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 07:57:38,354: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-01 07:57:38,769: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-01 07:57:39,130: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-01 07:57:39,459: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-01 07:57:40,320: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-01 07:57:40,798: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-01 07:57:41,144: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-01 07:57:41,474: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-01 07:57:41,855: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-01 07:57:42,278: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-01 07:57:42,711: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-01 07:57:43,132: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-01 07:57:43,759: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-01 07:57:44,215: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-01 07:57:44,601: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-01 07:57:45,013: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-01 07:57:45,325: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-01 07:57:45,751: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-01 07:57:46,170: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-01 07:57:46,568: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-01 07:57:46,969: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-01 07:57:47,392: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-01 07:57:47,739: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-01 07:57:48,147: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-01 07:57:48,573: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-01 07:57:48,971: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-01 07:57:49,359: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-01 07:57:49,732: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-01 07:57:50,094: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-01 07:57:50,415: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-01 07:57:50,700: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-01 07:57:51,134: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-01 07:57:51,437: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-01 07:57:51,756: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-01 07:57:52,064: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-01 07:57:52,449: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-01 07:57:52,815: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-01 07:57:53,156: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-01 07:57:53,469: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-01 07:57:53,867: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-01 07:57:54,281: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-01 07:57:54,673: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-01 07:57:55,219: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-01 07:57:55,607: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-01 07:57:56,007: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-01 07:57:56,297: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-01 07:57:56,798: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-01 07:57:57,227: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-01 07:57:57,621: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-01 07:57:57,972: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-01 07:57:58,352: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 50000 samples]
[2025-11-01 07:57:58,645: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_55 with 26974 samples]
[2025-11-01 07:57:58,960: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-01 07:57:59,314: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-01 07:57:59,671: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-01 07:58:00,020: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-01 07:58:00,203: INFO: model_trainer:  Combined total samples: 2726974]
[2025-11-01 07:58:00,204: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-01 07:58:00,471: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-01 07:58:00,688: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-01 07:58:00,943: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-01 07:58:01,200: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-01 07:58:01,477: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-01 07:58:01,687: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-01 07:58:01,745: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 4316 samples]
[2025-11-01 07:58:01,809: INFO: model_trainer:  Combined total samples: 304316]
[2025-11-01 07:58:02,065: INFO: model_trainer:  Train samples: 1000 | Eval samples: 500]
[2025-11-01 07:58:09,532: INFO: model_trainer:  Training model...]
[2025-11-02 11:37:06,316: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 11:37:06,336: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 11:37:06,337: INFO: common: created directory at: artifacts]
[2025-11-02 11:37:06,339: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 11:37:12,195: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-02 11:37:12,196: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 11:37:12,207: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-02 11:37:12,684: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-02 11:37:13,067: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-02 11:37:13,382: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-02 11:37:13,662: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-02 11:37:13,972: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-02 11:37:14,901: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-02 11:37:15,312: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-02 11:37:15,646: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-02 11:37:15,892: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-02 11:37:16,132: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-02 11:37:16,366: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-02 11:37:16,602: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-02 11:37:16,830: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-02 11:37:17,066: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-02 11:37:17,414: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-02 11:37:17,652: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-02 11:37:17,884: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-02 11:37:18,127: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-02 11:37:18,365: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-02 11:37:18,608: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-02 11:37:18,904: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-02 11:37:19,141: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-02 11:37:19,393: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-02 11:37:19,648: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-02 11:37:19,893: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-02 11:37:20,128: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-02 11:37:20,371: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-02 11:37:20,612: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-02 11:37:20,863: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-02 11:37:21,105: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-02 11:37:21,346: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-02 11:37:21,591: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-02 11:37:21,823: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-02 11:37:22,059: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-02 11:37:22,305: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-02 11:37:22,542: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-02 11:37:22,786: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-02 11:37:23,018: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-02 11:37:23,578: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-02 11:37:23,887: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-02 11:37:24,110: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-02 11:37:24,355: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-02 11:37:24,586: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-02 11:37:24,826: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-02 11:37:25,078: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-02 11:37:25,330: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-02 11:37:25,577: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-02 11:37:25,823: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-02 11:37:26,061: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-02 11:37:26,301: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 50000 samples]
[2025-11-02 11:37:26,444: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_55 with 26974 samples]
[2025-11-02 11:37:26,686: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-02 11:37:26,933: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-02 11:37:27,195: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-02 11:37:27,441: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-02 11:37:27,540: INFO: model_trainer:  Combined total samples: 2726974]
[2025-11-02 11:37:27,541: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-02 11:37:27,773: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-02 11:37:28,020: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-02 11:37:28,270: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-02 11:37:28,570: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-02 11:37:28,801: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-02 11:37:29,054: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-02 11:37:29,183: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 4316 samples]
[2025-11-02 11:37:29,197: INFO: model_trainer:  Combined total samples: 304316]
[2025-11-02 11:37:29,307: INFO: model_trainer:  Train samples: 1000 | Eval samples: 500]
[2025-11-02 11:37:35,259: INFO: model_trainer:  Training model...]
[2025-11-01 12:57:18,284: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 12:57:21,268: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 12:57:36,217: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 12:57:36,234: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 12:57:36,235: INFO: common: created directory at: artifacts]
[2025-11-01 12:57:36,236: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 12:57:37,937: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 12:57:41,706: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 12:57:41,712: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 12:57:44,139: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 12:57:44,140: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 12:57:44,140: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 12:59:19,631: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 12:59:20,886: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 12:59:27,830: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 12:59:27,832: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 12:59:27,832: INFO: common: created directory at: artifacts]
[2025-11-01 12:59:27,833: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 12:59:29,478: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 12:59:33,396: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 12:59:33,397: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 12:59:33,994: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 12:59:33,995: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 12:59:33,996: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:31:40,174: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:31:41,417: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:31:48,283: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 13:31:48,286: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 13:31:48,287: INFO: common: created directory at: artifacts]
[2025-11-01 13:31:48,288: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 13:31:49,544: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 13:31:53,968: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 13:31:53,969: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 13:31:54,744: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 13:31:54,830: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 13:31:54,831: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:32:18,898: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:32:20,177: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:32:27,329: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 13:32:27,331: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 13:32:27,331: INFO: common: created directory at: artifacts]
[2025-11-01 13:32:27,332: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 13:32:28,566: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 13:32:32,483: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 13:32:32,486: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 13:32:33,076: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 13:32:33,077: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 13:32:33,078: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:33:16,491: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:33:49,096: INFO: data_transformation:  Processed 600000 samples so far...]
[2025-11-01 13:34:15,652: INFO: data_transformation:  Processed 900000 samples so far...]
[2025-11-01 13:34:37,753: INFO: data_transformation:  Processed 1200000 samples so far...]
[2025-11-01 13:35:01,752: INFO: data_transformation:  Processed 1500000 samples so far...]
[2025-11-01 13:35:35,423: INFO: data_transformation:  Processed 1800000 samples so far...]
[2025-11-01 13:35:56,645: INFO: data_transformation:  Processed 2100000 samples so far...]
[2025-11-01 13:36:17,156: INFO: data_transformation:  Processed 2400000 samples so far...]
[2025-11-01 13:36:42,117: INFO: data_transformation:  Processed 2700000 samples so far...]
[2025-11-01 13:36:43,839: INFO: data_transformation:  Created 2726974 total conversation pairs]
[2025-11-01 13:36:43,858: INFO: data_transformation:  Preparing test conversation pairs...]
[2025-11-01 13:36:43,859: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:37:01,968: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:37:02,306: INFO: data_transformation:  Created 304316 total conversation pairs]
[2025-11-01 13:37:02,311: INFO: data_transformation:  Starting tokenization for train data...]
[2025-11-01 13:37:02,312: INFO: data_transformation:  Processing train batch 1/55: samples 0-50000]
[2025-11-01 13:37:14,660: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:37:14,661: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:37:17,902: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:37:17,902: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:37:17,903: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:38:45,006: INFO: data_transformation:  Processing train batch 2/55: samples 50000-100000]
[2025-11-01 13:38:47,045: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:38:47,045: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:38:47,052: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:38:48,370: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:38:48,372: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:38:48,373: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:39:57,454: INFO: data_transformation:  Processing train batch 3/55: samples 100000-150000]
[2025-11-01 13:40:00,445: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:40:00,764: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:40:00,853: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:40:03,029: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:40:03,536: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:40:04,308: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:40:20,698: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:40:20,802: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:40:56,708: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:40:57,703: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:41:02,640: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 13:41:02,663: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 13:41:02,666: INFO: common: created directory at: artifacts]
[2025-11-01 13:41:02,667: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 13:41:04,096: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 13:41:08,028: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 13:41:08,029: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 13:41:08,645: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 13:41:08,646: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 13:41:08,650: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:41:24,485: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:41:42,556: INFO: data_transformation:  Processed 600000 samples so far...]
[2025-11-01 13:42:02,274: INFO: data_transformation:  Processed 900000 samples so far...]
[2025-11-01 13:42:26,207: INFO: data_transformation:  Processed 1200000 samples so far...]
[2025-11-01 13:42:51,509: INFO: data_transformation:  Processed 1500000 samples so far...]
[2025-11-01 13:43:19,973: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:43:21,656: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:43:31,656: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 13:43:31,665: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 13:43:31,666: INFO: common: created directory at: artifacts]
[2025-11-01 13:43:31,666: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 13:43:32,936: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 13:43:36,737: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 13:43:36,738: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 13:43:37,301: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 13:43:37,302: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 13:43:37,302: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:43:50,277: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:44:06,106: INFO: data_transformation:  Processed 600000 samples so far...]
[2025-11-01 13:44:23,841: INFO: data_transformation:  Processed 900000 samples so far...]
[2025-11-01 13:44:43,663: INFO: data_transformation:  Processed 1200000 samples so far...]
[2025-11-01 13:45:01,338: INFO: data_transformation:  Processed 1500000 samples so far...]
[2025-11-01 13:45:21,284: INFO: data_transformation:  Processed 1800000 samples so far...]
[2025-11-01 13:45:41,898: INFO: data_transformation:  Processed 2100000 samples so far...]
[2025-11-01 13:46:01,188: INFO: data_transformation:  Processed 2400000 samples so far...]
[2025-11-01 13:46:24,581: INFO: data_transformation:  Processed 2700000 samples so far...]
[2025-11-01 13:46:26,272: INFO: data_transformation:  Created 2726974 total conversation pairs]
[2025-11-01 13:46:26,289: INFO: data_transformation:  Preparing test conversation pairs...]
[2025-11-01 13:46:26,290: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:46:43,716: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:46:44,135: INFO: data_transformation:  Created 304316 total conversation pairs]
[2025-11-01 13:46:44,207: INFO: data_transformation:  Starting tokenization for train data...]
[2025-11-01 13:46:44,208: INFO: data_transformation:  Processing train batch 1/55: samples 0-50000]
[2025-11-01 13:46:50,550: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:46:50,551: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:46:52,185: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:46:52,185: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:47:33,927: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:47:35,188: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:47:41,713: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 13:47:41,723: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 13:47:41,725: INFO: common: created directory at: artifacts]
[2025-11-01 13:47:41,726: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 13:47:42,946: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 13:47:46,532: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 13:47:46,533: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 13:47:47,093: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 13:47:47,094: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 13:47:47,095: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:48:00,729: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:48:16,237: INFO: data_transformation:  Processed 600000 samples so far...]
[2025-11-01 13:48:33,657: INFO: data_transformation:  Processed 900000 samples so far...]
[2025-11-01 13:48:50,911: INFO: data_transformation:  Processed 1200000 samples so far...]
[2025-11-01 13:49:10,373: INFO: data_transformation:  Processed 1500000 samples so far...]
[2025-11-01 13:49:28,815: INFO: data_transformation:  Processed 1800000 samples so far...]
[2025-11-01 13:49:46,095: INFO: data_transformation:  Processed 2100000 samples so far...]
[2025-11-01 13:50:05,851: INFO: data_transformation:  Processed 2400000 samples so far...]
[2025-11-01 13:50:36,519: INFO: data_transformation:  Processed 2700000 samples so far...]
[2025-11-01 13:50:39,703: INFO: data_transformation:  Created 2726974 total conversation pairs]
[2025-11-01 13:50:39,722: INFO: data_transformation:  Preparing test conversation pairs...]
[2025-11-01 13:50:39,735: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:50:57,562: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:50:57,818: INFO: data_transformation:  Created 304316 total conversation pairs]
[2025-11-01 13:50:57,827: INFO: data_transformation:  Starting tokenization for train data...]
[2025-11-01 13:50:57,828: INFO: data_transformation:  Processing train batch 1/55: samples 0-50000]
[2025-11-01 13:51:16,466: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:51:18,111: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:51:28,992: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 13:51:29,000: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 13:51:29,001: INFO: common: created directory at: artifacts]
[2025-11-01 13:51:29,002: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 13:51:30,318: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 13:51:33,509: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 13:51:33,510: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 13:51:33,698: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 13:51:33,699: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 13:51:33,700: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:51:51,481: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:52:07,474: INFO: data_transformation:  Processed 600000 samples so far...]
[2025-11-01 13:52:25,073: INFO: data_transformation:  Processed 900000 samples so far...]
[2025-11-01 13:52:42,459: INFO: data_transformation:  Processed 1200000 samples so far...]
[2025-11-01 13:53:00,210: INFO: data_transformation:  Processed 1500000 samples so far...]
[2025-11-01 13:53:18,416: INFO: data_transformation:  Processed 1800000 samples so far...]
[2025-11-01 13:53:36,543: INFO: data_transformation:  Processed 2100000 samples so far...]
[2025-11-01 13:54:00,555: INFO: data_transformation:  Processed 2400000 samples so far...]
[2025-11-01 13:54:24,546: INFO: data_transformation:  Processed 2700000 samples so far...]
[2025-11-01 13:54:26,729: INFO: data_transformation:  Created 2726974 total conversation pairs]
[2025-11-01 13:54:26,759: INFO: data_transformation:  Preparing test conversation pairs...]
[2025-11-01 13:54:26,759: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 13:54:44,681: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 13:54:45,001: INFO: data_transformation:  Created 304316 total conversation pairs]
[2025-11-01 13:54:45,025: INFO: data_transformation:  Starting tokenization for train data...]
[2025-11-01 13:54:45,028: INFO: data_transformation:  Processing train batch 1/55: samples 0-50000]
[2025-11-01 13:54:50,794: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:54:50,794: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:54:50,795: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:54:52,132: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:54:52,132: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:54:52,132: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:58:02,031: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 13:58:03,400: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 13:58:10,440: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 13:58:10,451: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 13:58:10,454: INFO: common: created directory at: artifacts]
[2025-11-01 13:58:10,455: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 13:58:11,819: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 13:58:15,687: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 13:58:15,688: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 13:58:16,271: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 13:58:16,272: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 13:58:16,272: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 14:03:49,011: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:03:49,870: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:03:54,475: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 14:03:54,477: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 14:03:54,478: INFO: common: created directory at: artifacts]
[2025-11-01 14:03:54,478: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 14:03:55,521: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 14:03:56,177: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 14:03:56,183: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 14:03:56,258: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 14:03:56,259: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 14:03:56,259: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 14:04:11,623: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 14:04:33,051: INFO: data_transformation:  Processed 600000 samples so far...]
[2025-11-01 14:04:52,189: INFO: data_transformation:  Processed 900000 samples so far...]
[2025-11-01 14:05:11,184: INFO: data_transformation:  Processed 1200000 samples so far...]
[2025-11-01 14:05:30,342: INFO: data_transformation:  Processed 1500000 samples so far...]
[2025-11-01 14:05:52,879: INFO: data_transformation:  Processed 1800000 samples so far...]
[2025-11-01 14:06:10,045: INFO: data_transformation:  Processed 2100000 samples so far...]
[2025-11-01 14:06:32,123: INFO: data_transformation:  Processed 2400000 samples so far...]
[2025-11-01 14:06:55,854: INFO: data_transformation:  Processed 2700000 samples so far...]
[2025-11-01 14:06:57,538: INFO: data_transformation:  Created 2726974 total conversation pairs]
[2025-11-01 14:06:57,562: INFO: data_transformation:  Preparing test conversation pairs...]
[2025-11-01 14:06:57,562: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 14:07:15,229: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 14:07:15,590: INFO: data_transformation:  Created 304316 total conversation pairs]
[2025-11-01 14:07:15,603: INFO: data_transformation:  Starting tokenization for train data...]
[2025-11-01 14:07:15,603: INFO: data_transformation:  Processing train batch 1/55: samples 0-50000]
[2025-11-01 14:07:18,788: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:07:18,788: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:07:20,172: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:07:20,849: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:07:20,851: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:12:49,432: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 14:12:49,437: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 14:12:49,439: INFO: common: created directory at: artifacts]
[2025-11-01 14:12:49,440: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 14:12:52,427: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-01 14:12:52,428: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 14:12:52,435: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-01 14:12:52,714: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-01 14:12:52,937: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-01 14:12:53,166: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-01 14:12:53,399: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-01 14:12:53,627: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-01 14:12:53,854: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-01 14:12:54,080: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-01 14:12:54,308: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-01 14:12:54,540: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-01 14:12:54,765: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-01 14:12:54,994: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-01 14:12:55,221: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-01 14:12:55,455: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-01 14:12:55,680: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-01 14:12:55,914: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-01 14:12:56,152: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-01 14:12:56,395: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-01 14:12:56,636: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-01 14:12:56,867: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-01 14:12:57,107: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-01 14:12:57,345: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-01 14:12:57,587: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-01 14:12:57,821: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-01 14:12:58,055: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-01 14:12:58,296: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-01 14:12:58,534: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-01 14:12:58,766: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-01 14:12:58,992: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-01 14:12:59,221: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-01 14:12:59,461: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-01 14:12:59,701: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-01 14:12:59,937: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-01 14:13:00,166: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-01 14:13:00,390: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-01 14:13:00,625: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-01 14:13:00,870: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-01 14:13:01,101: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-01 14:13:01,357: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-01 14:13:01,583: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-01 14:13:01,929: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-01 14:13:02,189: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-01 14:13:02,433: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-01 14:13:02,665: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-01 14:13:02,891: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-01 14:13:03,117: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-01 14:13:03,347: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-01 14:13:03,585: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-01 14:13:03,818: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-01 14:13:04,047: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-01 14:13:04,273: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 50000 samples]
[2025-11-01 14:13:04,422: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_55 with 26974 samples]
[2025-11-01 14:13:04,654: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-01 14:13:04,878: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-01 14:13:05,106: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-01 14:13:05,329: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-01 14:13:05,390: INFO: model_trainer:  Combined total samples: 2726974]
[2025-11-01 14:13:05,391: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-01 14:13:05,620: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-01 14:13:05,923: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-01 14:13:06,151: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-01 14:13:06,383: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-01 14:13:06,619: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-01 14:13:06,856: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-01 14:13:06,918: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 4316 samples]
[2025-11-01 14:13:06,933: INFO: model_trainer:  Combined total samples: 304316]
[2025-11-01 14:13:07,033: INFO: model_trainer:  Train samples: 1000 | Eval samples: 500]
[2025-11-01 14:13:11,306: INFO: model_trainer:  Training model...]
[2025-11-01 14:29:15,497: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:29:16,497: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:29:23,030: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 14:29:23,032: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 14:29:23,032: INFO: common: created directory at: artifacts]
[2025-11-01 14:29:23,033: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 14:29:24,620: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 14:29:28,416: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 14:29:28,417: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 14:29:28,968: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 14:29:28,968: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 14:29:28,969: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 14:29:51,971: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 14:30:40,011: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:30:41,473: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:30:49,087: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 14:30:49,089: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 14:30:49,090: INFO: common: created directory at: artifacts]
[2025-11-01 14:30:49,091: INFO: common: created directory at: artifacts/data_transformation]
[2025-11-01 14:30:50,474: INFO: data_transformation:  Loading train dataset from artifacts/data_ingestion\train ...]
[2025-11-01 14:30:54,316: INFO: data_transformation:  Train dataset loaded with 900000 samples]
[2025-11-01 14:30:54,325: INFO: data_transformation:  Loading test dataset from artifacts/data_ingestion\test ...]
[2025-11-01 14:30:55,045: INFO: data_transformation:  Test dataset loaded with 100000 samples]
[2025-11-01 14:30:55,045: INFO: data_transformation:  Preparing train conversation pairs...]
[2025-11-01 14:30:55,046: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 14:31:23,521: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 14:31:48,346: INFO: data_transformation:  Processed 600000 samples so far...]
[2025-11-01 14:32:07,542: INFO: data_transformation:  Processed 900000 samples so far...]
[2025-11-01 14:32:25,896: INFO: data_transformation:  Processed 1200000 samples so far...]
[2025-11-01 14:32:46,164: INFO: data_transformation:  Processed 1500000 samples so far...]
[2025-11-01 14:33:08,861: INFO: data_transformation:  Processed 1800000 samples so far...]
[2025-11-01 14:33:26,073: INFO: data_transformation:  Processed 2100000 samples so far...]
[2025-11-01 14:33:49,299: INFO: data_transformation:  Processed 2400000 samples so far...]
[2025-11-01 14:34:06,340: INFO: data_transformation:   Created 2693391 total conversation pairs]
[2025-11-01 14:34:06,933: INFO: data_transformation:  Preparing test conversation pairs...]
[2025-11-01 14:34:06,934: INFO: data_transformation:  Converting conversations into input-target pairs...]
[2025-11-01 14:34:25,054: INFO: data_transformation:  Processed 300000 samples so far...]
[2025-11-01 14:34:25,174: INFO: data_transformation:   Created 300679 total conversation pairs]
[2025-11-01 14:34:25,203: INFO: data_transformation:  Starting tokenization for train data...]
[2025-11-01 14:34:25,207: INFO: data_transformation:  Processing train batch 1/54: samples 0-50000]
[2025-11-01 14:34:29,116: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:34:29,116: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:34:30,576: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:34:30,576: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:36:18,029: INFO: data_transformation:  Processing train batch 2/54: samples 50000-100000]
[2025-11-01 14:36:20,495: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:36:20,496: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:36:20,497: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:36:22,077: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:36:22,107: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:36:22,171: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:37:45,786: INFO: data_transformation:  Processing train batch 3/54: samples 100000-150000]
[2025-11-01 14:37:47,948: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:37:47,963: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:37:47,979: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:37:49,578: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:37:49,578: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:37:49,578: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:38:56,769: INFO: data_transformation:  Processing train batch 4/54: samples 150000-200000]
[2025-11-01 14:38:58,160: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:38:58,164: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:38:58,209: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:38:59,176: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:38:59,197: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:38:59,246: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:39:57,758: INFO: data_transformation:  Processing train batch 5/54: samples 200000-250000]
[2025-11-01 14:39:59,254: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:39:59,255: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:39:59,269: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:40:00,213: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:40:00,220: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:40:00,233: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:40:57,758: INFO: data_transformation:  Processing train batch 6/54: samples 250000-300000]
[2025-11-01 14:40:59,269: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:40:59,283: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:40:59,316: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:41:00,223: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:41:00,240: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:41:00,295: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:42:03,782: INFO: data_transformation:  Processing train batch 7/54: samples 300000-350000]
[2025-11-01 14:42:05,215: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:42:05,332: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:42:05,339: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:42:06,309: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:42:06,479: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:42:06,492: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:43:08,595: INFO: data_transformation:  Processing train batch 8/54: samples 350000-400000]
[2025-11-01 14:43:10,610: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:43:10,622: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:43:10,631: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:43:11,805: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:43:11,806: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:43:11,818: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:44:11,743: INFO: data_transformation:  Processing train batch 9/54: samples 400000-450000]
[2025-11-01 14:44:13,208: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:44:13,228: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:44:13,230: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:44:14,191: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:44:14,208: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:44:14,215: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:45:07,143: INFO: data_transformation:  Processing train batch 10/54: samples 450000-500000]
[2025-11-01 14:45:08,729: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:45:08,747: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:45:08,760: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:45:09,696: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:45:09,712: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:45:09,723: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:46:07,445: INFO: data_transformation:  Processing train batch 11/54: samples 500000-550000]
[2025-11-01 14:46:08,878: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:46:08,885: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:46:08,902: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:46:09,872: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:46:09,874: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:46:09,887: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:47:11,532: INFO: data_transformation:  Processing train batch 12/54: samples 550000-600000]
[2025-11-01 14:47:12,959: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:47:12,977: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:47:13,025: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:47:14,125: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:47:14,208: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:47:14,219: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:48:06,946: INFO: data_transformation:  Processing train batch 13/54: samples 600000-650000]
[2025-11-01 14:48:08,357: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:48:08,365: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:48:08,428: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:48:09,334: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:48:09,337: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:48:09,433: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:49:26,362: INFO: data_transformation:  Processing train batch 14/54: samples 650000-700000]
[2025-11-01 14:49:28,720: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:49:28,720: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:49:30,290: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:49:30,337: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:49:30,341: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:51:17,644: INFO: data_transformation:  Processing train batch 15/54: samples 700000-750000]
[2025-11-01 14:51:20,071: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:51:20,072: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:51:20,080: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:51:21,815: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:51:21,899: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:51:22,384: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:52:54,249: INFO: data_transformation:  Processing train batch 16/54: samples 750000-800000]
[2025-11-01 14:52:57,259: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:52:57,260: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:52:57,262: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:52:59,235: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:52:59,456: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:52:59,490: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:54:33,309: INFO: data_transformation:  Processing train batch 17/54: samples 800000-850000]
[2025-11-01 14:54:36,158: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:54:36,158: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:54:37,706: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:54:37,706: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:54:37,728: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:55:41,492: INFO: data_transformation:  Processing train batch 18/54: samples 850000-900000]
[2025-11-01 14:55:43,251: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:55:43,252: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:55:44,329: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:55:44,334: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:55:44,346: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:56:47,951: INFO: data_transformation:  Processing train batch 19/54: samples 900000-950000]
[2025-11-01 14:56:49,933: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:56:49,955: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:56:50,010: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:56:51,503: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:56:51,610: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:56:51,693: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:58:28,450: INFO: data_transformation:  Processing train batch 20/54: samples 950000-1000000]
[2025-11-01 14:58:31,108: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:58:31,110: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 14:58:33,028: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:58:33,042: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 14:58:33,087: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:00:08,539: INFO: data_transformation:  Processing train batch 21/54: samples 1000000-1050000]
[2025-11-01 15:00:11,053: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:00:11,333: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:00:11,353: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:00:12,664: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:00:12,798: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:00:12,865: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:01:52,175: INFO: data_transformation:  Processing train batch 22/54: samples 1050000-1100000]
[2025-11-01 15:01:54,540: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:01:54,541: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:01:54,551: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:01:56,085: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:01:56,086: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:01:56,086: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:03:31,831: INFO: data_transformation:  Processing train batch 23/54: samples 1100000-1150000]
[2025-11-01 15:03:34,114: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:03:34,114: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:03:34,115: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:03:35,565: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:03:35,569: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:03:35,584: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:04:58,487: INFO: data_transformation:  Processing train batch 24/54: samples 1150000-1200000]
[2025-11-01 15:05:00,395: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:05:00,396: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:05:00,396: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:05:02,405: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:05:02,897: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:05:02,990: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:06:35,508: INFO: data_transformation:  Processing train batch 25/54: samples 1200000-1250000]
[2025-11-01 15:06:38,276: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:06:38,278: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:06:38,279: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:06:39,769: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:06:39,770: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:06:39,771: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:08:22,242: INFO: data_transformation:  Processing train batch 26/54: samples 1250000-1300000]
[2025-11-01 15:08:25,220: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:08:25,281: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:08:25,434: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:08:27,383: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:08:27,643: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:08:27,646: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:09:59,813: INFO: data_transformation:  Processing train batch 27/54: samples 1300000-1350000]
[2025-11-01 15:10:02,225: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:10:02,225: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:10:02,225: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:10:03,630: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:10:03,635: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:10:03,638: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:11:12,618: INFO: data_transformation:  Processing train batch 28/54: samples 1350000-1400000]
[2025-11-01 15:11:14,507: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:11:14,508: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:11:14,508: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:11:15,809: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:11:15,809: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:11:15,811: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:12:28,960: INFO: data_transformation:  Processing train batch 29/54: samples 1400000-1450000]
[2025-11-01 15:12:31,208: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:12:31,220: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:12:31,231: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:12:32,493: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:12:32,497: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:12:32,503: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:13:35,229: INFO: data_transformation:  Processing train batch 30/54: samples 1450000-1500000]
[2025-11-01 15:13:37,481: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:13:37,481: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:13:38,827: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:13:38,827: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:13:38,835: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:14:40,991: INFO: data_transformation:  Processing train batch 31/54: samples 1500000-1550000]
[2025-11-01 15:14:42,773: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:14:42,790: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:14:42,790: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:14:43,911: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:14:43,916: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:14:43,920: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:15:44,252: INFO: data_transformation:  Processing train batch 32/54: samples 1550000-1600000]
[2025-11-01 15:15:46,212: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:15:46,215: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:15:47,517: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:15:47,521: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:15:47,527: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:16:45,143: INFO: data_transformation:  Processing train batch 33/54: samples 1600000-1650000]
[2025-11-01 15:16:47,054: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:16:47,055: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:16:47,055: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:16:48,352: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:16:48,354: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:16:48,361: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:17:49,039: INFO: data_transformation:  Processing train batch 34/54: samples 1650000-1700000]
[2025-11-01 15:17:50,441: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:17:50,465: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:17:50,487: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:17:51,450: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:17:51,459: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:17:51,484: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:18:48,772: INFO: data_transformation:  Processing train batch 35/54: samples 1700000-1750000]
[2025-11-01 15:18:50,477: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:18:50,493: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:18:50,495: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:18:51,941: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:18:51,943: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:18:51,943: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:20:01,101: INFO: data_transformation:  Processing train batch 36/54: samples 1750000-1800000]
[2025-11-01 15:20:03,200: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:20:03,200: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:20:03,202: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:20:04,343: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:20:04,347: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:20:04,356: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:21:06,228: INFO: data_transformation:  Processing train batch 37/54: samples 1800000-1850000]
[2025-11-01 15:21:07,683: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:21:07,693: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:21:07,753: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:21:08,667: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:21:08,672: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:21:08,733: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:22:10,764: INFO: data_transformation:  Processing train batch 38/54: samples 1850000-1900000]
[2025-11-01 15:22:12,501: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:22:12,502: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:22:13,836: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:22:13,838: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:23:11,302: INFO: data_transformation:  Processing train batch 39/54: samples 1900000-1950000]
[2025-11-01 15:23:12,756: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:23:12,766: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:23:12,806: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:23:13,699: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:23:13,709: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:23:13,812: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:24:09,336: INFO: data_transformation:  Processing train batch 40/54: samples 1950000-2000000]
[2025-11-01 15:24:10,858: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:24:10,880: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:24:10,913: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:24:11,845: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:24:11,851: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:24:11,919: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:25:06,726: INFO: data_transformation:  Processing train batch 41/54: samples 2000000-2050000]
[2025-11-01 15:25:08,215: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:25:08,242: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:25:08,293: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:25:09,179: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:25:09,208: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:25:09,271: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:26:04,723: INFO: data_transformation:  Processing train batch 42/54: samples 2050000-2100000]
[2025-11-01 15:26:06,185: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:26:06,223: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:26:06,275: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:26:07,160: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:26:07,189: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:26:07,219: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:27:07,510: INFO: data_transformation:  Processing train batch 43/54: samples 2100000-2150000]
[2025-11-01 15:27:08,982: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:27:08,992: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:27:09,058: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:27:09,912: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:27:09,939: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:27:10,012: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:28:17,933: INFO: data_transformation:  Processing train batch 44/54: samples 2150000-2200000]
[2025-11-01 15:28:19,583: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:28:19,584: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:28:19,584: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:28:20,764: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:28:20,764: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:28:20,764: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:29:18,976: INFO: data_transformation:  Processing train batch 45/54: samples 2200000-2250000]
[2025-11-01 15:29:21,905: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:29:22,053: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:29:22,105: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:29:23,342: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:29:23,502: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:29:23,531: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:30:38,354: INFO: data_transformation:  Processing train batch 46/54: samples 2250000-2300000]
[2025-11-01 15:30:40,545: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:30:40,545: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:30:40,546: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:30:41,728: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:30:41,730: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:30:41,734: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:31:50,958: INFO: data_transformation:  Processing train batch 47/54: samples 2300000-2350000]
[2025-11-01 15:31:52,834: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:31:52,834: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:31:52,838: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:31:54,028: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:31:54,029: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:31:54,035: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:33:02,829: INFO: data_transformation:  Processing train batch 48/54: samples 2350000-2400000]
[2025-11-01 15:33:05,032: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:33:05,032: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:33:05,040: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:33:06,198: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:33:06,199: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:33:06,202: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:34:04,575: INFO: data_transformation:  Processing train batch 49/54: samples 2400000-2450000]
[2025-11-01 15:34:06,085: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:34:06,087: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:34:06,122: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:34:07,037: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:34:07,040: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:34:07,065: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:35:06,267: INFO: data_transformation:  Processing train batch 50/54: samples 2450000-2500000]
[2025-11-01 15:35:07,698: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:35:07,728: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:35:07,764: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:35:08,705: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:35:08,726: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:35:08,801: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:36:03,264: INFO: data_transformation:  Processing train batch 51/54: samples 2500000-2550000]
[2025-11-01 15:36:05,140: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:36:05,141: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:36:06,357: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:36:06,360: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:37:02,869: INFO: data_transformation:  Processing train batch 52/54: samples 2550000-2600000]
[2025-11-01 15:37:05,032: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:37:05,033: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:37:05,033: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:37:06,242: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:37:06,246: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:37:06,249: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:38:07,431: INFO: data_transformation:  Processing train batch 53/54: samples 2600000-2650000]
[2025-11-01 15:38:08,893: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:38:08,897: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:38:08,940: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:38:09,898: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:38:09,899: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:38:09,900: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:39:20,358: INFO: data_transformation:  Processing train batch 54/54: samples 2650000-2693391]
[2025-11-01 15:39:22,358: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:39:22,358: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:39:22,360: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:39:23,524: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:39:23,525: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:39:23,526: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:40:26,284: INFO: data_transformation:  Starting tokenization for test data...]
[2025-11-01 15:40:26,285: INFO: data_transformation:  Processing test batch 1/7: samples 0-50000]
[2025-11-01 15:40:28,248: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:40:28,263: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:40:28,311: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:40:29,433: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:40:29,435: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:40:29,477: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:41:59,888: INFO: data_transformation:  Processing test batch 2/7: samples 50000-100000]
[2025-11-01 15:42:02,047: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:42:02,051: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:42:03,339: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:42:03,341: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:42:03,348: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:43:09,650: INFO: data_transformation:  Processing test batch 3/7: samples 100000-150000]
[2025-11-01 15:43:12,184: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:43:12,189: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:43:12,194: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:43:13,308: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:43:13,315: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:43:13,321: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:44:17,707: INFO: data_transformation:  Processing test batch 4/7: samples 150000-200000]
[2025-11-01 15:44:19,431: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:44:19,431: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:44:20,615: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:44:20,618: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:44:20,623: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:45:41,072: INFO: data_transformation:  Processing test batch 5/7: samples 200000-250000]
[2025-11-01 15:45:42,774: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:45:42,780: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:45:42,782: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:45:43,856: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:45:43,861: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:45:43,894: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:47:09,759: INFO: data_transformation:  Processing test batch 6/7: samples 250000-300000]
[2025-11-01 15:47:13,006: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:47:13,009: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:47:13,029: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:47:14,515: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:47:14,516: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:47:14,516: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:48:41,350: INFO: data_transformation:  Processing test batch 7/7: samples 300000-300679]
[2025-11-01 15:48:43,351: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:48:43,352: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:48:43,352: INFO: utils: NumExpr defaulting to 4 threads.]
[2025-11-01 15:48:44,652: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:48:44,652: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:48:44,654: INFO: config: TensorFlow version 2.20.0 available.]
[2025-11-01 15:48:53,171: INFO: data_transformation:   All train and test batches tokenized and saved successfully!]
[2025-11-01 17:23:59,901: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 17:23:59,915: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 17:23:59,922: INFO: common: created directory at: artifacts]
[2025-11-01 17:23:59,926: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 17:24:04,047: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-01 17:24:04,049: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 17:24:04,141: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-01 17:24:04,204: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-01 17:24:04,470: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-01 17:24:04,752: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-01 17:24:05,135: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-01 17:24:05,426: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-01 17:24:05,720: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-01 17:24:06,017: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-01 17:24:06,306: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-01 17:24:06,592: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-01 17:24:06,887: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-01 17:24:07,208: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-01 17:24:07,606: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-01 17:24:08,020: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-01 17:24:08,377: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-01 17:24:08,751: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-01 17:24:09,130: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-01 17:24:09,867: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-01 17:24:10,561: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-01 17:24:10,882: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-01 17:24:11,415: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-01 17:24:11,894: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-01 17:24:12,425: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-01 17:24:12,775: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-01 17:24:13,048: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-01 17:24:13,397: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-01 17:24:13,758: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-01 17:24:14,276: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-01 17:24:14,708: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-01 17:24:15,064: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-01 17:24:15,418: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-01 17:24:15,698: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-01 17:24:16,089: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-01 17:24:16,469: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-01 17:24:16,910: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-01 17:24:17,393: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-01 17:24:17,795: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-01 17:24:18,075: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-01 17:24:18,463: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-01 17:24:18,860: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-01 17:24:19,265: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-01 17:24:19,672: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-01 17:24:20,118: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-01 17:24:20,385: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-01 17:24:20,808: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-01 17:24:21,206: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-01 17:24:21,897: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-01 17:24:22,224: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-01 17:24:22,618: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-01 17:24:22,922: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-01 17:24:23,308: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-01 17:24:23,711: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-01 17:24:24,077: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-01 17:24:24,425: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-01 17:24:24,819: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-01 17:24:25,121: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-01 17:24:25,124: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-01 17:24:25,448: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-01 17:24:25,800: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-01 17:24:26,125: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-01 17:24:26,475: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-01 17:24:26,856: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-01 17:24:27,235: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-01 17:24:27,409: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-01 17:24:27,524: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-01 17:24:27,529: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-01 17:24:27,866: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 17:24:27,869: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 17:24:27,870: INFO: common: created directory at: artifacts]
[2025-11-01 17:24:27,871: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 17:24:27,876: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 17:24:27,876: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-01 17:26:12,003: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 17:26:12,009: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 17:26:12,011: INFO: common: created directory at: artifacts]
[2025-11-01 17:26:12,016: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 17:26:16,215: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-01 17:26:16,218: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 17:26:16,305: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-01 17:26:16,574: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-01 17:26:16,811: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-01 17:26:17,064: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-01 17:26:17,301: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-01 17:26:17,502: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-01 17:26:17,720: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-01 17:26:17,931: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-01 17:26:18,151: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-01 17:26:18,428: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-01 17:26:18,716: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-01 17:26:18,968: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-01 17:26:19,238: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-01 17:26:19,452: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-01 17:26:19,663: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-01 17:26:19,892: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-01 17:26:20,134: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-01 17:26:20,345: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-01 17:26:20,563: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-01 17:26:20,852: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-01 17:26:21,086: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-01 17:26:21,372: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-01 17:26:21,620: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-01 17:26:21,919: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-01 17:26:22,197: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-01 17:26:22,427: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-01 17:26:22,675: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-01 17:26:22,885: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-01 17:26:23,127: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-01 17:26:23,322: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-01 17:26:23,529: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-01 17:26:23,748: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-01 17:26:24,039: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-01 17:26:24,362: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-01 17:26:24,602: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-01 17:26:24,856: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-01 17:26:25,110: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-01 17:26:25,348: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-01 17:26:25,547: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-01 17:26:25,828: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-01 17:26:26,082: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-01 17:26:26,320: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-01 17:26:26,580: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-01 17:26:26,837: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-01 17:26:27,115: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-01 17:26:27,359: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-01 17:26:27,624: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-01 17:26:27,954: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-01 17:26:28,244: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-01 17:26:28,475: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-01 17:26:28,794: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-01 17:26:29,016: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-01 17:26:29,265: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-01 17:26:29,573: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-01 17:26:29,974: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-01 17:26:30,304: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-01 17:26:30,341: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-01 17:26:30,810: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-01 17:26:31,080: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-01 17:26:31,312: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-01 17:26:31,512: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-01 17:26:31,721: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-01 17:26:31,977: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-01 17:26:32,004: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-01 17:26:32,072: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-01 17:26:32,072: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-01 17:26:32,363: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 17:26:32,368: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 17:26:32,370: INFO: common: created directory at: artifacts]
[2025-11-01 17:26:32,372: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 17:26:32,376: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 17:26:32,384: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-01 17:26:40,971: INFO: model_trainer:  Training model...]
[2025-11-01 19:56:35,393: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 19:56:35,406: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 19:56:35,409: INFO: common: created directory at: artifacts]
[2025-11-01 19:56:35,412: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 19:56:40,183: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-01 19:56:40,186: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 19:56:40,320: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-01 19:56:40,681: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-01 19:56:40,950: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-01 19:56:41,204: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-01 19:56:41,483: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-01 19:56:41,752: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-01 19:56:42,023: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-01 19:56:42,254: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-01 19:56:42,529: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-01 19:56:42,790: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-01 19:56:43,062: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-01 19:56:43,323: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-01 19:56:43,588: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-01 19:56:43,858: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-01 19:56:44,145: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-01 19:56:44,439: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-01 19:56:44,675: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-01 19:56:44,958: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-01 19:56:45,247: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-01 19:56:45,518: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-01 19:56:45,823: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-01 19:56:46,068: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-01 19:56:46,372: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-01 19:56:46,617: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-01 19:56:46,926: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-01 19:56:47,170: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-01 19:56:47,455: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-01 19:56:47,685: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-01 19:56:47,985: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-01 19:56:48,254: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-01 19:56:48,501: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-01 19:56:48,837: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-01 19:56:49,049: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-01 19:56:49,374: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-01 19:56:49,708: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-01 19:56:49,949: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-01 19:56:50,213: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-01 19:56:50,504: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-01 19:56:50,780: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-01 19:56:51,008: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-01 19:56:51,241: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-01 19:56:51,498: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-01 19:56:51,737: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-01 19:56:51,983: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-01 19:56:52,199: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-01 19:56:52,458: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-01 19:56:52,713: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-01 19:56:52,958: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-01 19:56:53,228: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-01 19:56:53,484: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-01 19:56:53,718: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-01 19:56:53,987: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-01 19:56:54,268: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-01 19:56:54,503: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-01 19:56:54,742: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-01 19:56:55,003: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-01 19:56:55,004: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-01 19:56:55,244: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-01 19:56:55,483: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-01 19:56:55,753: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-01 19:56:56,018: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-01 19:56:56,279: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-01 19:56:56,551: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-01 19:56:56,592: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-01 19:56:56,672: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-01 19:56:56,674: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-01 19:56:57,102: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 19:56:57,109: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 19:56:57,112: INFO: common: created directory at: artifacts]
[2025-11-01 19:56:57,116: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 19:56:57,121: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 19:56:57,126: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-01 19:57:04,919: INFO: model_trainer:  Training model...]
[2025-11-01 20:07:14,774: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 20:07:14,781: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 20:07:14,784: INFO: common: created directory at: artifacts]
[2025-11-01 20:07:14,787: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 20:07:17,822: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-01 20:07:17,825: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 20:07:17,884: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-01 20:07:18,216: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-01 20:07:18,447: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-01 20:07:18,706: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-01 20:07:18,960: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-01 20:07:19,199: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-01 20:07:19,445: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-01 20:07:19,692: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-01 20:07:19,960: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-01 20:07:20,221: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-01 20:07:20,466: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-01 20:07:20,726: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-01 20:07:20,935: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-01 20:07:21,145: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-01 20:07:21,419: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-01 20:07:21,656: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-01 20:07:21,894: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-01 20:07:22,118: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-01 20:07:22,347: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-01 20:07:22,590: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-01 20:07:22,845: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-01 20:07:23,091: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-01 20:07:23,300: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-01 20:07:23,541: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-01 20:07:23,778: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-01 20:07:24,017: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-01 20:07:24,259: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-01 20:07:24,511: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-01 20:07:24,762: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-01 20:07:25,019: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-01 20:07:25,241: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-01 20:07:25,496: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-01 20:07:25,700: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-01 20:07:25,939: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-01 20:07:26,197: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-01 20:07:26,460: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-01 20:07:26,710: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-01 20:07:26,976: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-01 20:07:27,233: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-01 20:07:27,490: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-01 20:07:27,747: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-01 20:07:27,998: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-01 20:07:28,223: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-01 20:07:28,481: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-01 20:07:28,741: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-01 20:07:29,014: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-01 20:07:29,279: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-01 20:07:29,533: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-01 20:07:29,792: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-01 20:07:30,051: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-01 20:07:30,298: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-01 20:07:30,536: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-01 20:07:30,772: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-01 20:07:31,032: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-01 20:07:31,289: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-01 20:07:31,531: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-01 20:07:31,532: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-01 20:07:31,752: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-01 20:07:31,979: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-01 20:07:32,207: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-01 20:07:32,438: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-01 20:07:32,668: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-01 20:07:32,898: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-01 20:07:32,923: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-01 20:07:32,984: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-01 20:07:32,985: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-01 20:07:33,177: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 20:07:33,180: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 20:07:33,181: INFO: common: created directory at: artifacts]
[2025-11-01 20:07:33,182: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 20:07:33,184: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 20:07:33,185: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-01 20:07:41,114: INFO: model_trainer:  Training model...]
[2025-11-01 20:08:58,604: INFO: rouge_scorer: Using default tokenizer.]
[2025-11-01 20:14:28,225: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 20:14:28,234: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 20:14:28,237: INFO: common: created directory at: artifacts]
[2025-11-01 20:14:28,240: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 20:14:31,400: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-01 20:14:31,402: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 20:14:31,449: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-01 20:14:31,732: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-01 20:14:31,957: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-01 20:14:32,189: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-01 20:14:32,412: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-01 20:14:32,636: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-01 20:14:32,861: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-01 20:14:33,096: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-01 20:14:33,331: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-01 20:14:33,676: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-01 20:14:33,967: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-01 20:14:34,220: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-01 20:14:34,534: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-01 20:14:34,807: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-01 20:14:35,065: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-01 20:14:35,299: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-01 20:14:35,543: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-01 20:14:35,792: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-01 20:14:36,054: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-01 20:14:36,325: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-01 20:14:36,555: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-01 20:14:36,778: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-01 20:14:37,019: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-01 20:14:37,249: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-01 20:14:37,495: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-01 20:14:37,740: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-01 20:14:37,984: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-01 20:14:38,280: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-01 20:14:38,552: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-01 20:14:38,798: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-01 20:14:39,005: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-01 20:14:39,241: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-01 20:14:39,488: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-01 20:14:39,749: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-01 20:14:40,003: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-01 20:14:40,258: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-01 20:14:40,521: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-01 20:14:40,763: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-01 20:14:40,997: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-01 20:14:41,231: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-01 20:14:41,451: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-01 20:14:41,728: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-01 20:14:41,948: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-01 20:14:42,216: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-01 20:14:42,510: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-01 20:14:42,761: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-01 20:14:43,043: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-01 20:14:43,257: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-01 20:14:43,475: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-01 20:14:43,689: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-01 20:14:43,885: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-01 20:14:44,122: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-01 20:14:44,433: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-01 20:14:44,689: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-01 20:14:44,934: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-01 20:14:45,109: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-01 20:14:45,111: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-01 20:14:45,325: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-01 20:14:45,579: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-01 20:14:45,833: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-01 20:14:46,051: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-01 20:14:46,258: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-01 20:14:46,517: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-01 20:14:46,593: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-01 20:14:46,643: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-01 20:14:46,644: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-01 20:14:47,060: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 20:14:47,069: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 20:14:47,072: INFO: common: created directory at: artifacts]
[2025-11-01 20:14:47,074: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 20:14:47,083: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 20:14:47,092: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-01 20:14:53,461: INFO: model_trainer:  Training model...]
[2025-11-01 20:15:45,296: INFO: rouge_scorer: Using default tokenizer.]
[2025-11-01 20:16:33,227: INFO: rouge_scorer: Using default tokenizer.]
[2025-11-01 20:17:26,004: INFO: rouge_scorer: Using default tokenizer.]
[2025-11-01 20:26:14,243: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 20:26:14,265: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 20:26:14,267: INFO: common: created directory at: artifacts]
[2025-11-01 20:26:14,270: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 20:26:19,015: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-01 20:26:19,018: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-01 20:26:19,107: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-01 20:26:19,498: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-01 20:26:19,896: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-01 20:26:20,290: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-01 20:26:20,681: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-01 20:26:21,082: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-01 20:26:21,500: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-01 20:26:21,794: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-01 20:26:22,192: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-01 20:26:22,600: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-01 20:26:22,991: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-01 20:26:23,386: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-01 20:26:23,794: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-01 20:26:24,181: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-01 20:26:24,625: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-01 20:26:25,002: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-01 20:26:25,383: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-01 20:26:25,722: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-01 20:26:26,071: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-01 20:26:26,378: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-01 20:26:26,681: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-01 20:26:27,052: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-01 20:26:27,450: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-01 20:26:27,850: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-01 20:26:28,229: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-01 20:26:28,552: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-01 20:26:28,928: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-01 20:26:29,298: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-01 20:26:29,709: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-01 20:26:30,103: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-01 20:26:30,500: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-01 20:26:30,908: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-01 20:26:31,315: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-01 20:26:31,611: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-01 20:26:31,947: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-01 20:26:32,332: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-01 20:26:32,704: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-01 20:26:33,057: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-01 20:26:33,452: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-01 20:26:33,840: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-01 20:26:34,179: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-01 20:26:34,545: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-01 20:26:35,149: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-01 20:26:35,436: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-01 20:26:35,744: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-01 20:26:36,068: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-01 20:26:36,383: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-01 20:26:36,621: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-01 20:26:36,896: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-01 20:26:37,212: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-01 20:26:37,501: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-01 20:26:37,817: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-01 20:26:38,127: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-01 20:26:38,453: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-01 20:26:38,757: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-01 20:26:38,950: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-01 20:26:38,951: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-01 20:26:39,208: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-01 20:26:39,480: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-01 20:26:39,739: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-01 20:26:40,019: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-01 20:26:40,306: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-01 20:26:40,588: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-01 20:26:40,671: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-01 20:26:40,707: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-01 20:26:40,708: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-01 20:26:40,951: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-01 20:26:40,955: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-01 20:26:40,956: INFO: common: created directory at: artifacts]
[2025-11-01 20:26:40,957: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-01 20:26:40,960: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-01 20:26:40,961: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-02 02:42:50,858: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 02:42:50,889: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 02:42:50,895: INFO: common: created directory at: artifacts]
[2025-11-02 02:42:50,897: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 02:42:59,256: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-02 02:42:59,271: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 02:42:59,339: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-02 02:42:59,876: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-02 02:43:00,225: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-02 02:43:00,709: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-02 02:43:01,055: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-02 02:43:01,325: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-02 02:43:01,614: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-02 02:43:01,905: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-02 02:43:02,234: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-02 02:43:02,571: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-02 02:43:02,900: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-02 02:43:03,212: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-02 02:43:03,522: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-02 02:43:03,853: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-02 02:43:04,260: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-02 02:43:04,729: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-02 02:43:05,185: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-02 02:43:05,523: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-02 02:43:05,959: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-02 02:43:06,283: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-02 02:43:06,728: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-02 02:43:07,108: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-02 02:43:07,453: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-02 02:43:07,873: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-02 02:43:08,258: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-02 02:43:08,584: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-02 02:43:08,843: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-02 02:43:09,158: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-02 02:43:09,419: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-02 02:43:09,685: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-02 02:43:09,978: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-02 02:43:10,260: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-02 02:43:10,637: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-02 02:43:11,038: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-02 02:43:11,383: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-02 02:43:11,735: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-02 02:43:12,353: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-02 02:43:12,749: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-02 02:43:13,092: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-02 02:43:13,418: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-02 02:43:13,705: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-02 02:43:14,008: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-02 02:43:14,323: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-02 02:43:14,662: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-02 02:43:15,103: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-02 02:43:15,441: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-02 02:43:15,745: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-02 02:43:16,091: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-02 02:43:16,345: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-02 02:43:16,568: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-02 02:43:16,776: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-02 02:43:17,012: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-02 02:43:17,260: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-02 02:43:17,675: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-02 02:43:17,907: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-02 02:43:18,102: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-02 02:43:18,160: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-02 02:43:18,460: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-02 02:43:18,871: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-02 02:43:19,185: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-02 02:43:19,543: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-02 02:43:19,924: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-02 02:43:20,323: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-02 02:43:20,407: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-02 02:43:20,426: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-02 02:43:20,426: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-02 02:43:20,734: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 02:43:20,738: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 02:43:20,740: INFO: common: created directory at: artifacts]
[2025-11-02 02:43:20,741: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-02 02:43:20,743: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-02 02:43:20,744: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-02 02:43:28,395: INFO: model_trainer:  Training model...]
[2025-11-02 03:01:43,522: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 03:01:43,539: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 03:01:43,542: INFO: common: created directory at: artifacts]
[2025-11-02 03:01:43,545: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 03:01:49,586: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-02 03:01:49,588: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 03:01:49,689: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-02 03:01:50,000: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-02 03:01:50,302: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-02 03:01:50,568: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-02 03:01:50,850: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-02 03:01:51,222: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-02 03:01:51,527: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-02 03:01:51,831: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-02 03:01:52,084: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-02 03:01:52,314: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-02 03:01:52,562: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-02 03:01:52,816: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-02 03:01:53,067: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-02 03:01:53,312: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-02 03:01:53,581: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-02 03:01:53,872: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-02 03:01:54,164: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-02 03:01:54,442: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-02 03:01:54,733: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-02 03:01:55,013: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-02 03:01:55,276: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-02 03:01:55,513: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-02 03:01:55,747: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-02 03:01:55,996: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-02 03:01:56,244: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-02 03:01:56,461: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-02 03:01:56,691: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-02 03:01:56,943: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-02 03:01:57,197: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-02 03:01:57,449: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-02 03:01:57,715: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-02 03:01:57,969: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-02 03:01:58,209: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-02 03:01:58,443: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-02 03:01:58,677: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-02 03:01:58,897: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-02 03:01:59,188: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-02 03:01:59,409: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-02 03:01:59,659: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-02 03:01:59,905: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-02 03:02:00,156: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-02 03:02:00,402: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-02 03:02:00,629: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-02 03:02:00,905: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-02 03:02:01,136: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-02 03:02:01,370: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-02 03:02:01,634: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-02 03:02:01,853: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-02 03:02:02,069: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-02 03:02:02,287: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-02 03:02:02,473: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-02 03:02:02,686: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-02 03:02:02,956: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-02 03:02:03,217: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-02 03:02:03,483: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-02 03:02:03,660: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-02 03:02:03,662: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-02 03:02:03,959: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-02 03:02:04,249: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-02 03:02:04,524: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-02 03:02:04,820: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-02 03:02:05,124: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-02 03:02:05,407: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-02 03:02:05,449: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-02 03:02:05,515: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-02 03:02:05,516: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-02 03:02:05,896: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 03:02:05,902: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 03:02:05,905: INFO: common: created directory at: artifacts]
[2025-11-02 03:02:05,911: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-02 03:02:05,920: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-02 03:02:05,922: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-02 03:02:14,956: INFO: model_trainer:  Training model...]
[2025-11-02 03:02:21,634: INFO: model_trainer:  Model saved at: artifacts\model_trainer\t5-small-trained]
[2025-11-02 04:07:53,101: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 04:07:53,107: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 04:07:53,108: INFO: common: created directory at: artifacts]
[2025-11-02 04:07:53,110: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 04:07:58,231: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-02 04:07:58,248: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 04:07:58,368: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-02 04:07:58,662: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-02 04:07:58,919: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-02 04:07:59,140: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-02 04:07:59,352: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-02 04:07:59,575: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-02 04:07:59,810: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-02 04:08:00,010: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-02 04:08:00,313: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-02 04:08:00,536: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-02 04:08:00,763: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-02 04:08:00,972: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-02 04:08:01,174: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-02 04:08:01,380: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-02 04:08:01,591: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-02 04:08:01,808: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-02 04:08:02,019: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-02 04:08:02,240: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-02 04:08:02,461: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-02 04:08:02,744: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-02 04:08:03,040: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-02 04:08:03,280: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-02 04:08:03,540: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-02 04:08:03,748: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-02 04:08:04,011: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-02 04:08:04,261: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-02 04:08:04,526: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-02 04:08:04,782: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-02 04:08:04,990: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-02 04:08:05,255: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-02 04:08:05,534: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-02 04:08:05,801: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-02 04:08:06,046: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-02 04:08:06,314: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-02 04:08:06,535: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-02 04:08:06,778: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-02 04:08:07,042: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-02 04:08:07,288: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-02 04:08:07,532: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-02 04:08:07,862: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-02 04:08:08,093: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-02 04:08:08,344: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-02 04:08:08,573: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-02 04:08:08,808: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-02 04:08:09,043: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-02 04:08:09,279: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-02 04:08:09,518: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-02 04:08:09,755: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-02 04:08:09,956: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-02 04:08:10,184: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-02 04:08:10,424: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-02 04:08:10,704: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-02 04:08:10,954: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-02 04:08:11,193: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-02 04:08:11,421: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-02 04:08:11,666: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-02 04:08:11,668: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-02 04:08:11,902: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-02 04:08:12,122: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-02 04:08:12,348: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-02 04:08:12,590: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-02 04:08:12,861: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-02 04:08:13,114: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-02 04:08:13,146: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-02 04:08:13,225: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-02 04:08:13,229: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-02 04:08:13,726: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 04:08:13,733: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 04:08:13,735: INFO: common: created directory at: artifacts]
[2025-11-02 04:08:13,739: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-02 04:08:13,756: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-02 04:08:13,757: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-02 04:08:20,676: INFO: model_trainer:  Training model...]
[2025-11-02 04:08:24,949: INFO: model_trainer:  Model saved at: artifacts\model_trainer\t5-small-trained]
[2025-11-02 06:24:28,790: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 06:24:28,792: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 06:24:28,793: INFO: common: created directory at: artifacts]
[2025-11-02 06:24:28,794: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 06:24:28,814: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/tokenizer_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E5EC980>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 57b22ab4-1585-44b2-804a-38e54661f3fb)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/tokenizer_config.json]
[2025-11-02 06:24:28,818: WARNING: _http: Retrying in 1s [Retry 1/5].]
[2025-11-02 06:24:29,825: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/tokenizer_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E50F750>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: a5ed21e3-0912-46f6-9b47-a69bd29e955b)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/tokenizer_config.json]
[2025-11-02 06:24:29,826: WARNING: _http: Retrying in 2s [Retry 2/5].]
[2025-11-02 06:24:31,862: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/tokenizer_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E50FD90>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 2329f8fb-8435-4237-8fd8-dd835cc257ae)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/tokenizer_config.json]
[2025-11-02 06:24:31,864: WARNING: _http: Retrying in 4s [Retry 3/5].]
[2025-11-02 06:24:35,902: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/tokenizer_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E600190>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: e4c6cc06-d0a4-4460-a66a-e880950664fb)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/tokenizer_config.json]
[2025-11-02 06:24:35,911: WARNING: _http: Retrying in 8s [Retry 4/5].]
[2025-11-02 06:24:43,920: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/tokenizer_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E600550>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 37609ede-b8c3-4006-9c38-5d4186a74939)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/tokenizer_config.json]
[2025-11-02 06:24:43,921: WARNING: _http: Retrying in 8s [Retry 5/5].]
[2025-11-02 06:24:51,981: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/tokenizer_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E600910>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 3b498074-ad79-4f62-bb11-cff1388bfc3d)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/tokenizer_config.json]
[2025-11-02 06:24:52,212: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E50FD90>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 153c1c8b-88bd-4315-889e-c706616279ef)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json]
[2025-11-02 06:24:52,213: WARNING: _http: Retrying in 1s [Retry 1/5].]
[2025-11-02 06:24:53,223: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E50F610>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 247afbdb-6910-49c8-a5e1-f833d9bc1672)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json]
[2025-11-02 06:24:53,227: WARNING: _http: Retrying in 2s [Retry 2/5].]
[2025-11-02 06:24:55,240: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E50F890>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 47af62e9-29f6-401f-911f-7c36b1a68ab0)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json]
[2025-11-02 06:24:55,242: WARNING: _http: Retrying in 4s [Retry 3/5].]
[2025-11-02 06:24:59,250: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E50F390>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 00557614-8764-4c29-8653-e7a0f089902b)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json]
[2025-11-02 06:24:59,253: WARNING: _http: Retrying in 8s [Retry 4/5].]
[2025-11-02 06:25:07,269: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E50F250>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 1a0dc925-cebb-4ef4-bd40-0eab9e9227af)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json]
[2025-11-02 06:25:07,270: WARNING: _http: Retrying in 8s [Retry 5/5].]
[2025-11-02 06:25:15,273: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E6142D0>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: cf105b6d-10e1-4ae9-b350-8f57d20055f8)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json]
[2025-11-02 06:25:15,658: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E617890>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 4208376f-0570-4e16-bd11-982b30f95d41)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json]
[2025-11-02 06:25:15,660: WARNING: _http: Retrying in 1s [Retry 1/5].]
[2025-11-02 06:25:16,664: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7E617C50>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 55789764-4cbb-4434-8356-42889027052b)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json]
[2025-11-02 06:25:16,665: WARNING: _http: Retrying in 2s [Retry 2/5].]
[2025-11-02 06:25:18,669: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7EFE0050>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 904d174f-6fda-476f-81d2-fae872789c61)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json]
[2025-11-02 06:25:18,671: WARNING: _http: Retrying in 4s [Retry 3/5].]
[2025-11-02 06:25:22,673: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7EFE0410>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: c3d49dca-8030-4070-8bfb-9e0728c58c36)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json]
[2025-11-02 06:25:22,675: WARNING: _http: Retrying in 8s [Retry 4/5].]
[2025-11-02 06:25:30,678: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7EFE07D0>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: 536f0d8e-a17c-4358-955f-c6ce338caefa)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json]
[2025-11-02 06:25:30,679: WARNING: _http: Retrying in 8s [Retry 5/5].]
[2025-11-02 06:25:38,684: WARNING: _http: '(MaxRetryError('HTTPSConnectionPool(host=\'huggingface.co\', port=443): Max retries exceeded with url: /t5-small/resolve/main/generation_config.json (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x0000016F7EFE0B90>: Failed to resolve \'huggingface.co\' ([Errno 11001] getaddrinfo failed)"))'), '(Request ID: f83827f3-500a-4165-9e4a-4be955238f6e)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/generation_config.json]
[2025-11-02 06:25:38,687: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-02 06:25:38,687: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-02 06:25:38,702: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-02 06:25:38,940: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-02 06:25:39,148: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-02 06:25:39,356: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-02 06:25:39,573: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-02 06:25:39,786: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-02 06:25:39,987: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-02 06:25:40,201: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-02 06:25:40,410: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-02 06:25:40,608: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-02 06:25:40,822: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-02 06:25:41,033: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-02 06:25:41,247: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-02 06:25:41,464: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-02 06:25:41,687: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-02 06:25:41,908: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-02 06:25:42,119: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-02 06:25:42,336: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-02 06:25:42,557: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-02 06:25:42,755: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-02 06:25:42,978: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-02 06:25:43,197: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-02 06:25:43,414: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-02 06:25:43,628: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-02 06:25:43,851: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-02 06:25:44,067: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-02 06:25:44,284: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-02 06:25:44,504: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-02 06:25:44,887: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-02 06:25:45,126: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-02 06:25:45,416: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-02 06:25:45,787: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-02 06:25:46,165: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-02 06:25:46,482: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-02 06:25:46,854: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-02 06:25:47,191: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-02 06:25:47,517: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-02 06:25:47,751: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-02 06:25:48,101: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-02 06:25:48,682: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-02 06:25:49,045: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-02 06:25:49,400: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-02 06:25:49,696: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-02 06:25:50,058: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-02 06:25:50,301: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-02 06:25:50,594: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-02 06:25:50,850: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-02 06:25:51,128: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-02 06:25:51,411: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-02 06:25:51,690: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-02 06:25:51,949: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-02 06:25:52,224: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-02 06:25:52,526: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-02 06:25:52,741: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-02 06:25:53,075: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-02 06:25:53,386: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-02 06:25:53,389: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-02 06:25:53,644: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-02 06:25:53,926: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-02 06:25:54,306: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-02 06:25:54,613: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-02 06:25:54,944: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-02 06:25:55,194: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-02 06:25:55,212: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-02 06:25:55,244: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-02 06:25:55,245: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-02 06:25:55,635: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-02 06:25:55,661: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-02 06:25:55,677: INFO: common: created directory at: artifacts]
[2025-11-02 06:25:55,693: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-02 06:25:55,708: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-02 06:25:55,726: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-02 06:26:04,713: INFO: model_trainer:  Training model...]
[2025-11-04 04:00:48,770: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-04 04:00:48,797: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-04 04:00:48,804: INFO: common: created directory at: artifacts]
[2025-11-04 04:00:48,806: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-04 04:00:52,754: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-04 04:00:52,757: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-04 04:00:52,825: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-04 04:00:53,387: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-04 04:00:53,704: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-04 04:00:54,017: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-04 04:00:54,305: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-04 04:00:54,571: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-04 04:00:54,886: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-04 04:00:55,163: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-04 04:00:55,452: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-04 04:00:55,799: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-04 04:00:56,132: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-04 04:00:56,496: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-04 04:00:56,921: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-04 04:00:57,226: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-04 04:00:57,602: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-04 04:00:57,887: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-04 04:00:58,233: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-04 04:00:58,983: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-04 04:00:59,259: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-04 04:00:59,521: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-04 04:00:59,786: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-04 04:01:00,066: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-04 04:01:00,319: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-04 04:01:00,580: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-04 04:01:00,855: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-04 04:01:01,204: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-04 04:01:01,555: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-04 04:01:01,920: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-04 04:01:02,727: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-04 04:01:03,150: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-04 04:01:03,439: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-04 04:01:03,852: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-04 04:01:04,144: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-04 04:01:04,485: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-04 04:01:04,911: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-04 04:01:05,323: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-04 04:01:05,657: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-04 04:01:05,925: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-04 04:01:06,187: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-04 04:01:06,469: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-04 04:01:06,752: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-04 04:01:07,094: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-04 04:01:07,363: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-04 04:01:07,661: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-04 04:01:07,973: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-04 04:01:08,272: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-04 04:01:08,573: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-04 04:01:08,901: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-04 04:01:09,207: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-04 04:01:09,529: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-04 04:01:09,813: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-04 04:01:10,100: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-04 04:01:10,386: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-04 04:01:10,671: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-04 04:01:10,965: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-04 04:01:11,172: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-04 04:01:11,174: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-04 04:01:11,518: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-04 04:01:11,874: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-04 04:01:12,203: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-04 04:01:12,504: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-04 04:01:12,872: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-04 04:01:13,279: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-04 04:01:13,485: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-04 04:01:13,541: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-04 04:01:13,554: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-04 04:01:14,921: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-04 04:01:14,936: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-04 04:01:14,969: INFO: common: created directory at: artifacts]
[2025-11-04 04:01:15,006: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-04 04:01:15,024: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-04 04:01:15,053: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-04 04:01:23,221: INFO: model_trainer:  Training model...]
[2025-11-04 04:19:01,471: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-04 04:19:01,479: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-04 04:19:01,481: INFO: common: created directory at: artifacts]
[2025-11-04 04:19:01,483: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-04 04:19:07,180: INFO: model_trainer:  Starting model training pipeline...]
[2025-11-04 04:19:07,182: INFO: common: created directory at: artifacts/model_trainer]
[2025-11-04 04:19:07,227: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/train]
[2025-11-04 04:19:07,560: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_1 with 50000 samples]
[2025-11-04 04:19:07,826: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_10 with 50000 samples]
[2025-11-04 04:19:08,103: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_11 with 50000 samples]
[2025-11-04 04:19:08,360: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_12 with 50000 samples]
[2025-11-04 04:19:08,622: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_13 with 50000 samples]
[2025-11-04 04:19:08,903: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_14 with 50000 samples]
[2025-11-04 04:19:09,125: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_15 with 50000 samples]
[2025-11-04 04:19:09,327: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_16 with 50000 samples]
[2025-11-04 04:19:09,552: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_17 with 50000 samples]
[2025-11-04 04:19:09,752: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_18 with 50000 samples]
[2025-11-04 04:19:10,046: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_19 with 50000 samples]
[2025-11-04 04:19:10,294: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_2 with 50000 samples]
[2025-11-04 04:19:10,652: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_20 with 50000 samples]
[2025-11-04 04:19:10,906: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_21 with 50000 samples]
[2025-11-04 04:19:11,146: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_22 with 50000 samples]
[2025-11-04 04:19:11,360: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_23 with 50000 samples]
[2025-11-04 04:19:11,569: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_24 with 50000 samples]
[2025-11-04 04:19:11,788: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_25 with 50000 samples]
[2025-11-04 04:19:12,007: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_26 with 50000 samples]
[2025-11-04 04:19:12,497: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_27 with 50000 samples]
[2025-11-04 04:19:12,837: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_28 with 50000 samples]
[2025-11-04 04:19:13,056: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_29 with 50000 samples]
[2025-11-04 04:19:13,301: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_3 with 50000 samples]
[2025-11-04 04:19:13,537: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_30 with 50000 samples]
[2025-11-04 04:19:13,753: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_31 with 50000 samples]
[2025-11-04 04:19:14,051: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_32 with 50000 samples]
[2025-11-04 04:19:14,271: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_33 with 50000 samples]
[2025-11-04 04:19:14,485: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_34 with 50000 samples]
[2025-11-04 04:19:14,724: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_35 with 50000 samples]
[2025-11-04 04:19:14,974: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_36 with 50000 samples]
[2025-11-04 04:19:15,218: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_37 with 50000 samples]
[2025-11-04 04:19:15,439: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_38 with 50000 samples]
[2025-11-04 04:19:15,698: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_39 with 50000 samples]
[2025-11-04 04:19:15,924: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_4 with 50000 samples]
[2025-11-04 04:19:16,174: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_40 with 50000 samples]
[2025-11-04 04:19:16,458: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_41 with 50000 samples]
[2025-11-04 04:19:16,705: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_42 with 50000 samples]
[2025-11-04 04:19:16,954: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_43 with 50000 samples]
[2025-11-04 04:19:17,224: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_44 with 50000 samples]
[2025-11-04 04:19:17,493: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_45 with 50000 samples]
[2025-11-04 04:19:17,759: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_46 with 50000 samples]
[2025-11-04 04:19:18,043: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_47 with 50000 samples]
[2025-11-04 04:19:18,315: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_48 with 50000 samples]
[2025-11-04 04:19:18,606: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_49 with 50000 samples]
[2025-11-04 04:19:18,832: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_5 with 50000 samples]
[2025-11-04 04:19:19,086: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_50 with 50000 samples]
[2025-11-04 04:19:19,354: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_51 with 50000 samples]
[2025-11-04 04:19:19,650: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_52 with 50000 samples]
[2025-11-04 04:19:19,861: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_53 with 50000 samples]
[2025-11-04 04:19:20,063: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_54 with 43391 samples]
[2025-11-04 04:19:20,414: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_6 with 50000 samples]
[2025-11-04 04:19:20,651: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_7 with 50000 samples]
[2025-11-04 04:19:20,983: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_8 with 50000 samples]
[2025-11-04 04:19:21,326: INFO: model_trainer:  Loaded artifacts\data_transformation\train\train_tokenized_batch_9 with 50000 samples]
[2025-11-04 04:19:21,556: INFO: model_trainer:  Combined total samples: 2693391]
[2025-11-04 04:19:21,559: INFO: model_trainer:  Loading batches from: artifacts/data_transformation/test]
[2025-11-04 04:19:21,944: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_1 with 50000 samples]
[2025-11-04 04:19:22,249: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_2 with 50000 samples]
[2025-11-04 04:19:22,505: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_3 with 50000 samples]
[2025-11-04 04:19:22,789: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_4 with 50000 samples]
[2025-11-04 04:19:23,047: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_5 with 50000 samples]
[2025-11-04 04:19:23,388: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_6 with 50000 samples]
[2025-11-04 04:19:23,447: INFO: model_trainer:  Loaded artifacts\data_transformation\test\test_tokenized_batch_7 with 679 samples]
[2025-11-04 04:19:23,474: INFO: model_trainer:  Combined total samples: 300679]
[2025-11-04 04:19:23,475: INFO: model_trainer:  Train samples: 2693391 | Eval samples: 300679]
[2025-11-04 04:19:31,396: INFO: model_trainer:  Training model...]
[2025-11-04 04:20:59,128: INFO: model_trainer:  Model saved at: artifacts\model_trainer\t5-small-trained]
[2025-11-04 04:23:40,077: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-04 04:23:40,092: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-04 04:23:40,094: INFO: common: created directory at: artifacts]
[2025-11-04 04:23:40,096: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-04 04:23:40,099: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-04 04:23:40,100: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-04 04:23:40,101: INFO: model_evaluation:  Loading tokenized batches from artifacts/data_transformation/test]
[2025-11-04 04:23:41,606: INFO: model_evaluation:  Loaded total 300679 samples.]
[2025-11-04 04:23:41,628: INFO: model_evaluation:  Generating predictions...]
[2025-11-04 04:25:04,304: INFO: common: yaml file: config\config.yaml loaded successfully]
[2025-11-04 04:25:04,308: INFO: common: yaml file: param\params.yaml loaded successfully]
[2025-11-04 04:25:04,309: INFO: common: created directory at: artifacts]
[2025-11-04 04:25:04,309: INFO: common: created directory at: artifacts/model_evaluation]
[2025-11-04 04:25:04,311: INFO: model_evaluation:  No checkpoint found, loading model from: artifacts\model_trainer\t5-small-trained]
[2025-11-04 04:25:04,311: INFO: model_evaluation:  Loading model and tokenizer...]
[2025-11-04 04:25:05,215: INFO: model_evaluation:  Loading tokenized batches from artifacts/data_transformation/test]
[2025-11-04 04:25:05,479: INFO: model_evaluation:  Loaded total 300679 samples.]
[2025-11-04 04:25:05,493: INFO: model_evaluation:  Generating predictions...]
[2025-11-04 04:25:34,762: INFO: model_evaluation: Generated 10 samples...]
[2025-11-04 04:25:48,325: INFO: model_evaluation: Generated 20 samples...]
[2025-11-04 04:25:48,634: INFO: model_evaluation:  Calculating BLEU and ROUGE scores...]
[2025-11-04 04:26:13,628: INFO: rouge_scorer: Using default tokenizer.]
[2025-11-04 04:26:13,948: INFO: model_evaluation:  BLEU Score: 0.1067]
[2025-11-04 04:26:13,948: INFO: model_evaluation:  ROUGE Scores: {'rouge1': np.float64(0.21185766509411302), 'rouge2': np.float64(0.11671939246151347), 'rougeL': np.float64(0.16757374515276446)}]
[2025-11-04 04:26:13,949: INFO: model_evaluation:  BLEU Score: 0.1067]
[2025-11-04 04:26:13,949: INFO: model_evaluation:  ROUGE Scores: {'rouge1': np.float64(0.21185766509411302), 'rouge2': np.float64(0.11671939246151347), 'rougeL': np.float64(0.16757374515276446)}]
[2025-11-04 04:26:13,950: INFO: model_evaluation:  Metrics saved to: artifacts\model_evaluation\metrics.csv]
[2025-11-04 04:26:13,971: INFO: model_evaluation:  Sample predictions saved to: artifacts\model_evaluation\predictions.csv]
